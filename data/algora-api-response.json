{
  "result": {
    "data": {
      "json": {
        "items": [
          {
            "id": "qdrant#3531",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "qdrant",
              "id": "generated-qdrant",
              "name": "Qdrant",
              "description": "",
              "members": [],
              "display_name": "Qdrant",
              "created_at": "2025-12-16T21:16:31.674Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/qdrant?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "qdrant",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:31.674Z",
            "created_at": "2025-12-16T21:16:31.674Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-qdrant#3531",
              "status": "open",
              "type": "issue",
              "number": 3531,
              "title": "Better error response for wrong datetime format in REST filter",
              "source": {
                "data": {
                  "id": "source-qdrant#3531",
                  "user": {
                    "login": "timvisee",
                    "id": 856222,
                    "node_id": "MDQ6VXNlcjg1NjIyMg==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/856222?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/timvisee",
                    "html_url": "https://github.com/timvisee",
                    "followers_url": "https://api.github.com/users/timvisee/followers",
                    "following_url": "https://api.github.com/users/timvisee/following{/other_user}",
                    "gists_url": "https://api.github.com/users/timvisee/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/timvisee/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/timvisee/subscriptions",
                    "organizations_url": "https://api.github.com/users/timvisee/orgs",
                    "repos_url": "https://api.github.com/users/timvisee/repos",
                    "events_url": "https://api.github.com/users/timvisee/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/timvisee/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Better error response for wrong datetime format in REST filter",
                  "body": "**Is your feature request related to a problem? Please describe.**\r\nWe recently merged <https://github.com/qdrant/qdrant/pull/3395> which adds a datetime payload index.\r\n\r\nCurrently, the datetime parser is very strict, only allowing [RFC 3339](https://www.rfc-editor.org/rfc/rfc3339) formats in our REST API. A common format such as `YYYY-MM-DD HH:MM:SS` is currently not accepted.\r\n\r\nIf you'd send the following request:\r\n\r\n```json\r\nPOST collections/test_collection/points/scroll\r\n{\r\n  \"limit\": 10,\r\n  \"filter\": {\r\n    \"must\": [\r\n      {\r\n        \"key\": \"updated\",\r\n        \"datetime_range\": {\r\n          \"gt\": \"2014-01-01T00:00:00\"\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nYou'd receive the following error:\r\n\r\n```\r\nFormat error in JSON body: data did not match any variant of untagged enum Condition at line 1 column 96\r\n```\r\n\r\nThis error is very confusing.\r\n\r\n**Describe the solution you'd like**\r\nWe'd strongly prefer a more descriptive error message instead.\r\n\r\nSomething like this would be a lot better:\r\n\r\n```\r\nFormat error in JSON body: '2014-01-01T00:00:00' does not match any accepted datetime format\r\n```\r\n\r\n**Describe alternatives you've considered**\r\n_None_\r\n\r\n**Additional context**\r\nRelated issue: <https://github.com/qdrant/qdrant/issues/3529>",
                  "html_url": "https://github.com/qdrant/qdrant/issues/3531"
                },
                "type": "github"
              },
              "hash": "qdrant/qdrant#3531",
              "body": "**Is your feature request related to a problem? Please describe.**\r\nWe recently merged <https://github.com/qdrant/qdrant/pull/3395> which adds a datetime payload index.\r\n\r\nCurrently, the datetime parser is very strict, only allowing [RFC 3339](https://www.rfc-editor.org/rfc/rfc3339) formats in our REST API. A common format such as `YYYY-MM-DD HH:MM:SS` is currently not accepted.\r\n\r\nIf you'd send the following request:\r\n\r\n```json\r\nPOST collections/test_collection/points/scroll\r\n{\r\n  \"limit\": 10,\r\n  \"filter\": {\r\n    \"must\": [\r\n      {\r\n        \"key\": \"updated\",\r\n        \"datetime_range\": {\r\n          \"gt\": \"2014-01-01T00:00:00\"\r\n        }\r\n      }\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nYou'd receive the following error:\r\n\r\n```\r\nFormat error in JSON body: data did not match any variant of untagged enum Condition at line 1 column 96\r\n```\r\n\r\nThis error is very confusing.\r\n\r\n**Describe the solution you'd like**\r\nWe'd strongly prefer a more descriptive error message instead.\r\n\r\nSomething like this would be a lot better:\r\n\r\n```\r\nFormat error in JSON body: '2014-01-01T00:00:00' does not match any accepted datetime format\r\n```\r\n\r\n**Describe alternatives you've considered**\r\n_None_\r\n\r\n**Additional context**\r\nRelated issue: <https://github.com/qdrant/qdrant/issues/3529>",
              "url": "https://github.com/qdrant/qdrant/issues/3531",
              "tech": [
                "go"
              ],
              "repo_name": "qdrant",
              "repo_owner": "qdrant",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "qdrant#3322",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "qdrant",
              "id": "generated-qdrant",
              "name": "Qdrant",
              "description": "",
              "members": [],
              "display_name": "Qdrant",
              "created_at": "2025-12-16T21:16:32.841Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/qdrant?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "qdrant",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:32.841Z",
            "created_at": "2025-12-16T21:16:32.841Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-qdrant#3322",
              "status": "open",
              "type": "issue",
              "number": 3322,
              "title": "Per-collection metrics for Prometheus",
              "source": {
                "data": {
                  "id": "source-qdrant#3322",
                  "user": {
                    "login": "generall",
                    "id": 1935623,
                    "node_id": "MDQ6VXNlcjE5MzU2MjM=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1935623?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/generall",
                    "html_url": "https://github.com/generall",
                    "followers_url": "https://api.github.com/users/generall/followers",
                    "following_url": "https://api.github.com/users/generall/following{/other_user}",
                    "gists_url": "https://api.github.com/users/generall/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/generall/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/generall/subscriptions",
                    "organizations_url": "https://api.github.com/users/generall/orgs",
                    "repos_url": "https://api.github.com/users/generall/repos",
                    "events_url": "https://api.github.com/users/generall/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/generall/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Per-collection metrics for Prometheus",
                  "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently, all metrics in `/metrics` are global, meaning that it’s impossible to see differences per collection.\r\n\r\nIn addition to that, all our metrics should have per-collection granularity to allow better aggregation in Prometheus, including:\r\n\r\n- point/vector counts\r\n- REST/gRPC requests\r\n\r\n**Describe the solution you'd like**\r\n\r\nExample:\r\n```\r\ngrpc_responses_min_duration_seconds{endpoint=\"/qdrant.Points/Upsert\"} 0.000046\r\ngrpc_responses_min_duration_seconds{endpoint=\"/qdrant.Points/Upsert\",collection=\"my-collection\"} 0.000049\r\ngrpc_responses_min_duration_seconds{endpoint=\"/qdrant.Points/Upsert\",collection=\"my-collection-2\"} 0.000046\r\n```\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCreate dedicated endpoint for each collection `/collections/my-collecton/metrics`\r\nbut feedback from DevOps on this idea was negative.\r\n\r\n**Additional context**\r\n\r\nIt might be beneficial to allow users to disable per-collection output. It is especially relevant if there are a lot of collections and metric response could become huge. But this is a nice-to-have requirement.\r\n\r\n\r\n---\r\n\r\nNote for contributors: Please consider this as tracking issue. If you think that it would be beneficial to split the task into multiple smaller PRs, please you are welcome to do so. Bounty will be rewarded for each PR independently\r\n\r\n",
                  "html_url": "https://github.com/qdrant/qdrant/issues/3322"
                },
                "type": "github"
              },
              "hash": "qdrant/qdrant#3322",
              "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nCurrently, all metrics in `/metrics` are global, meaning that it’s impossible to see differences per collection.\r\n\r\nIn addition to that, all our metrics should have per-collection granularity to allow better aggregation in Prometheus, including:\r\n\r\n- point/vector counts\r\n- REST/gRPC requests\r\n\r\n**Describe the solution you'd like**\r\n\r\nExample:\r\n```\r\ngrpc_responses_min_duration_seconds{endpoint=\"/qdrant.Points/Upsert\"} 0.000046\r\ngrpc_responses_min_duration_seconds{endpoint=\"/qdrant.Points/Upsert\",collection=\"my-collection\"} 0.000049\r\ngrpc_responses_min_duration_seconds{endpoint=\"/qdrant.Points/Upsert\",collection=\"my-collection-2\"} 0.000046\r\n```\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCreate dedicated endpoint for each collection `/collections/my-collecton/metrics`\r\nbut feedback from DevOps on this idea was negative.\r\n\r\n**Additional context**\r\n\r\nIt might be beneficial to allow users to disable per-collection output. It is especially relevant if there are a lot of collections and metric response could become huge. But this is a nice-to-have requirement.\r\n\r\n\r\n---\r\n\r\nNote for contributors: Please consider this as tracking issue. If you think that it would be beneficial to split the task into multiple smaller PRs, please you are welcome to do so. Bounty will be rewarded for each PR independently\r\n\r\n",
              "url": "https://github.com/qdrant/qdrant/issues/3322",
              "tech": [],
              "repo_name": "qdrant",
              "repo_owner": "qdrant",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "outerbase#59",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "outerbase",
              "id": "generated-outerbase",
              "name": "Outerbase",
              "description": "",
              "members": [],
              "display_name": "Outerbase",
              "created_at": "2025-12-16T21:16:31.674Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/outerbase?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "outerbase",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:31.674Z",
            "created_at": "2025-12-16T21:16:31.674Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-outerbase#59",
              "status": "open",
              "type": "issue",
              "number": 59,
              "title": "Database dumps do not work on large databases",
              "source": {
                "data": {
                  "id": "source-outerbase#59",
                  "user": {
                    "login": "Brayden",
                    "id": 1066085,
                    "node_id": "MDQ6VXNlcjEwNjYwODU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1066085?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Brayden",
                    "html_url": "https://github.com/Brayden",
                    "followers_url": "https://api.github.com/users/Brayden/followers",
                    "following_url": "https://api.github.com/users/Brayden/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Brayden/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Brayden/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Brayden/subscriptions",
                    "organizations_url": "https://api.github.com/users/Brayden/orgs",
                    "repos_url": "https://api.github.com/users/Brayden/repos",
                    "events_url": "https://api.github.com/users/Brayden/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Brayden/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Database dumps do not work on large databases",
                  "body": "**Describe the bug**\nIf you try to use any of the database dump endpoints such as SQL, CSV or JSON the data is loaded into memory and then created as a dump file. To support any size database we should investigate enhancements to allow any sized database to be exported. Currently the size limitations are 1GB for Durable Objects with 10GB in the future. Operate under the assumption that we might be attempting to dump a 10GB database into a `.sql` file.\n\nAnother consideration to make is because Durable Objects execute synchronous operations we may need to allow for \"breathing intervals\". An example might be we allow our export operation to run for 5 seconds, and take 5 seconds off if other requests are in a queue, then it can pick up again. The goal here would be to prevent locking the database for long periods of time.\n\nBut then poses the questions: \n1. How do we continue operations that need more than 30 seconds to work?\n2. Where is the data stored as it's being created? (R2, S3, something else)?\n3. How do we deliver that dump information to the user after its completed?\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n1. Hit the `/export/dump` endpoint on a large database\n2. Will eventually fail when the 30 second request response time window closes\n\nRun the following command in Terminal (replace the URL with yours) and if your operation exceeds 30 seconds you should see a failed network response instead of a dump file.\n```\ncurl --location 'https://starbasedb.YOUR-ID-HERE.workers.dev/export/dump' \\\n--header 'Authorization: Bearer ABC123' \\\n--output database_dump.sql\n```\n\nIf you can't create a large enough test database feel free to add code in to `sleep` for 29 seconds before proceeding with the `/export/dump` functional code and should also see the failure.\n\n**Expected behavior**\nAs a user I would expect any and all of the specified data to be dumped out without an error and without partial results. Where it ends up for the user to access if the operation takes more than 30 seconds is up for discussion. Ideally if shorter than 30 seconds it could be returned as our cURL above works today (downloads the file from the response of the origin request), but perhaps after the timeout it continues on uploads it to a destination source to access afterwards?\n\n**Proposed Solution:**\n1. For backups require an R2 binding\n2. Have a `.sql` file that gets created in R2 with the filename like `dump_20240101-170000.sql` where it represents `2024-01-01 17:00:00`\n3. Create the file and continuously append new chunks to it until reaching the end\n4. May need to utilize a DO alarm to continue the work after X time if a timeout occurs & mark where it currently is in the process in internal memory so it can pick up and continue.\n5. Provide a callback URL when the operation is finally completed so users can create custom logic to notify them (e.g. Email, Slack, etc)",
                  "html_url": "https://github.com/outerbase/starbasedb/issues/59"
                },
                "type": "github"
              },
              "hash": "outerbase/starbasedb#59",
              "body": "**Describe the bug**\nIf you try to use any of the database dump endpoints such as SQL, CSV or JSON the data is loaded into memory and then created as a dump file. To support any size database we should investigate enhancements to allow any sized database to be exported. Currently the size limitations are 1GB for Durable Objects with 10GB in the future. Operate under the assumption that we might be attempting to dump a 10GB database into a `.sql` file.\n\nAnother consideration to make is because Durable Objects execute synchronous operations we may need to allow for \"breathing intervals\". An example might be we allow our export operation to run for 5 seconds, and take 5 seconds off if other requests are in a queue, then it can pick up again. The goal here would be to prevent locking the database for long periods of time.\n\nBut then poses the questions: \n1. How do we continue operations that need more than 30 seconds to work?\n2. Where is the data stored as it's being created? (R2, S3, something else)?\n3. How do we deliver that dump information to the user after its completed?\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n1. Hit the `/export/dump` endpoint on a large database\n2. Will eventually fail when the 30 second request response time window closes\n\nRun the following command in Terminal (replace the URL with yours) and if your operation exceeds 30 seconds you should see a failed network response instead of a dump file.\n```\ncurl --location 'https://starbasedb.YOUR-ID-HERE.workers.dev/export/dump' \\\n--header 'Authorization: Bearer ABC123' \\\n--output database_dump.sql\n```\n\nIf you can't create a large enough test database feel free to add code in to `sleep` for 29 seconds before proceeding with the `/export/dump` functional code and should also see the failure.\n\n**Expected behavior**\nAs a user I would expect any and all of the specified data to be dumped out without an error and without partial results. Where it ends up for the user to access if the operation takes more than 30 seconds is up for discussion. Ideally if shorter than 30 seconds it could be returned as our cURL above works today (downloads the file from the response of the origin request), but perhaps after the timeout it continues on uploads it to a destination source to access afterwards?\n\n**Proposed Solution:**\n1. For backups require an R2 binding\n2. Have a `.sql` file that gets created in R2 with the filename like `dump_20240101-170000.sql` where it represents `2024-01-01 17:00:00`\n3. Create the file and continuously append new chunks to it until reaching the end\n4. May need to utilize a DO alarm to continue the work after X time if a timeout occurs & mark where it currently is in the process in internal memory so it can pick up and continue.\n5. Provide a callback URL when the operation is finally completed so users can create custom logic to notify them (e.g. Email, Slack, etc)",
              "url": "https://github.com/outerbase/starbasedb/issues/59",
              "tech": [
                "go"
              ],
              "repo_name": "starbasedb",
              "repo_owner": "outerbase",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "outerbase#72",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "outerbase",
              "id": "generated-outerbase",
              "name": "Outerbase",
              "description": "",
              "members": [],
              "display_name": "Outerbase",
              "created_at": "2025-12-16T21:16:32.841Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/outerbase?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "outerbase",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:32.841Z",
            "created_at": "2025-12-16T21:16:32.841Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-outerbase#72",
              "status": "open",
              "type": "issue",
              "number": 72,
              "title": "Replicate data from external source to internal source with a Plugin",
              "source": {
                "data": {
                  "id": "source-outerbase#72",
                  "user": {
                    "login": "Brayden",
                    "id": 1066085,
                    "node_id": "MDQ6VXNlcjEwNjYwODU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1066085?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Brayden",
                    "html_url": "https://github.com/Brayden",
                    "followers_url": "https://api.github.com/users/Brayden/followers",
                    "following_url": "https://api.github.com/users/Brayden/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Brayden/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Brayden/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Brayden/subscriptions",
                    "organizations_url": "https://api.github.com/users/Brayden/orgs",
                    "repos_url": "https://api.github.com/users/Brayden/repos",
                    "events_url": "https://api.github.com/users/Brayden/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Brayden/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Replicate data from external source to internal source with a Plugin",
                  "body": "**Is your feature request related to a problem? Please describe.**\nStarbaseDB instances support by default an internal database (SQLite offered by the Durable Object) as well as an optional external data source. External data sources can be powered in one of two ways, both by providing values in the `wrangler.toml` file of the project.\n\n- Outerbase API Key\n- Connection details of the database\n\n<img width=\"481\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/485d4b88-a7f8-432d-9f29-d3239a6e6577\" />\n\n**Describe the solution you'd like**\nWhat would be beneficial for some use cases is the ability to bring in an external data source (e.g. a Postgres on Supabase) and have a pull mechanism where data can be brought into the internal DO SQLite so that the instance serves as a close-to-edge replica that can be queried alternatively to querying the Supabase Postgres instance.\n\n**Describe alternatives you've considered**\n- Considering the pull vs push mechanism. A pull mechanism seems to be a better global solution where a push mechanism would be required to live elsewhere on a per provider basis.\n\n**Additional context**\n- Might be beneficial for users to be able to define in the plugin what intervals data should be pulled at\n- Might be beneficial to allow users to define which tables should have data pulled into it (perhaps not all tables need replicated)\n- Likely need a way to know for each table what the last queried items were so you can do append-only type polling for new data. Does a user need to define a column to base this on (e.g. `id` or `created_at` columns perhaps)?\n",
                  "html_url": "https://github.com/outerbase/starbasedb/issues/72"
                },
                "type": "github"
              },
              "hash": "outerbase/starbasedb#72",
              "body": "**Is your feature request related to a problem? Please describe.**\nStarbaseDB instances support by default an internal database (SQLite offered by the Durable Object) as well as an optional external data source. External data sources can be powered in one of two ways, both by providing values in the `wrangler.toml` file of the project.\n\n- Outerbase API Key\n- Connection details of the database\n\n<img width=\"481\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/485d4b88-a7f8-432d-9f29-d3239a6e6577\" />\n\n**Describe the solution you'd like**\nWhat would be beneficial for some use cases is the ability to bring in an external data source (e.g. a Postgres on Supabase) and have a pull mechanism where data can be brought into the internal DO SQLite so that the instance serves as a close-to-edge replica that can be queried alternatively to querying the Supabase Postgres instance.\n\n**Describe alternatives you've considered**\n- Considering the pull vs push mechanism. A pull mechanism seems to be a better global solution where a push mechanism would be required to live elsewhere on a per provider basis.\n\n**Additional context**\n- Might be beneficial for users to be able to define in the plugin what intervals data should be pulled at\n- Might be beneficial to allow users to define which tables should have data pulled into it (perhaps not all tables need replicated)\n- Likely need a way to know for each table what the last queried items were so you can do append-only type polling for new data. Does a user need to define a column to base this on (e.g. `id` or `created_at` columns perhaps)?\n",
              "url": "https://github.com/outerbase/starbasedb/issues/72",
              "tech": [],
              "repo_name": "starbasedb",
              "repo_owner": "outerbase",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#378",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:31.553Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:31.553Z",
            "created_at": "2025-12-16T21:16:31.553Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#378",
              "status": "open",
              "type": "issue",
              "number": 378,
              "title": "Create workflow: \"Supplier invoice reconciliation\"",
              "source": {
                "data": {
                  "id": "source-mediar-ai#378",
                  "user": {
                    "login": "m13v",
                    "id": 104702220,
                    "node_id": "U_kgDOBj2hDA",
                    "avatar_url": "https://avatars.githubusercontent.com/u/104702220?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/m13v",
                    "html_url": "https://github.com/m13v",
                    "followers_url": "https://api.github.com/users/m13v/followers",
                    "following_url": "https://api.github.com/users/m13v/following{/other_user}",
                    "gists_url": "https://api.github.com/users/m13v/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/m13v/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/m13v/subscriptions",
                    "organizations_url": "https://api.github.com/users/m13v/orgs",
                    "repos_url": "https://api.github.com/users/m13v/repos",
                    "events_url": "https://api.github.com/users/m13v/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/m13v/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Create workflow: \"Supplier invoice reconciliation\"",
                  "body": "/bounty 100\n\n1. Download the app https://app.mediar.ai/\n2. Create 'New' workflow\n3. Prompt AI in the chat to create it\n4. Adjust and correct the steps to make it work reliably\n5. Deploy to the dashboard\n6. Ask me questions in the comments please\n\nRequirements:\n- CHECK IF ANYONE ALREADY started working on the issue in the comments below\n- comment /attempt to start working on the issue\n- as part of submission you need to create a screen recording to show that the workflow works on your end, end-to-end\n\nOutput table destination:\nhttps://docs.google.com/spreadsheets/d/1P7tioZ7sC3GdP5aOSrBhYKb_svkFjDbTransEqDMqvA/edit?gid=0#gid=0",
                  "html_url": "https://github.com/mediar-ai/terminator/issues/378"
                },
                "type": "github"
              },
              "hash": "mediar-ai/terminator#378",
              "body": "/bounty 100\n\n1. Download the app https://app.mediar.ai/\n2. Create 'New' workflow\n3. Prompt AI in the chat to create it\n4. Adjust and correct the steps to make it work reliably\n5. Deploy to the dashboard\n6. Ask me questions in the comments please\n\nRequirements:\n- CHECK IF ANYONE ALREADY started working on the issue in the comments below\n- comment /attempt to start working on the issue\n- as part of submission you need to create a screen recording to show that the workflow works on your end, end-to-end\n\nOutput table destination:\nhttps://docs.google.com/spreadsheets/d/1P7tioZ7sC3GdP5aOSrBhYKb_svkFjDbTransEqDMqvA/edit?gid=0#gid=0",
              "url": "https://github.com/mediar-ai/terminator/issues/378",
              "tech": [
                "go"
              ],
              "repo_name": "terminator",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#357",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:32.142Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:32.142Z",
            "created_at": "2025-12-16T21:16:32.142Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#357",
              "status": "open",
              "type": "issue",
              "number": 357,
              "title": "create a TS workflow that delete the oldest version of PyPI before publishing to PyPI",
              "source": {
                "data": {
                  "id": "source-mediar-ai#357",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "create a TS workflow that delete the oldest version of PyPI before publishing to PyPI",
                  "body": "\n/bounty 100 \n\n\nproblem:\n- cannot publish pypi bcs there is limit of versions \n- there is no api / cli to do that so we need to do this w UI \n\ndefinition of done:\n- before publishing to pypi the gh action runs a TS workflow that delete oldest pypi release \n- share video it works (use your own pypi account for the video idk)\n\n\n\n",
                  "html_url": "https://github.com/mediar-ai/terminator/issues/357"
                },
                "type": "github"
              },
              "hash": "mediar-ai/terminator#357",
              "body": "\n/bounty 100 \n\n\nproblem:\n- cannot publish pypi bcs there is limit of versions \n- there is no api / cli to do that so we need to do this w UI \n\ndefinition of done:\n- before publishing to pypi the gh action runs a TS workflow that delete oldest pypi release \n- share video it works (use your own pypi account for the video idk)\n\n\n\n",
              "url": "https://github.com/mediar-ai/terminator/issues/357",
              "tech": [
                "go"
              ],
              "repo_name": "terminator",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#353",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:33.288Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:33.288Z",
            "created_at": "2025-12-16T21:16:33.288Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#353",
              "status": "open",
              "type": "issue",
              "number": 353,
              "title": "create a typescript workflow that test the browser script bridge",
              "source": {
                "data": {
                  "id": "source-mediar-ai#353",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "create a typescript workflow that test the browser script bridge",
                  "body": "\n/bounty 100\n\nshould be able to reproduce this:\n\n```\nFailed to enable debugger domains: Debugger is not attached to the tab with id: 648628626.\n```\n\nexample ts workflow:\n\nhttps://github.com/mediar-ai/terminator/tree/main/examples/simple_notepad_workflow\n\n\ndefinition of done:\n- uses workflow sdk, terminator sdk, can be ran one liner using terminator cli \n- test different edge cases of the rust bridge using UI (executeBrowserScript function)\n- PR with video showing it runs \n- make it run in our github action \n\n\n\n\n\n\n\n",
                  "html_url": "https://github.com/mediar-ai/terminator/issues/353"
                },
                "type": "github"
              },
              "hash": "mediar-ai/terminator#353",
              "body": "\n/bounty 100\n\nshould be able to reproduce this:\n\n```\nFailed to enable debugger domains: Debugger is not attached to the tab with id: 648628626.\n```\n\nexample ts workflow:\n\nhttps://github.com/mediar-ai/terminator/tree/main/examples/simple_notepad_workflow\n\n\ndefinition of done:\n- uses workflow sdk, terminator sdk, can be ran one liner using terminator cli \n- test different edge cases of the rust bridge using UI (executeBrowserScript function)\n- PR with video showing it runs \n- make it run in our github action \n\n\n\n\n\n\n\n",
              "url": "https://github.com/mediar-ai/terminator/issues/353",
              "tech": [],
              "repo_name": "terminator",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#352",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:34.271Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:34.271Z",
            "created_at": "2025-12-16T21:16:34.271Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#352",
              "status": "open",
              "type": "issue",
              "number": 352,
              "title": "increase success rate of browser extension workflow",
              "source": {
                "data": {
                  "id": "source-mediar-ai#352",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "increase success rate of browser extension workflow",
                  "body": "/bounty 150\n\nproblem:\n\nbrowser extension workflow does not always work:\n- for example if user has multiple monitors or different environment\n- on slow machines workflow fails around step 2-5. probably has to do with hardcoded delays, machine doesn't react in time, can't validate typing input, etc\n- when chrome window opens up a user might have a profile selection window instead of a browser window which will prevent execution\n\ndefinition of done:\n- keep it a single yaml file (not typescript)\n- add more edge case in current implementation of the workflow, send a PR\n- provide a video showing it covers different environments like\n  - slow computer (like cheap VM)\n  - multiple monitors \n  - chrome profiles handling \n  - anything else useful to cover\n\n\n```yaml\n\n\nvariables:\n  extension_dir:\n    default: \"%TEMP%\\\\terminator-bridge\"\n    label: Folder to load (will be created by the download step)\n    type: string\n  release_url:\n    default: https://github.com/mediar-ai/terminator/releases/latest/download/terminator-browser-extension.zip\n    label: GitHub Release asset URL (zip)\n    type: string\n  zip_path:\n    default: \"%TEMP%\\\\terminator-browser-extension.zip\"\n    label: Path to downloaded zip\n    type: string\nselectors:\n  address_bar: role:Edit|name:Address and search bar\n  dev_mode_toggle: role:Button|name:Developer mode\n  extensions_doc: role:Document|name:Extensions\n  folder_field: \"role:Edit|name:Folder:\"\n  load_unpacked: role:Button|name:Load unpacked\n  reload_button: role:Button|name:Reload\n  select_folder_btn: role:Button|name:Select Folder\nsteps:\n  - arguments:\n      app_name: Chrome\n    name: Open Chrome\n    tool_name: open_application\n    id: open_chrome\n  - arguments:\n      engine: javascript\n      run: >\n        const fs = require('fs');\n\n        const path = require('path');\n\n        const os = require('os');\n\n        (async () => {\n          const url = \"${{release_url}}\";\n          if (!url || !url.trim()) throw new Error('release_url is empty');\n          const isWin = process.platform === 'win32';\n          const tmp = isWin ? (process.env.TEMP || os.tmpdir()) : os.tmpdir();\n          const zipPath = isWin ? path.join(tmp, 'terminator-browser-extension.zip') : path.join(tmp, 'terminator-browser-extension.zip');\n          const destDir = isWin ? path.join(tmp, 'terminator-bridge') : path.join(tmp, 'terminator-bridge');\n          const existedBefore = fs.existsSync(destDir);\n          try { fs.rmSync(destDir, { recursive: true, force: true }); } catch (_) {}\n          try { fs.mkdirSync(destDir, { recursive: true }); } catch (e) { throw new Error('Failed to create dest dir: ' + e.message); }\n\n          const res = await fetch(url);\n          if (!res.ok) throw new Error(`Download failed: ${res.status} ${res.statusText}`);\n          const arrayBuf = await res.arrayBuffer();\n          fs.writeFileSync(zipPath, Buffer.from(arrayBuf));\n\n          // Export values via ::set-env for the workflow engine AND return set_env for robust propagation\n          console.log(`::set-env name=zip_path::${zipPath}`);\n          console.log(`::set-env name=extension_dir::${destDir}`);\n          console.log(`::set-env name=is_update_mode::${existedBefore}`);\n          return { set_env: { zip_path: zipPath, extension_dir: destDir, is_update_mode: existedBefore } };\n        })();\n    delay_ms: 200\n    id: step_0\n    tool_name: run_command\n  - arguments:\n      run: |\n        $ErrorActionPreference = 'Stop'\n        # Avoid template substitution issues: compute paths directly\n        $zip = Join-Path $env:TEMP 'terminator-browser-extension.zip'\n        $dest = Join-Path $env:TEMP 'terminator-bridge'\n        if (Test-Path $dest) { Remove-Item -Recurse -Force $dest }\n        New-Item -ItemType Directory -Force -Path $dest | Out-Null\n        Expand-Archive -Path $zip -DestinationPath $dest -Force\n      shell: powershell\n    delay_ms: 400\n    id: step_1\n    tool_name: run_command\n  - arguments:\n      engine: javascript\n      run: >\n        const fs = require('fs');\n\n        const path = require('path');\n\n        const os = require('os');\n\n        (async () => {\n          const isWin = process.platform === 'win32';\n          const root = isWin ? path.join(process.env.TEMP || os.tmpdir(), 'terminator-bridge') : path.join(os.tmpdir(), 'terminator-bridge');\n          const stack = [root];\n          let picked = null;\n          while (stack.length) {\n            const dir = stack.pop();\n            let entries;\n            try { entries = fs.readdirSync(dir, { withFileTypes: true }); } catch (_) { continue; }\n            if (entries.some(e => e.isFile && e.name.toLowerCase() === 'manifest.json' || (!e.isFile && !e.isDirectory && e.name && e.name.toLowerCase() === 'manifest.json'))) {\n              picked = dir; break;\n            }\n            for (const e of entries) {\n              if ((e.isDirectory && e.isDirectory()) || (e.isDirectory === true)) {\n                stack.push(path.join(dir, e.name));\n              }\n            }\n          }\n          if (!picked) {\n            console.log(`::set-env name=extension_dir_text::${root}`);\n            return { set_env: { extension_dir_text: root } };\n          }\n          console.log(`::set-env name=extension_dir_text::${picked}`);\n          return { set_env: { extension_dir_text: picked } };\n        })();\n    continue_on_error: false\n    delay_ms: 100\n    id: step_2\n    tool_name: run_command\n  - arguments:\n      url: chrome://extensions\n    delay_ms: 1000\n    id: step_3\n    tool_name: navigate_browser\n  - arguments:\n      condition: visible\n      selector: ${{ selectors.address_bar }}\n      timeout_ms: 15000\n    continue_on_error: true\n    id: step_4\n    tool_name: wait_for_element\n  - arguments:\n      selector: ${{ selectors.address_bar }}\n    continue_on_error: true\n    id: step_5\n    tool_name: click_element\n  - arguments:\n      clear_before_typing: true\n      selector: ${{ selectors.address_bar }}\n      text_to_type: chrome://extensions\n    continue_on_error: true\n    id: step_6\n    tool_name: type_into_element\n  - arguments:\n      key: \"{Enter}\"\n    continue_on_error: true\n    delay_ms: 800\n    id: step_7\n    tool_name: press_key_global\n  - arguments:\n      engine: javascript\n      run: >\n        // Use terminator.js via global 'desktop'\n\n        const toggleSel = \"role:Button|name:Developer mode\";\n\n        const loadSel = \"role:Button|name:Load unpacked\";\n\n\n        // Wait for Developer mode toggle to appear\n\n        const devToggle = await desktop.locator(toggleSel).first(30000);\n\n        // Presence-based check: if Load unpacked is not visible yet, toggle Dev\n        Mode once\n\n        let loadVisible = false;\n\n        try { await desktop.locator(loadSel).first(1500); loadVisible = true; }\n        catch (_) {}\n\n        if (!loadVisible) {\n          await devToggle.click();\n          await sleep(300);\n        }\n\n        // No explicit click on Load unpacked here; later steps handle it\n    continue_on_error: true\n    delay_ms: 200\n    id: step_8\n    tool_name: run_command\n  - arguments:\n      engine: javascript\n      run: >\n        // Find and remove only Terminator Bridge extension\n\n        const extensionName = \"Terminator Bridge\";\n\n\n        try {\n          // Wait a bit for extensions page to load\n          await sleep(1000);\n\n          // Look for all extension cards on the page\n          const allElements = await desktop.locator(\"role:Group\").all();\n          log(`Found ${allElements.length} groups on extensions page`);\n\n          let terminatorFound = false;\n\n          // Search through elements to find Terminator Bridge\n          for (let element of allElements) {\n            try {\n              const name = await element.name();\n              const text = await element.value();\n\n              // Check if this element contains \"Terminator Bridge\" text\n              if ((name && name.includes(extensionName)) || (text && text.includes(extensionName))) {\n                log(`Found Terminator Bridge extension card`);\n                terminatorFound = true;\n\n                // Look for Remove button within this specific card\n                // Try to find the Remove button that's a child of this card\n                const removeButton = await element.locator(\"role:Button|name:Remove\").first();\n\n                if (removeButton) {\n                  log(`Found Remove button for Terminator Bridge, clicking it`);\n                  await removeButton.click();\n                  await sleep(500);\n\n                  // Confirm removal in the dialog\n                  await desktop.press_key(\"{Enter}\");\n                  log(`Confirmed removal of Terminator Bridge`);\n                  await sleep(1000);\n                  break;\n                } else {\n                  log(`Remove button not found in Terminator Bridge card`);\n                }\n              }\n            } catch (e) {\n              // Skip elements that can't be read\n              continue;\n            }\n          }\n\n          if (!terminatorFound) {\n            log(`Terminator Bridge extension not found - probably not installed`);\n          }\n\n        } catch (error) {\n          log(`Error while trying to remove old extension: ${error.message}`);\n          log(`Continuing with installation anyway...`);\n        }\n    continue_on_error: true\n    delay_ms: 500\n    id: step_9\n    tool_name: run_command\n  - arguments:\n      selector: ${{ selectors.load_unpacked }}\n    continue_on_error: false\n    delay_ms: 300\n    id: step_10\n    tool_name: click_element\n  - arguments:\n      condition: exists\n      selector: ${{ selectors.folder_field }}\n      timeout_ms: 3000\n    continue_on_error: true\n    id: step_11\n    tool_name: wait_for_element\n  - arguments:\n      clear_before_typing: true\n      selector: ${{ selectors.folder_field }}\n      text_to_type: ${{env.extension_dir_text}}\n    continue_on_error: true\n    id: step_12\n    tool_name: type_into_element\n  - arguments:\n      selector: ${{ selectors.select_folder_btn }}\n    continue_on_error: true\n    delay_ms: 1200\n    id: step_13\n    tool_name: click_element\n  - arguments:\n      condition: exists\n      selector: ${{ selectors.reload_button }}\n      timeout_ms: 15000\n    id: step_14\n    tool_name: wait_for_element\n  - arguments:\n      continue1_on_error: true\n      selector: role:Window|name:Google Chrome\n    name: Close Chrome\n    tool_name: close_element\n    id: close_chrome\nstop_on_error: true\n```\n\ni think this is roughly the same\n\nhttps://github.com/mediar-ai/terminator/blob/main/crates/terminator/browser-extension/install_chrome_extension_ui.yml\n",
                  "html_url": "https://github.com/mediar-ai/terminator/issues/352"
                },
                "type": "github"
              },
              "hash": "mediar-ai/terminator#352",
              "body": "/bounty 150\n\nproblem:\n\nbrowser extension workflow does not always work:\n- for example if user has multiple monitors or different environment\n- on slow machines workflow fails around step 2-5. probably has to do with hardcoded delays, machine doesn't react in time, can't validate typing input, etc\n- when chrome window opens up a user might have a profile selection window instead of a browser window which will prevent execution\n\ndefinition of done:\n- keep it a single yaml file (not typescript)\n- add more edge case in current implementation of the workflow, send a PR\n- provide a video showing it covers different environments like\n  - slow computer (like cheap VM)\n  - multiple monitors \n  - chrome profiles handling \n  - anything else useful to cover\n\n\n```yaml\n\n\nvariables:\n  extension_dir:\n    default: \"%TEMP%\\\\terminator-bridge\"\n    label: Folder to load (will be created by the download step)\n    type: string\n  release_url:\n    default: https://github.com/mediar-ai/terminator/releases/latest/download/terminator-browser-extension.zip\n    label: GitHub Release asset URL (zip)\n    type: string\n  zip_path:\n    default: \"%TEMP%\\\\terminator-browser-extension.zip\"\n    label: Path to downloaded zip\n    type: string\nselectors:\n  address_bar: role:Edit|name:Address and search bar\n  dev_mode_toggle: role:Button|name:Developer mode\n  extensions_doc: role:Document|name:Extensions\n  folder_field: \"role:Edit|name:Folder:\"\n  load_unpacked: role:Button|name:Load unpacked\n  reload_button: role:Button|name:Reload\n  select_folder_btn: role:Button|name:Select Folder\nsteps:\n  - arguments:\n      app_name: Chrome\n    name: Open Chrome\n    tool_name: open_application\n    id: open_chrome\n  - arguments:\n      engine: javascript\n      run: >\n        const fs = require('fs');\n\n        const path = require('path');\n\n        const os = require('os');\n\n        (async () => {\n          const url = \"${{release_url}}\";\n          if (!url || !url.trim()) throw new Error('release_url is empty');\n          const isWin = process.platform === 'win32';\n          const tmp = isWin ? (process.env.TEMP || os.tmpdir()) : os.tmpdir();\n          const zipPath = isWin ? path.join(tmp, 'terminator-browser-extension.zip') : path.join(tmp, 'terminator-browser-extension.zip');\n          const destDir = isWin ? path.join(tmp, 'terminator-bridge') : path.join(tmp, 'terminator-bridge');\n          const existedBefore = fs.existsSync(destDir);\n          try { fs.rmSync(destDir, { recursive: true, force: true }); } catch (_) {}\n          try { fs.mkdirSync(destDir, { recursive: true }); } catch (e) { throw new Error('Failed to create dest dir: ' + e.message); }\n\n          const res = await fetch(url);\n          if (!res.ok) throw new Error(`Download failed: ${res.status} ${res.statusText}`);\n          const arrayBuf = await res.arrayBuffer();\n          fs.writeFileSync(zipPath, Buffer.from(arrayBuf));\n\n          // Export values via ::set-env for the workflow engine AND return set_env for robust propagation\n          console.log(`::set-env name=zip_path::${zipPath}`);\n          console.log(`::set-env name=extension_dir::${destDir}`);\n          console.log(`::set-env name=is_update_mode::${existedBefore}`);\n          return { set_env: { zip_path: zipPath, extension_dir: destDir, is_update_mode: existedBefore } };\n        })();\n    delay_ms: 200\n    id: step_0\n    tool_name: run_command\n  - arguments:\n      run: |\n        $ErrorActionPreference = 'Stop'\n        # Avoid template substitution issues: compute paths directly\n        $zip = Join-Path $env:TEMP 'terminator-browser-extension.zip'\n        $dest = Join-Path $env:TEMP 'terminator-bridge'\n        if (Test-Path $dest) { Remove-Item -Recurse -Force $dest }\n        New-Item -ItemType Directory -Force -Path $dest | Out-Null\n        Expand-Archive -Path $zip -DestinationPath $dest -Force\n      shell: powershell\n    delay_ms: 400\n    id: step_1\n    tool_name: run_command\n  - arguments:\n      engine: javascript\n      run: >\n        const fs = require('fs');\n\n        const path = require('path');\n\n        const os = require('os');\n\n        (async () => {\n          const isWin = process.platform === 'win32';\n          const root = isWin ? path.join(process.env.TEMP || os.tmpdir(), 'terminator-bridge') : path.join(os.tmpdir(), 'terminator-bridge');\n          const stack = [root];\n          let picked = null;\n          while (stack.length) {\n            const dir = stack.pop();\n            let entries;\n            try { entries = fs.readdirSync(dir, { withFileTypes: true }); } catch (_) { continue; }\n            if (entries.some(e => e.isFile && e.name.toLowerCase() === 'manifest.json' || (!e.isFile && !e.isDirectory && e.name && e.name.toLowerCase() === 'manifest.json'))) {\n              picked = dir; break;\n            }\n            for (const e of entries) {\n              if ((e.isDirectory && e.isDirectory()) || (e.isDirectory === true)) {\n                stack.push(path.join(dir, e.name));\n              }\n            }\n          }\n          if (!picked) {\n            console.log(`::set-env name=extension_dir_text::${root}`);\n            return { set_env: { extension_dir_text: root } };\n          }\n          console.log(`::set-env name=extension_dir_text::${picked}`);\n          return { set_env: { extension_dir_text: picked } };\n        })();\n    continue_on_error: false\n    delay_ms: 100\n    id: step_2\n    tool_name: run_command\n  - arguments:\n      url: chrome://extensions\n    delay_ms: 1000\n    id: step_3\n    tool_name: navigate_browser\n  - arguments:\n      condition: visible\n      selector: ${{ selectors.address_bar }}\n      timeout_ms: 15000\n    continue_on_error: true\n    id: step_4\n    tool_name: wait_for_element\n  - arguments:\n      selector: ${{ selectors.address_bar }}\n    continue_on_error: true\n    id: step_5\n    tool_name: click_element\n  - arguments:\n      clear_before_typing: true\n      selector: ${{ selectors.address_bar }}\n      text_to_type: chrome://extensions\n    continue_on_error: true\n    id: step_6\n    tool_name: type_into_element\n  - arguments:\n      key: \"{Enter}\"\n    continue_on_error: true\n    delay_ms: 800\n    id: step_7\n    tool_name: press_key_global\n  - arguments:\n      engine: javascript\n      run: >\n        // Use terminator.js via global 'desktop'\n\n        const toggleSel = \"role:Button|name:Developer mode\";\n\n        const loadSel = \"role:Button|name:Load unpacked\";\n\n\n        // Wait for Developer mode toggle to appear\n\n        const devToggle = await desktop.locator(toggleSel).first(30000);\n\n        // Presence-based check: if Load unpacked is not visible yet, toggle Dev\n        Mode once\n\n        let loadVisible = false;\n\n        try { await desktop.locator(loadSel).first(1500); loadVisible = true; }\n        catch (_) {}\n\n        if (!loadVisible) {\n          await devToggle.click();\n          await sleep(300);\n        }\n\n        // No explicit click on Load unpacked here; later steps handle it\n    continue_on_error: true\n    delay_ms: 200\n    id: step_8\n    tool_name: run_command\n  - arguments:\n      engine: javascript\n      run: >\n        // Find and remove only Terminator Bridge extension\n\n        const extensionName = \"Terminator Bridge\";\n\n\n        try {\n          // Wait a bit for extensions page to load\n          await sleep(1000);\n\n          // Look for all extension cards on the page\n          const allElements = await desktop.locator(\"role:Group\").all();\n          log(`Found ${allElements.length} groups on extensions page`);\n\n          let terminatorFound = false;\n\n          // Search through elements to find Terminator Bridge\n          for (let element of allElements) {\n            try {\n              const name = await element.name();\n              const text = await element.value();\n\n              // Check if this element contains \"Terminator Bridge\" text\n              if ((name && name.includes(extensionName)) || (text && text.includes(extensionName))) {\n                log(`Found Terminator Bridge extension card`);\n                terminatorFound = true;\n\n                // Look for Remove button within this specific card\n                // Try to find the Remove button that's a child of this card\n                const removeButton = await element.locator(\"role:Button|name:Remove\").first();\n\n                if (removeButton) {\n                  log(`Found Remove button for Terminator Bridge, clicking it`);\n                  await removeButton.click();\n                  await sleep(500);\n\n                  // Confirm removal in the dialog\n                  await desktop.press_key(\"{Enter}\");\n                  log(`Confirmed removal of Terminator Bridge`);\n                  await sleep(1000);\n                  break;\n                } else {\n                  log(`Remove button not found in Terminator Bridge card`);\n                }\n              }\n            } catch (e) {\n              // Skip elements that can't be read\n              continue;\n            }\n          }\n\n          if (!terminatorFound) {\n            log(`Terminator Bridge extension not found - probably not installed`);\n          }\n\n        } catch (error) {\n          log(`Error while trying to remove old extension: ${error.message}`);\n          log(`Continuing with installation anyway...`);\n        }\n    continue_on_error: true\n    delay_ms: 500\n    id: step_9\n    tool_name: run_command\n  - arguments:\n      selector: ${{ selectors.load_unpacked }}\n    continue_on_error: false\n    delay_ms: 300\n    id: step_10\n    tool_name: click_element\n  - arguments:\n      condition: exists\n      selector: ${{ selectors.folder_field }}\n      timeout_ms: 3000\n    continue_on_error: true\n    id: step_11\n    tool_name: wait_for_element\n  - arguments:\n      clear_before_typing: true\n      selector: ${{ selectors.folder_field }}\n      text_to_type: ${{env.extension_dir_text}}\n    continue_on_error: true\n    id: step_12\n    tool_name: type_into_element\n  - arguments:\n      selector: ${{ selectors.select_folder_btn }}\n    continue_on_error: true\n    delay_ms: 1200\n    id: step_13\n    tool_name: click_element\n  - arguments:\n      condition: exists\n      selector: ${{ selectors.reload_button }}\n      timeout_ms: 15000\n    id: step_14\n    tool_name: wait_for_element\n  - arguments:\n      continue1_on_error: true\n      selector: role:Window|name:Google Chrome\n    name: Close Chrome\n    tool_name: close_element\n    id: close_chrome\nstop_on_error: true\n```\n\ni think this is roughly the same\n\nhttps://github.com/mediar-ai/terminator/blob/main/crates/terminator/browser-extension/install_chrome_extension_ui.yml\n",
              "url": "https://github.com/mediar-ai/terminator/issues/352",
              "tech": [],
              "repo_name": "terminator",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#315",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:34.674Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:34.674Z",
            "created_at": "2025-12-16T21:16:34.674Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#315",
              "status": "open",
              "type": "issue",
              "number": 315,
              "title": "[bounty] test and showcase the new gemini computer use model with terminator MCP",
              "source": {
                "data": {
                  "id": "source-mediar-ai#315",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] test and showcase the new gemini computer use model with terminator MCP",
                  "body": "basic idea: try the new gemini CU model:\n\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/computer-use\n\nand try to build some workflows like reading PDFs or spreadsheets on file system and doing data entry in another software \n\n\ngoal: evaluate if their model adds worthwhile performance gain by using both vision and accessibility to build workflow compared to claude etc\n\n\nrequirements:\n- record yourself building the workflow \n- use the new model somehow (gemini CLI + terminator MCP + gemini model configured for example, or build a custom MCP client in python/js, high preference for JS)\n- use terminator MCP server\n\n\nbonus:\n- professional video level that can be published on social\n\n\nplease share your plan before building things, one-prompt PRs will be banned \n\n/bounty 200\n",
                  "html_url": "https://github.com/mediar-ai/terminator/issues/315"
                },
                "type": "github"
              },
              "hash": "mediar-ai/terminator#315",
              "body": "basic idea: try the new gemini CU model:\n\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/computer-use\n\nand try to build some workflows like reading PDFs or spreadsheets on file system and doing data entry in another software \n\n\ngoal: evaluate if their model adds worthwhile performance gain by using both vision and accessibility to build workflow compared to claude etc\n\n\nrequirements:\n- record yourself building the workflow \n- use the new model somehow (gemini CLI + terminator MCP + gemini model configured for example, or build a custom MCP client in python/js, high preference for JS)\n- use terminator MCP server\n\n\nbonus:\n- professional video level that can be published on social\n\n\nplease share your plan before building things, one-prompt PRs will be banned \n\n/bounty 200\n",
              "url": "https://github.com/mediar-ai/terminator/issues/315",
              "tech": [],
              "repo_name": "terminator",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1626",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:34.954Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:34.954Z",
            "created_at": "2025-12-16T21:16:34.954Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1626",
              "status": "open",
              "type": "issue",
              "number": 1626,
              "title": "[bounty] $400 fix audio device randomly stopping sometimes",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1626",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] $400 fix audio device randomly stopping sometimes",
                  "body": "\ndisplay audio / microphone randomly stops on mac sometimes \n\nhad it running for 48h now and display audio somehow stopped a bit later after 50h\n\n/bounty 400 \n\n@EzraEllette any idea?\n\n\ni suggest finding way to reproduce and then some automated test and a fix ",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1626"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1626",
              "body": "\ndisplay audio / microphone randomly stops on mac sometimes \n\nhad it running for 48h now and display audio somehow stopped a bit later after 50h\n\n/bounty 400 \n\n@EzraEllette any idea?\n\n\ni suggest finding way to reproduce and then some automated test and a fix ",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1626",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1560",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:35.111Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.111Z",
            "created_at": "2025-12-16T21:16:35.111Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1560",
              "status": "open",
              "type": "issue",
              "number": 1560,
              "title": "[feature]  Implement Session Tracking for Application and Window Usage",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1560",
                  "user": {
                    "login": "rodgomesc",
                    "id": 4893591,
                    "node_id": "MDQ6VXNlcjQ4OTM1OTE=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/4893591?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/rodgomesc",
                    "html_url": "https://github.com/rodgomesc",
                    "followers_url": "https://api.github.com/users/rodgomesc/followers",
                    "following_url": "https://api.github.com/users/rodgomesc/following{/other_user}",
                    "gists_url": "https://api.github.com/users/rodgomesc/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/rodgomesc/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/rodgomesc/subscriptions",
                    "organizations_url": "https://api.github.com/users/rodgomesc/orgs",
                    "repos_url": "https://api.github.com/users/rodgomesc/repos",
                    "events_url": "https://api.github.com/users/rodgomesc/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/rodgomesc/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[feature]  Implement Session Tracking for Application and Window Usage",
                  "body": "\n## Challenge\n\nI'm working on an application that requires accurate session tracking to analyze user behavior patterns,  and provide meaningful insights across different applications and windows. \n\nIn the existing Screenpipe architecture, OCR data is stored with only timestamp information, lacking precise session boundaries (start/end times) for application usage tracking. This limitation prevents us from properly segmenting user activity into coherent sessions.\n\n## 1. Current Workaround\ninternally I implementing a suboptimal solution that:\n1. Queries all OCR database entries chronologically\n2. Manually identifies session boundaries by analyzing timestamp sequences for each application\n3. Determines session end when detecting a different application in subsequent timestamps\n4. Assumes a session is ongoing if no clear delimiter exists in future timestamps\n\nWith that said I would love to hear any feedbacks that could help solve this problem with a minimal overhead on code changes, and make sure I'm not overthinking the solution: cc @louis030195 , i'm planning to start playing with some day next week\n\n## 2. Requirements\n\n1.  **Session Definition:** A \"session\" should be defined as a continuous period of user interaction with a specific application and window.  A break in activity (e.g., switching to a different application or window) should end the current session and potentially start a new one.  A configurable inactivity timeout (e.g., 5 minutes) should also end a session.\n\n2.  **Data Storage:** The database schema must be modified to store session start and end times, associated with specific applications and windows.  It must maintain links to the relevant OCR and audio data chunks.\n\n3.  **API Endpoint:**  A new or modified API endpoint (e.g., `/sessions`) is needed to query for session data.  This endpoint should support filtering by:\n    *   Date range (start and end timestamps).\n    *   Application name.\n    *   Window name.\n    *   Minimum and maximum session duration.\n    *   Associated tags.\n    *   Pagination (limit and offset).\n\n4.  **Real-time Capability:** The system should be able to detect session boundaries in near real-time as data is being ingested. This implies integration with the existing data capture and processing pipeline.\n\n5. **Configurability:** The inactivity timeout for ending a session should be configurable.\n\n## 3. Proposed Solution\n\n### 3.1. Database Schema Changes\n\nWe'll introduce a new table `sessions` and modify the existing tables.\n\n**New Table: `sessions`**\n\n| Column Name      | Data Type  | Constraints                                  | Description                                                                  |\n| :--------------- | :--------- | :------------------------------------------- | :--------------------------------------------------------------------------- |\n| `id`             | INTEGER    | PRIMARY KEY AUTOINCREMENT                    | Unique identifier for the session.                                       |\n| `app_name`       | TEXT       | NOT NULL                                     | Name of the application.                                                 |\n| `window_name`    | TEXT       |                                               | Title of the window.                                                     |\n| `start_time`     | DATETIME   | NOT NULL                                     | Timestamp of the session start.                                            |\n| `end_time`       | DATETIME   | NOT NULL                                     | Timestamp of the session end.                                              |\n| `duration_secs`  | REAL       | NOT NULL                                     | Duration of the session in seconds.                                        |\n\n**Modified Tables:**\n\n*   **`ocr_text`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`audio_transcriptions`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`ui_monitoring`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n\n**Indexes:**\n\n*   Create indexes on `sessions` table columns (`app_name`, `window_name`, `start_time`, `end_time`) for efficient querying.\n*   Create indexes on the `session_id` columns in `ocr_text`, `audio_transcriptions`, and `ui_monitoring`.\n\n### 3.2. API Enhancements\n\nWe'll modify the existing `/search` endpoint to also support session queries.\n\n**Request:**\n\n```\nGET /search?content_type=session&app_name=Cursor&start_time=2024-11-27T00:00:00Z&end_time=2024-11-28T00:00:00Z&min_duration=60&limit=10&offset=0\n```\n\n*   `content_type=session`: Indicates a session-based search.\n*   `app_name`, `window_name`: Filter by application and window (optional).\n*   `start_time`, `end_time`: Date range for the session (optional, defaults to last 24 hours).\n*   `min_duration`, `max_duration`: Filter by session duration in seconds (optional).\n*   `limit`, `offset`: Pagination parameters.\n\n\nafter discussing with @louis030195 on discord he proposed something like this as the output\n\n**Response:**\n\n```json\n{\n  \"data\": [\n    {\n      \"app_name\": \"Cursor\",\n      \"total_usage_time\": 3600,\n      \"windows\": [\n        {\n          \"window_name\": \"page.tsx\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T12:00:00Z\",\n              \"end_time\": \"2024-07-16T12:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [123, 124, 125],\n              \"tags\": [\"coding\", \"frontend\"]\n            },\n            {\n              \"start_time\": \"2024-07-16T14:00:00Z\",\n              \"end_time\": \"2024-07-16T14:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [223, 224, 225],\n              \"tags\": [\"coding\", \"frontend\"]\n            }\n          ],\n          \"total_usage_time\": 3600\n        }\n      ]\n    },\n    {\n      \"app_name\": \"Arc\",\n      \"total_usage_time\": 1200,\n      \"windows\": [\n        {\n          \"window_name\": \"x.com\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T10:30:00Z\",\n              \"end_time\": \"2024-07-16T10:50:00Z\",\n              \"duration\": 1200,\n              \"content_ids\": [126, 127, 128],\n              \"tags\": [\"browsing\", \"social-media\"]\n            }\n          ],\n          \"total_usage_time\": 1200\n        }\n      ]\n    }\n  ],\n  \"pagination\": {\n    \"limit\": 50,\n    \"offset\": 0,\n    \"total\": 2\n  }\n}\n```\n\n### 3.3. Session Boundary Detection Logic\n\n1.  **Real-time Monitoring:**  The existing event listeners for OCR, audio, and UI events will be leveraged.\n\n2.  **Session Tracking:**  Maintain a data structure (likely in memory, possibly augmented with periodic database writes for persistence) to track active sessions.  This structure should map:\n    *   `(app_name, window_name)` -> `(session_id, last_activity_timestamp)`\n\n3.  **New Event Handling:**\n    *   When a new event (OCR, audio, UI) arrives:\n        *   Extract `app_name` and `window_name`.\n        *   Check if an active session exists for this (app, window) combination.\n        *   If a session exists:\n            *   Check if `now() - last_activity_timestamp` exceeds the inactivity timeout.\n            *   If timeout exceeded, end the existing session and create a new one.\n            *   If not timed out, update `last_activity_timestamp` to the event's timestamp.\n        *   If no session exists:\n            *   Create a new session with `start_time` and `end_time` set to the event's timestamp.\n            *   Generate a new `session_id`.\n            *   Insert a new row into the `sessions` table.\n        *   Insert the new data (OCR, audio, UI) into the relevant table, linking it to the active `session_id`.\n\n4.  **Inactivity Timeout:** A configurable timeout (e.g., 5 minutes) will be used to determine session end due to inactivity.\n\n5.  **Periodic Flushing:** To avoid excessive in-memory state, periodically flush closed sessions (those that have exceeded the inactivity timeout) to the database.\n\n6. **Data Migration:** A migration script will be needed to analyze existing data and create initial session records based on timestamp proximity and app/window changes.\n\n### 3.4. Implementation Details\n\n*   **Language/Framework:** Continue using Rust for core logic and database interaction.\n*   **Database:** Continue using SQLite.\n*   **Concurrency:** Use asynchronous operations with `tokio` for handling concurrent events and database interactions.  Leverage `Arc` and `Mutex` for shared state, and channels for communication between tasks.\n*   **Configuration:** Add a new setting for the inactivity timeout.\n*   **Error Handling:** Implement robust error handling with `anyhow` and propagate errors appropriately. Log errors using `tracing`.\n* **Testing:** Thoroughly test session boundary detection with various scenarios, including rapid switching, long inactivity periods, and concurrent events.\n\n## 4. Diagrams\n\n### 4.1. Current Data Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Database\n    User->>Screen: Interacts with screen\n    Screen->>Screenpipe: Captures screen content (OCR)\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Database: Stores OCR data with timestamps\n    Screenpipe->>Database: Stores audio transcriptions with timestamps\n    User->>Screenpipe: Requests data\n    Screenpipe->>Database: Queries data by time range\n    Database->>Screenpipe: Returns data\n    Screenpipe->>User: Returns data\n```\n\n### 4.2. Proposed Database Schema Changes\n\n```mermaid\nclassDiagram\n    class frames {\n        +id : INTEGER [PK]\n        +video_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +name : TEXT\n    }\n    class video_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +device_name : TEXT\n    }\n    class ocr_text {\n        +frame_id : INTEGER [FK]\n        +text : TEXT\n        +text_json : TEXT\n        +app_name : TEXT\n        +window_name : TEXT\n        +ocr_engine : TEXT\n        +focused : BOOLEAN\n        +session_id : INTEGER [FK]\n    }\n    class audio_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +timestamp : TIMESTAMP\n    }\n    class audio_transcriptions {\n        +id : INTEGER [PK]\n        +audio_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +transcription : TEXT\n        +device : TEXT\n        +is_input_device : BOOLEAN\n        +speaker_id : INTEGER\n        +transcription_engine : TEXT\n        +start_time : REAL\n        +end_time : REAL\n        +text_length : INTEGER\n        +session_id : INTEGER [FK]\n    }\n    class ui_monitoring {\n      +id : INTEGER [PK]\n      +text_output : TEXT\n      +timestamp : DATETIME\n      +app : TEXT\n      +window : TEXT\n      +initial_traversal_at : DATETIME\n      +session_id : INTEGER [FK]\n    }\n  class sessions {\n    +id : INTEGER [PK]\n    +app_name : TEXT\n    +window_name : TEXT\n    +start_time : DATETIME\n    +end_time : DATETIME\n    +duration_secs : REAL\n  }\n  frames --|> video_chunks : video_chunk_id\n  ocr_text --|> frames : frame_id\n  audio_transcriptions --|> audio_chunks : audio_chunk_id\n  ocr_text --|> sessions : session_id\n  audio_transcriptions --|> sessions : session_id\n  ui_monitoring --|> sessions : session_id\n```\n\n### 4.3. Modified Data Flow with Session Tracking\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Session Manager\n    participant Database\n\n    User->>Screen: Interacts with App A, Window 1\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to session_id\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: Active session found (session_id)\n    Screenpipe->>Database: Store audio data, link to session_id\n\n    Note over User,Screenpipe: User switches to App B, Window 2\n    User->>Screen: Interacts with App B, Window 2\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App B, Window 2)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: End previous session (App A, Window 1)\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to new session_id\n\n    Note over User,Screenpipe: Inactivity Timeout\n    Screenpipe->>Session Manager: Check for activity\n    Session Manager->>Database: End current session (App B, Window 2)\n```\n\n### 4.4. New API Endpoint (Modified /search)\n\nThe existing `/search` endpoint will be extended to support session-based queries.  No new endpoint is needed.\n\n## 5. Performance Implications\n\n*   **Database:**  The addition of the `sessions` table and foreign keys will add some overhead to write operations.  Proper indexing is crucial to mitigate this.  Read performance for session-based queries should be good with appropriate indexes.\n*   **Memory:**  The in-memory session tracking will require additional memory.  The size of this will depend on the number of concurrent users and the configured timeout.  This should be monitored and potentially optimized (e.g., using a more efficient data structure than a simple `HashMap`).\n*   **CPU:**  The session boundary detection logic will add some CPU overhead.  This should be minimized by efficient checks and the use of asynchronous operations.\n\n## 6. Migration Path\n\n1.  **Add new `sessions` table.**\n2.  **Add `session_id` columns** to `ocr_text`, `audio_transcriptions`, and `ui_monitoring` tables.\n3.  **Backfill `session_id`:**  A migration script will be needed to analyze existing data and create initial session records.  This script will need to:\n    *   Iterate through existing OCR, audio, and UI data in chronological order.\n    *   Apply the session boundary detection logic (described above) to group data into sessions.\n    *   Insert corresponding rows into the `sessions` table.\n    *   Update the `session_id` foreign key in the existing tables.\n\n\n## 7. Alternative Approaches\n\n1.  **Session Table per Content Type:** Instead of a single `sessions` table, we could have separate session tables for each content type (e.g., `ocr_sessions`, `audio_sessions`, `ui_sessions`).  This might simplify queries for a specific content type but could make cross-content type analysis more complex.\n2.  **No Session Table (Denormalized):** We could add `start_time` and `end_time` directly to the existing tables (`ocr_text`, `audio_transcriptions`, `ui_monitoring`). This would avoid the need for joins when querying for sessions, but could lead to data redundancy and potential inconsistencies.\n3.  any other suggestions ????\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1560"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1560",
              "body": "\n## Challenge\n\nI'm working on an application that requires accurate session tracking to analyze user behavior patterns,  and provide meaningful insights across different applications and windows. \n\nIn the existing Screenpipe architecture, OCR data is stored with only timestamp information, lacking precise session boundaries (start/end times) for application usage tracking. This limitation prevents us from properly segmenting user activity into coherent sessions.\n\n## 1. Current Workaround\ninternally I implementing a suboptimal solution that:\n1. Queries all OCR database entries chronologically\n2. Manually identifies session boundaries by analyzing timestamp sequences for each application\n3. Determines session end when detecting a different application in subsequent timestamps\n4. Assumes a session is ongoing if no clear delimiter exists in future timestamps\n\nWith that said I would love to hear any feedbacks that could help solve this problem with a minimal overhead on code changes, and make sure I'm not overthinking the solution: cc @louis030195 , i'm planning to start playing with some day next week\n\n## 2. Requirements\n\n1.  **Session Definition:** A \"session\" should be defined as a continuous period of user interaction with a specific application and window.  A break in activity (e.g., switching to a different application or window) should end the current session and potentially start a new one.  A configurable inactivity timeout (e.g., 5 minutes) should also end a session.\n\n2.  **Data Storage:** The database schema must be modified to store session start and end times, associated with specific applications and windows.  It must maintain links to the relevant OCR and audio data chunks.\n\n3.  **API Endpoint:**  A new or modified API endpoint (e.g., `/sessions`) is needed to query for session data.  This endpoint should support filtering by:\n    *   Date range (start and end timestamps).\n    *   Application name.\n    *   Window name.\n    *   Minimum and maximum session duration.\n    *   Associated tags.\n    *   Pagination (limit and offset).\n\n4.  **Real-time Capability:** The system should be able to detect session boundaries in near real-time as data is being ingested. This implies integration with the existing data capture and processing pipeline.\n\n5. **Configurability:** The inactivity timeout for ending a session should be configurable.\n\n## 3. Proposed Solution\n\n### 3.1. Database Schema Changes\n\nWe'll introduce a new table `sessions` and modify the existing tables.\n\n**New Table: `sessions`**\n\n| Column Name      | Data Type  | Constraints                                  | Description                                                                  |\n| :--------------- | :--------- | :------------------------------------------- | :--------------------------------------------------------------------------- |\n| `id`             | INTEGER    | PRIMARY KEY AUTOINCREMENT                    | Unique identifier for the session.                                       |\n| `app_name`       | TEXT       | NOT NULL                                     | Name of the application.                                                 |\n| `window_name`    | TEXT       |                                               | Title of the window.                                                     |\n| `start_time`     | DATETIME   | NOT NULL                                     | Timestamp of the session start.                                            |\n| `end_time`       | DATETIME   | NOT NULL                                     | Timestamp of the session end.                                              |\n| `duration_secs`  | REAL       | NOT NULL                                     | Duration of the session in seconds.                                        |\n\n**Modified Tables:**\n\n*   **`ocr_text`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`audio_transcriptions`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`ui_monitoring`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n\n**Indexes:**\n\n*   Create indexes on `sessions` table columns (`app_name`, `window_name`, `start_time`, `end_time`) for efficient querying.\n*   Create indexes on the `session_id` columns in `ocr_text`, `audio_transcriptions`, and `ui_monitoring`.\n\n### 3.2. API Enhancements\n\nWe'll modify the existing `/search` endpoint to also support session queries.\n\n**Request:**\n\n```\nGET /search?content_type=session&app_name=Cursor&start_time=2024-11-27T00:00:00Z&end_time=2024-11-28T00:00:00Z&min_duration=60&limit=10&offset=0\n```\n\n*   `content_type=session`: Indicates a session-based search.\n*   `app_name`, `window_name`: Filter by application and window (optional).\n*   `start_time`, `end_time`: Date range for the session (optional, defaults to last 24 hours).\n*   `min_duration`, `max_duration`: Filter by session duration in seconds (optional).\n*   `limit`, `offset`: Pagination parameters.\n\n\nafter discussing with @louis030195 on discord he proposed something like this as the output\n\n**Response:**\n\n```json\n{\n  \"data\": [\n    {\n      \"app_name\": \"Cursor\",\n      \"total_usage_time\": 3600,\n      \"windows\": [\n        {\n          \"window_name\": \"page.tsx\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T12:00:00Z\",\n              \"end_time\": \"2024-07-16T12:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [123, 124, 125],\n              \"tags\": [\"coding\", \"frontend\"]\n            },\n            {\n              \"start_time\": \"2024-07-16T14:00:00Z\",\n              \"end_time\": \"2024-07-16T14:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [223, 224, 225],\n              \"tags\": [\"coding\", \"frontend\"]\n            }\n          ],\n          \"total_usage_time\": 3600\n        }\n      ]\n    },\n    {\n      \"app_name\": \"Arc\",\n      \"total_usage_time\": 1200,\n      \"windows\": [\n        {\n          \"window_name\": \"x.com\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T10:30:00Z\",\n              \"end_time\": \"2024-07-16T10:50:00Z\",\n              \"duration\": 1200,\n              \"content_ids\": [126, 127, 128],\n              \"tags\": [\"browsing\", \"social-media\"]\n            }\n          ],\n          \"total_usage_time\": 1200\n        }\n      ]\n    }\n  ],\n  \"pagination\": {\n    \"limit\": 50,\n    \"offset\": 0,\n    \"total\": 2\n  }\n}\n```\n\n### 3.3. Session Boundary Detection Logic\n\n1.  **Real-time Monitoring:**  The existing event listeners for OCR, audio, and UI events will be leveraged.\n\n2.  **Session Tracking:**  Maintain a data structure (likely in memory, possibly augmented with periodic database writes for persistence) to track active sessions.  This structure should map:\n    *   `(app_name, window_name)` -> `(session_id, last_activity_timestamp)`\n\n3.  **New Event Handling:**\n    *   When a new event (OCR, audio, UI) arrives:\n        *   Extract `app_name` and `window_name`.\n        *   Check if an active session exists for this (app, window) combination.\n        *   If a session exists:\n            *   Check if `now() - last_activity_timestamp` exceeds the inactivity timeout.\n            *   If timeout exceeded, end the existing session and create a new one.\n            *   If not timed out, update `last_activity_timestamp` to the event's timestamp.\n        *   If no session exists:\n            *   Create a new session with `start_time` and `end_time` set to the event's timestamp.\n            *   Generate a new `session_id`.\n            *   Insert a new row into the `sessions` table.\n        *   Insert the new data (OCR, audio, UI) into the relevant table, linking it to the active `session_id`.\n\n4.  **Inactivity Timeout:** A configurable timeout (e.g., 5 minutes) will be used to determine session end due to inactivity.\n\n5.  **Periodic Flushing:** To avoid excessive in-memory state, periodically flush closed sessions (those that have exceeded the inactivity timeout) to the database.\n\n6. **Data Migration:** A migration script will be needed to analyze existing data and create initial session records based on timestamp proximity and app/window changes.\n\n### 3.4. Implementation Details\n\n*   **Language/Framework:** Continue using Rust for core logic and database interaction.\n*   **Database:** Continue using SQLite.\n*   **Concurrency:** Use asynchronous operations with `tokio` for handling concurrent events and database interactions.  Leverage `Arc` and `Mutex` for shared state, and channels for communication between tasks.\n*   **Configuration:** Add a new setting for the inactivity timeout.\n*   **Error Handling:** Implement robust error handling with `anyhow` and propagate errors appropriately. Log errors using `tracing`.\n* **Testing:** Thoroughly test session boundary detection with various scenarios, including rapid switching, long inactivity periods, and concurrent events.\n\n## 4. Diagrams\n\n### 4.1. Current Data Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Database\n    User->>Screen: Interacts with screen\n    Screen->>Screenpipe: Captures screen content (OCR)\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Database: Stores OCR data with timestamps\n    Screenpipe->>Database: Stores audio transcriptions with timestamps\n    User->>Screenpipe: Requests data\n    Screenpipe->>Database: Queries data by time range\n    Database->>Screenpipe: Returns data\n    Screenpipe->>User: Returns data\n```\n\n### 4.2. Proposed Database Schema Changes\n\n```mermaid\nclassDiagram\n    class frames {\n        +id : INTEGER [PK]\n        +video_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +name : TEXT\n    }\n    class video_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +device_name : TEXT\n    }\n    class ocr_text {\n        +frame_id : INTEGER [FK]\n        +text : TEXT\n        +text_json : TEXT\n        +app_name : TEXT\n        +window_name : TEXT\n        +ocr_engine : TEXT\n        +focused : BOOLEAN\n        +session_id : INTEGER [FK]\n    }\n    class audio_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +timestamp : TIMESTAMP\n    }\n    class audio_transcriptions {\n        +id : INTEGER [PK]\n        +audio_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +transcription : TEXT\n        +device : TEXT\n        +is_input_device : BOOLEAN\n        +speaker_id : INTEGER\n        +transcription_engine : TEXT\n        +start_time : REAL\n        +end_time : REAL\n        +text_length : INTEGER\n        +session_id : INTEGER [FK]\n    }\n    class ui_monitoring {\n      +id : INTEGER [PK]\n      +text_output : TEXT\n      +timestamp : DATETIME\n      +app : TEXT\n      +window : TEXT\n      +initial_traversal_at : DATETIME\n      +session_id : INTEGER [FK]\n    }\n  class sessions {\n    +id : INTEGER [PK]\n    +app_name : TEXT\n    +window_name : TEXT\n    +start_time : DATETIME\n    +end_time : DATETIME\n    +duration_secs : REAL\n  }\n  frames --|> video_chunks : video_chunk_id\n  ocr_text --|> frames : frame_id\n  audio_transcriptions --|> audio_chunks : audio_chunk_id\n  ocr_text --|> sessions : session_id\n  audio_transcriptions --|> sessions : session_id\n  ui_monitoring --|> sessions : session_id\n```\n\n### 4.3. Modified Data Flow with Session Tracking\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Session Manager\n    participant Database\n\n    User->>Screen: Interacts with App A, Window 1\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to session_id\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: Active session found (session_id)\n    Screenpipe->>Database: Store audio data, link to session_id\n\n    Note over User,Screenpipe: User switches to App B, Window 2\n    User->>Screen: Interacts with App B, Window 2\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App B, Window 2)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: End previous session (App A, Window 1)\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to new session_id\n\n    Note over User,Screenpipe: Inactivity Timeout\n    Screenpipe->>Session Manager: Check for activity\n    Session Manager->>Database: End current session (App B, Window 2)\n```\n\n### 4.4. New API Endpoint (Modified /search)\n\nThe existing `/search` endpoint will be extended to support session-based queries.  No new endpoint is needed.\n\n## 5. Performance Implications\n\n*   **Database:**  The addition of the `sessions` table and foreign keys will add some overhead to write operations.  Proper indexing is crucial to mitigate this.  Read performance for session-based queries should be good with appropriate indexes.\n*   **Memory:**  The in-memory session tracking will require additional memory.  The size of this will depend on the number of concurrent users and the configured timeout.  This should be monitored and potentially optimized (e.g., using a more efficient data structure than a simple `HashMap`).\n*   **CPU:**  The session boundary detection logic will add some CPU overhead.  This should be minimized by efficient checks and the use of asynchronous operations.\n\n## 6. Migration Path\n\n1.  **Add new `sessions` table.**\n2.  **Add `session_id` columns** to `ocr_text`, `audio_transcriptions`, and `ui_monitoring` tables.\n3.  **Backfill `session_id`:**  A migration script will be needed to analyze existing data and create initial session records.  This script will need to:\n    *   Iterate through existing OCR, audio, and UI data in chronological order.\n    *   Apply the session boundary detection logic (described above) to group data into sessions.\n    *   Insert corresponding rows into the `sessions` table.\n    *   Update the `session_id` foreign key in the existing tables.\n\n\n## 7. Alternative Approaches\n\n1.  **Session Table per Content Type:** Instead of a single `sessions` table, we could have separate session tables for each content type (e.g., `ocr_sessions`, `audio_sessions`, `ui_sessions`).  This might simplify queries for a specific content type but could make cross-content type analysis more complex.\n2.  **No Session Table (Denormalized):** We could add `start_time` and `end_time` directly to the existing tables (`ocr_text`, `audio_transcriptions`, `ui_monitoring`). This would avoid the need for joins when querying for sessions, but could lead to data redundancy and potential inconsistencies.\n3.  any other suggestions ????\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1560",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1441",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:35.417Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.417Z",
            "created_at": "2025-12-16T21:16:35.417Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1441",
              "status": "open",
              "type": "issue",
              "number": 1441,
              "title": "[bounty] general purpose scrapper",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1441",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] general purpose scrapper",
                  "body": "\nv0\n\nshould be able to scrap whatsapp\n\nideally you can use the keyboard / mouse api \n\nhttps://docs.screenpi.pe/docs/sdk-reference#input-control-api\n\n\nbasically many ppl would like to scrap data from desktop app and it's hard, like whatsapp, imessage, or even web but without getting banned or because it's local authentication is already done by user usually and stays local, and much less likely detected \n\n\n\n/bounty 200\n\nplease suggest how you would implement a very simple first version that work for whatsapp and UX\n\n\n\n\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1441"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1441",
              "body": "\nv0\n\nshould be able to scrap whatsapp\n\nideally you can use the keyboard / mouse api \n\nhttps://docs.screenpi.pe/docs/sdk-reference#input-control-api\n\n\nbasically many ppl would like to scrap data from desktop app and it's hard, like whatsapp, imessage, or even web but without getting banned or because it's local authentication is already done by user usually and stays local, and much less likely detected \n\n\n\n/bounty 200\n\nplease suggest how you would implement a very simple first version that work for whatsapp and UX\n\n\n\n\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1441",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1298",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:35.846Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.846Z",
            "created_at": "2025-12-16T21:16:35.846Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1298",
              "status": "open",
              "type": "issue",
              "number": 1298,
              "title": "[docs] document env var somewhere somehow ",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1298",
                  "user": {
                    "login": "dassio",
                    "id": 733502,
                    "node_id": "MDQ6VXNlcjczMzUwMg==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/733502?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/dassio",
                    "html_url": "https://github.com/dassio",
                    "followers_url": "https://api.github.com/users/dassio/followers",
                    "following_url": "https://api.github.com/users/dassio/following{/other_user}",
                    "gists_url": "https://api.github.com/users/dassio/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/dassio/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/dassio/subscriptions",
                    "organizations_url": "https://api.github.com/users/dassio/orgs",
                    "repos_url": "https://api.github.com/users/dassio/repos",
                    "events_url": "https://api.github.com/users/dassio/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/dassio/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[docs] document env var somewhere somehow ",
                  "body": "**describe the feature**\nin the desktop app, you can configure hugging face mirror, but for the screenpipe server cli ,there is no such option , when try to use whisper for audio transcription, need to download mode from hugging face \n\n**why is this needed?**\nhugging face is banned in China\n\n\n**additional context**\n```\nyou are using local processing. all your data stays on your computer.\n\nwarning: telemetry is enabled. only error-level data will be sent to highlight.io.\nto disable, use the --disable-telemetry flag.\n\ncheck latest changes here: https://github.com/mediar-ai/screenpipe/releases\n2025-02-07T14:15:37.861529Z  INFO screenpipe: starting pipes\nfailed to start pipe timeline: No pipe.js/pipe.ts found in the pipe/dist directory\n2025-02-07T14:15:37.864597Z  INFO screenpipe_server::server: Server starting on 127.0.0.1:3030\n2025-02-07T14:15:45.839709Z  INFO screenpipe_server::video: Starting FFmpeg process for file: C:\\Users\\dassi\\.screenpipe\\data\\monitor_490607759_2025-02-07_14-15-45.mp4\n2025-02-07T14:15:48.283937Z  INFO screenpipe_server::resource_monitor: Runtime: 10s, Total Memory: 2% (0.25 GB / 16.09 GB), Total CPU: 88%\n2025-02-07T14:15:58.322953Z  INFO screenpipe_server::resource_monitor: Runtime: 20s, Total Memory: 2% (0.26 GB / 16.09 GB), Total CPU: 92%\n2025-02-07T14:16:00.374953Z ERROR screenpipe: continuous recording error: request error: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n\nCaused by:\n    0: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n    1: connection timed out\n```\n\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1298"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1298",
              "body": "**describe the feature**\nin the desktop app, you can configure hugging face mirror, but for the screenpipe server cli ,there is no such option , when try to use whisper for audio transcription, need to download mode from hugging face \n\n**why is this needed?**\nhugging face is banned in China\n\n\n**additional context**\n```\nyou are using local processing. all your data stays on your computer.\n\nwarning: telemetry is enabled. only error-level data will be sent to highlight.io.\nto disable, use the --disable-telemetry flag.\n\ncheck latest changes here: https://github.com/mediar-ai/screenpipe/releases\n2025-02-07T14:15:37.861529Z  INFO screenpipe: starting pipes\nfailed to start pipe timeline: No pipe.js/pipe.ts found in the pipe/dist directory\n2025-02-07T14:15:37.864597Z  INFO screenpipe_server::server: Server starting on 127.0.0.1:3030\n2025-02-07T14:15:45.839709Z  INFO screenpipe_server::video: Starting FFmpeg process for file: C:\\Users\\dassi\\.screenpipe\\data\\monitor_490607759_2025-02-07_14-15-45.mp4\n2025-02-07T14:15:48.283937Z  INFO screenpipe_server::resource_monitor: Runtime: 10s, Total Memory: 2% (0.25 GB / 16.09 GB), Total CPU: 88%\n2025-02-07T14:15:58.322953Z  INFO screenpipe_server::resource_monitor: Runtime: 20s, Total Memory: 2% (0.26 GB / 16.09 GB), Total CPU: 92%\n2025-02-07T14:16:00.374953Z ERROR screenpipe: continuous recording error: request error: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n\nCaused by:\n    0: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n    1: connection timed out\n```\n\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1298",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1383",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-12-16T21:16:36.257Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:36.257Z",
            "created_at": "2025-12-16T21:16:36.257Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1383",
              "status": "open",
              "type": "issue",
              "number": 1383,
              "title": "[bounty] implement deep research in search pipe",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1383",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] implement deep research in search pipe",
                  "body": "\n\ni love perplexity deep research \n\ni think we should have something like this in screenpipe search, which would be something that go beyond surface level, really in depth on your data \n\nnot sure about the UX yet, suggest any design first \n\nshould allow async (eg ask something and come back later to it, stored in some kind of history, maybe related to #1382 check perplexity UI too)\n\n/bounty 200 \n\n\n\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1383"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1383",
              "body": "\n\ni love perplexity deep research \n\ni think we should have something like this in screenpipe search, which would be something that go beyond surface level, really in depth on your data \n\nnot sure about the UX yet, suggest any design first \n\nshould allow async (eg ask something and come back later to it, stored in some kind of history, maybe related to #1382 check perplexity UI too)\n\n/bounty 200 \n\n\n\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1383",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#23104",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:31.675Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:31.675Z",
            "created_at": "2025-12-16T21:16:31.675Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#23104",
              "status": "open",
              "type": "issue",
              "number": 23104,
              "title": "Local dev crazy slow",
              "source": {
                "data": {
                  "id": "source-cal#23104",
                  "user": {
                    "login": "keithwillcode",
                    "id": 2538462,
                    "node_id": "MDQ6VXNlcjI1Mzg0NjI=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/2538462?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/keithwillcode",
                    "html_url": "https://github.com/keithwillcode",
                    "followers_url": "https://api.github.com/users/keithwillcode/followers",
                    "following_url": "https://api.github.com/users/keithwillcode/following{/other_user}",
                    "gists_url": "https://api.github.com/users/keithwillcode/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/keithwillcode/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/keithwillcode/subscriptions",
                    "organizations_url": "https://api.github.com/users/keithwillcode/orgs",
                    "repos_url": "https://api.github.com/users/keithwillcode/repos",
                    "events_url": "https://api.github.com/users/keithwillcode/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/keithwillcode/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Local dev crazy slow",
                  "body": "$2000 USD Bounty here!\n\nIf you can find a nice solution to our local dev experience, take home a $2000 USD bounty. We have a very strong feeling the culprit is the App Store. The problem is that refactoring it is a challenge since we have 100+ apps, there are tons of circular dependencies and our `main` branch is constantly merged to, making conflicts hard to work around.\n\nTake these PRs as inspiration to ideally not load the entire App Store upon any given page load in our app. If you debug locally and watch what happens in the `.next/server/chunks` folder, you will see it's compiling a ton of code that's not needed.\n\nThese PRs have gotten close but refactoring the test suite to continue passing has been quite a blocker.\n\nhttps://github.com/calcom/cal.com/pull/22450 - EDIT 2025-08-17 This is being brought back to life with the help of @joeauyeung to fix the test suite.\nhttps://github.com/calcom/cal.com/pull/19771\n\nCurrent initial page load times per page even on a beast MacBook Pro can be upwards of 10-12s. **This needs to be cut by at least 80%.**",
                  "html_url": "https://github.com/calcom/cal.com/issues/23104"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#23104",
              "body": "$2000 USD Bounty here!\n\nIf you can find a nice solution to our local dev experience, take home a $2000 USD bounty. We have a very strong feeling the culprit is the App Store. The problem is that refactoring it is a challenge since we have 100+ apps, there are tons of circular dependencies and our `main` branch is constantly merged to, making conflicts hard to work around.\n\nTake these PRs as inspiration to ideally not load the entire App Store upon any given page load in our app. If you debug locally and watch what happens in the `.next/server/chunks` folder, you will see it's compiling a ton of code that's not needed.\n\nThese PRs have gotten close but refactoring the test suite to continue passing has been quite a blocker.\n\nhttps://github.com/calcom/cal.com/pull/22450 - EDIT 2025-08-17 This is being brought back to life with the help of @joeauyeung to fix the test suite.\nhttps://github.com/calcom/cal.com/pull/19771\n\nCurrent initial page load times per page even on a beast MacBook Pro can be upwards of 10-12s. **This needs to be cut by at least 80%.**",
              "url": "https://github.com/calcom/cal.com/issues/23104",
              "tech": [
                "go"
              ],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#9485",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:32.840Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:32.840Z",
            "created_at": "2025-12-16T21:16:32.840Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#9485",
              "status": "open",
              "type": "issue",
              "number": 9485,
              "title": "CalDAV integration with Fastmail is generating duplicate, erroneous invitation emails.",
              "source": {
                "data": {
                  "id": "source-cal#9485",
                  "user": {
                    "login": "mtnowl",
                    "id": 11601915,
                    "node_id": "MDQ6VXNlcjExNjAxOTE1",
                    "avatar_url": "https://avatars.githubusercontent.com/u/11601915?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/mtnowl",
                    "html_url": "https://github.com/mtnowl",
                    "followers_url": "https://api.github.com/users/mtnowl/followers",
                    "following_url": "https://api.github.com/users/mtnowl/following{/other_user}",
                    "gists_url": "https://api.github.com/users/mtnowl/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/mtnowl/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/mtnowl/subscriptions",
                    "organizations_url": "https://api.github.com/users/mtnowl/orgs",
                    "repos_url": "https://api.github.com/users/mtnowl/repos",
                    "events_url": "https://api.github.com/users/mtnowl/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/mtnowl/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "CalDAV integration with Fastmail is generating duplicate, erroneous invitation emails.",
                  "body": "### Issue Summary\r\n\r\ncreated from https://github.com/calcom/cal.com/issues/3457#issuecomment-1581465501, but I am also using Fastmail and seeing the same issues. This is making it so I can't use the integration.\r\n\r\nWe also use Fastmail and have been seeing some oddities with the [cal.com](http://cal.com/) integration. Namely, that participants are receiving multiple calendar invites, which on it's own isn't that big of a deal. However, the Fastmail invites are always in UTC, not in the local time of the cal.com invite.\r\n\r\nThis is creating a lot of confusion with participants, they keep missing our meetings, not realizing one invite is UTC and one in local time. I spoke with the Fastmail team and they relayed the following notes about cal's specific integration.\r\n\r\n1. There is a property (SCHEDULE-AGENT=CLIENT) a CalDAV client can add to the event that would prevent CalDAV servers from sending invitations. It appears cal.com is not setting this property even though they are sending an invitation.\r\n2. The invitation being sent by [cal.com](http://cal.com/) is using a different UID. Basically every event has a UID that uniquely identifies the event. Since [cal.com](http://cal.com/) is sending the invitation of the event with a different UID, if you end up adding this event to your calendar [cal.com](http://cal.com/), it will create a duplicate event in your calendar as well. They need to fix it so that all emails associated with a single event use the same original UID.\r\n3. The event added from [cal.com](http://cal.com/) doesn't have a timezone associated with it. Though it mentions the invitees timezone as \"America/Chicago\" in the notes, the event itself doesn't include a timezone and only specifies the time in UTC. Since the event time is specified in UTC, Fastmail's invitation email will also be sent in UTC.\r\n4. There seems to be no way to stop Cal.com from creating the calendar events at all, and thus no real workaround.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Connect to Fastmail using CalDav integration\r\n2. Create a test booking\r\n3. See issues above.\r\n\r\nAny other relevant information. For example, why do you consider this a bug and what did you expect to happen instead?\r\n1. SCHEDULE-AGENT=CLIENT should be set, so Fastmail knows not to send an invitation. **This is the main one.**\r\n2. UID should be unique per event.\r\n3. Event time should have a timezone associated with it.\r\n4. An option could be added to make calendar event creation optional (nice-to-have, not essential).\r\n\r\n### Technical details\r\n\r\nNone other than Fastmail/CalDAV.",
                  "html_url": "https://github.com/calcom/cal.com/issues/9485"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#9485",
              "body": "### Issue Summary\r\n\r\ncreated from https://github.com/calcom/cal.com/issues/3457#issuecomment-1581465501, but I am also using Fastmail and seeing the same issues. This is making it so I can't use the integration.\r\n\r\nWe also use Fastmail and have been seeing some oddities with the [cal.com](http://cal.com/) integration. Namely, that participants are receiving multiple calendar invites, which on it's own isn't that big of a deal. However, the Fastmail invites are always in UTC, not in the local time of the cal.com invite.\r\n\r\nThis is creating a lot of confusion with participants, they keep missing our meetings, not realizing one invite is UTC and one in local time. I spoke with the Fastmail team and they relayed the following notes about cal's specific integration.\r\n\r\n1. There is a property (SCHEDULE-AGENT=CLIENT) a CalDAV client can add to the event that would prevent CalDAV servers from sending invitations. It appears cal.com is not setting this property even though they are sending an invitation.\r\n2. The invitation being sent by [cal.com](http://cal.com/) is using a different UID. Basically every event has a UID that uniquely identifies the event. Since [cal.com](http://cal.com/) is sending the invitation of the event with a different UID, if you end up adding this event to your calendar [cal.com](http://cal.com/), it will create a duplicate event in your calendar as well. They need to fix it so that all emails associated with a single event use the same original UID.\r\n3. The event added from [cal.com](http://cal.com/) doesn't have a timezone associated with it. Though it mentions the invitees timezone as \"America/Chicago\" in the notes, the event itself doesn't include a timezone and only specifies the time in UTC. Since the event time is specified in UTC, Fastmail's invitation email will also be sent in UTC.\r\n4. There seems to be no way to stop Cal.com from creating the calendar events at all, and thus no real workaround.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Connect to Fastmail using CalDav integration\r\n2. Create a test booking\r\n3. See issues above.\r\n\r\nAny other relevant information. For example, why do you consider this a bug and what did you expect to happen instead?\r\n1. SCHEDULE-AGENT=CLIENT should be set, so Fastmail knows not to send an invitation. **This is the main one.**\r\n2. UID should be unique per event.\r\n3. Event time should have a timezone associated with it.\r\n4. An option could be added to make calendar event creation optional (nice-to-have, not essential).\r\n\r\n### Technical details\r\n\r\nNone other than Fastmail/CalDAV.",
              "url": "https://github.com/calcom/cal.com/issues/9485",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#18947",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:34.271Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:34.271Z",
            "created_at": "2025-12-16T21:16:34.271Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#18947",
              "status": "open",
              "type": "issue",
              "number": 18947,
              "title": "[CAL-5091] additional settings: \"add team member as optional guest",
              "source": {
                "data": {
                  "id": "source-cal#18947",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-5091] additional settings: \"add team member as optional guest",
                  "body": "![Image](https://github.com/user-attachments/assets/305d292b-a984-44c0-b89f-f8ed979d4c71)\n\n1. team members only (to protect spam) and show `<UpgradeTeamsBadge />`\n2.  don't check the team members calendar for conflict checking, only invite them\n3. if possible mark them as \"optional\":\n\n![Image](https://github.com/user-attachments/assets/7d2a1ec0-6c68-4334-a5ea-a1137279b98f)\n\n<sub>[CAL-5091](https://linear.app/calcom/issue/CAL-5091/additional-settings-add-team-member-as-optional-guest)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/18947"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#18947",
              "body": "![Image](https://github.com/user-attachments/assets/305d292b-a984-44c0-b89f-f8ed979d4c71)\n\n1. team members only (to protect spam) and show `<UpgradeTeamsBadge />`\n2.  don't check the team members calendar for conflict checking, only invite them\n3. if possible mark them as \"optional\":\n\n![Image](https://github.com/user-attachments/assets/7d2a1ec0-6c68-4334-a5ea-a1137279b98f)\n\n<sub>[CAL-5091](https://linear.app/calcom/issue/CAL-5091/additional-settings-add-team-member-as-optional-guest)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/18947",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#18992",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:34.512Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:34.512Z",
            "created_at": "2025-12-16T21:16:34.512Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#18992",
              "status": "open",
              "type": "issue",
              "number": 18992,
              "title": "[CAL-5107] add no-show to zapier (its a webhook only right now)",
              "source": {
                "data": {
                  "id": "source-cal#18992",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-5107] add no-show to zapier (its a webhook only right now)",
                  "body": "![Image](https://github.com/user-attachments/assets/d1992193-e964-4382-aff1-ba90d29c0562)\n\n<sub>[CAL-5107](https://linear.app/calcom/issue/CAL-5107/add-no-show-to-zapier-its-a-webhook-only-right-now)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/18992"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#18992",
              "body": "![Image](https://github.com/user-attachments/assets/d1992193-e964-4382-aff1-ba90d29c0562)\n\n<sub>[CAL-5107](https://linear.app/calcom/issue/CAL-5107/add-no-show-to-zapier-its-a-webhook-only-right-now)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/18992",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#18987",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:34.676Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:34.676Z",
            "created_at": "2025-12-16T21:16:34.676Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#18987",
              "status": "open",
              "type": "issue",
              "number": 18987,
              "title": "[CAL-5097] add the same \"booking questions\" to routing forms",
              "source": {
                "data": {
                  "id": "source-cal#18987",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-5097] add the same \"booking questions\" to routing forms",
                  "body": "right now \"booking forms\" for event-types have more (and different) inputs that routing forms:\n\n\nevent-types:\n\nhttps://github.com/user-attachments/assets/29c921ea-8677-4238-a198-55be00ce8188\n\nrouting forms:\n\nhttps://github.com/user-attachments/assets/cdaf70e0-7c5b-416b-a984-a57ded4c8525\n\n\nwe should reuse the entire event-type booking question UI inside routing forms.\n\n\nwe should also ask for the identifier **first** and not prefill it. (see event-types)\n\n<sub>[CAL-5097](https://linear.app/calcom/issue/CAL-5097/add-the-same-booking-questions-to-routing-forms)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/18987"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#18987",
              "body": "right now \"booking forms\" for event-types have more (and different) inputs that routing forms:\n\n\nevent-types:\n\nhttps://github.com/user-attachments/assets/29c921ea-8677-4238-a198-55be00ce8188\n\nrouting forms:\n\nhttps://github.com/user-attachments/assets/cdaf70e0-7c5b-416b-a984-a57ded4c8525\n\n\nwe should reuse the entire event-type booking question UI inside routing forms.\n\n\nwe should also ask for the identifier **first** and not prefill it. (see event-types)\n\n<sub>[CAL-5097](https://linear.app/calcom/issue/CAL-5097/add-the-same-booking-questions-to-routing-forms)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/18987",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#16797",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:34.955Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:34.955Z",
            "created_at": "2025-12-16T21:16:34.955Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#16797",
              "status": "open",
              "type": "issue",
              "number": 16797,
              "title": "Native Pipedrive integration",
              "source": {
                "data": {
                  "id": "source-cal#16797",
                  "user": {
                    "login": "JakobStadlhuber",
                    "id": 20328243,
                    "node_id": "MDQ6VXNlcjIwMzI4MjQz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/20328243?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/JakobStadlhuber",
                    "html_url": "https://github.com/JakobStadlhuber",
                    "followers_url": "https://api.github.com/users/JakobStadlhuber/followers",
                    "following_url": "https://api.github.com/users/JakobStadlhuber/following{/other_user}",
                    "gists_url": "https://api.github.com/users/JakobStadlhuber/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/JakobStadlhuber/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/JakobStadlhuber/subscriptions",
                    "organizations_url": "https://api.github.com/users/JakobStadlhuber/orgs",
                    "repos_url": "https://api.github.com/users/JakobStadlhuber/repos",
                    "events_url": "https://api.github.com/users/JakobStadlhuber/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/JakobStadlhuber/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Native Pipedrive integration",
                  "body": "### Is your proposal related to a problem?\r\n\r\nYes, the Pipedrive integration in Cal.com is currently not functioning because it relies on Revert.dev, which has been deprecated and no longer maintained since September 2023. When I try to add Pipedrive via the web UI, I receive an error message stating \"pipedrive client id missing.\"\r\n\r\n### Describe the solution you'd like\r\n\r\nI would like the Pipedrive integration to be updated to remove the dependency on Revert.dev. Instead, the integration should directly use Pipedrive's API, implementing OAuth2 authentication. This would involve updating the web UI to allow users to input their Pipedrive Client ID and Client Secret to establish the connection.\r\n\r\n### Describe alternatives you've considered\r\n\r\nI've considered manually integrating Pipedrive using custom API scripts, but this approach is not practical for most users due to its complexity. I also looked into third-party integration tools, but they may not offer the same seamless experience and could incur additional costs.\r\n\r\n### Additional context\r\n\r\nMaintaining a functional Pipedrive integration is crucial for users who rely on it to sync their scheduling with their CRM systems. The deprecation of Revert.dev has disrupted this workflow, impacting productivity. Updating the integration will restore this essential functionality for many users.\r\n\r\n### Requirement/Document\r\n\r\n- [Pipedrive API Authentication Documentation](https://pipedrive.readme.io/docs/core-api-concepts-authentication)\r\n",
                  "html_url": "https://github.com/calcom/cal.com/issues/16797"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#16797",
              "body": "### Is your proposal related to a problem?\r\n\r\nYes, the Pipedrive integration in Cal.com is currently not functioning because it relies on Revert.dev, which has been deprecated and no longer maintained since September 2023. When I try to add Pipedrive via the web UI, I receive an error message stating \"pipedrive client id missing.\"\r\n\r\n### Describe the solution you'd like\r\n\r\nI would like the Pipedrive integration to be updated to remove the dependency on Revert.dev. Instead, the integration should directly use Pipedrive's API, implementing OAuth2 authentication. This would involve updating the web UI to allow users to input their Pipedrive Client ID and Client Secret to establish the connection.\r\n\r\n### Describe alternatives you've considered\r\n\r\nI've considered manually integrating Pipedrive using custom API scripts, but this approach is not practical for most users due to its complexity. I also looked into third-party integration tools, but they may not offer the same seamless experience and could incur additional costs.\r\n\r\n### Additional context\r\n\r\nMaintaining a functional Pipedrive integration is crucial for users who rely on it to sync their scheduling with their CRM systems. The deprecation of Revert.dev has disrupted this workflow, impacting productivity. Updating the integration will restore this essential functionality for many users.\r\n\r\n### Requirement/Document\r\n\r\n- [Pipedrive API Authentication Documentation](https://pipedrive.readme.io/docs/core-api-concepts-authentication)\r\n",
              "url": "https://github.com/calcom/cal.com/issues/16797",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#16651",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:35.111Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.178Z",
            "created_at": "2025-12-16T21:16:35.178Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#16651",
              "status": "open",
              "type": "issue",
              "number": 16651,
              "title": "[CAL-4716] Missing Notifications for Events",
              "source": {
                "data": {
                  "id": "source-cal#16651",
                  "user": {
                    "login": "tiagovasc",
                    "id": 39215546,
                    "node_id": "MDQ6VXNlcjM5MjE1NTQ2",
                    "avatar_url": "https://avatars.githubusercontent.com/u/39215546?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/tiagovasc",
                    "html_url": "https://github.com/tiagovasc",
                    "followers_url": "https://api.github.com/users/tiagovasc/followers",
                    "following_url": "https://api.github.com/users/tiagovasc/following{/other_user}",
                    "gists_url": "https://api.github.com/users/tiagovasc/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/tiagovasc/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/tiagovasc/subscriptions",
                    "organizations_url": "https://api.github.com/users/tiagovasc/orgs",
                    "repos_url": "https://api.github.com/users/tiagovasc/repos",
                    "events_url": "https://api.github.com/users/tiagovasc/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/tiagovasc/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-4716] Missing Notifications for Events",
                  "body": "I propose integrating a feature in Cal.com that allows users to set default Google Calendar notifications for events booked through the platform. This functionality should enable users to select preferred notification times (e.g., 10 minutes, 30 minutes, 1 hour before the event) that automatically apply to all events created via Cal.com bookings. Currently, the absence of such a feature leads to missed appointments since default notifications set in Google Calendar do not apply to these events. Integrating this feature would enhance user experience by ensuring consistency in how notifications are handled, regardless of how an event is booked.\n\n<sub>[CAL-4716](https://linear.app/calcom/issue/CAL-4716/missing-notifications-for-events)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/16651"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#16651",
              "body": "I propose integrating a feature in Cal.com that allows users to set default Google Calendar notifications for events booked through the platform. This functionality should enable users to select preferred notification times (e.g., 10 minutes, 30 minutes, 1 hour before the event) that automatically apply to all events created via Cal.com bookings. Currently, the absence of such a feature leads to missed appointments since default notifications set in Google Calendar do not apply to these events. Integrating this feature would enhance user experience by ensuring consistency in how notifications are handled, regardless of how an event is booked.\n\n<sub>[CAL-4716](https://linear.app/calcom/issue/CAL-4716/missing-notifications-for-events)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/16651",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#8123",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:35.417Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.417Z",
            "created_at": "2025-12-16T21:16:35.417Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#8123",
              "status": "open",
              "type": "issue",
              "number": 8123,
              "title": "[CAL-1425] Exchange on Premise 2016",
              "source": {
                "data": {
                  "id": "source-cal#8123",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-1425] Exchange on Premise 2016",
                  "body": " > If I use Exchange 2016 i get the message \"cannot added\". and if i use standard Exchange as Option I get \"unauthorized\". but i checked that the EWS login works fine via URL\r\n\r\n\n\n<sub>[CAL-1425](https://linear.app/calcom/issue/CAL-1425/exchange-on-premise-2016)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/8123"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#8123",
              "body": " > If I use Exchange 2016 i get the message \"cannot added\". and if i use standard Exchange as Option I get \"unauthorized\". but i checked that the EWS login works fine via URL\r\n\r\n\n\n<sub>[CAL-1425](https://linear.app/calcom/issue/CAL-1425/exchange-on-premise-2016)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/8123",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#16378",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:35.846Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.846Z",
            "created_at": "2025-12-16T21:16:35.846Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#16378",
              "status": "open",
              "type": "issue",
              "number": 16378,
              "title": "[CAL-4531] Take into account guest's availability when rescheduling",
              "source": {
                "data": {
                  "id": "source-cal#16378",
                  "user": {
                    "login": "pumfleet",
                    "id": 25907159,
                    "node_id": "MDQ6VXNlcjI1OTA3MTU5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25907159?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/pumfleet",
                    "html_url": "https://github.com/pumfleet",
                    "followers_url": "https://api.github.com/users/pumfleet/followers",
                    "following_url": "https://api.github.com/users/pumfleet/following{/other_user}",
                    "gists_url": "https://api.github.com/users/pumfleet/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/pumfleet/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/pumfleet/subscriptions",
                    "organizations_url": "https://api.github.com/users/pumfleet/orgs",
                    "repos_url": "https://api.github.com/users/pumfleet/repos",
                    "events_url": "https://api.github.com/users/pumfleet/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/pumfleet/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-4531] Take into account guest's availability when rescheduling",
                  "body": "If I (a Cal.com user) book another Cal.com user and they want to reschedule it on their end (instead of requesting me to reschedule), it doesn't take into account my availability and just lets them freely choose a time.\r\n\r\nThis is because the person who booked the meeting may or may not be a Cal.com user, hence we don't bother checking for their availability. But we do have the user's email, so we should look up the user and see if they're a Cal.com user, and then if so, retrieve the available times for them, and only display those when the host tries to reschedule\n\n<sub>[CAL-4531](https://linear.app/calcom/issue/CAL-4531/take-into-account-guests-availability-when-rescheduling)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/16378"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#16378",
              "body": "If I (a Cal.com user) book another Cal.com user and they want to reschedule it on their end (instead of requesting me to reschedule), it doesn't take into account my availability and just lets them freely choose a time.\r\n\r\nThis is because the person who booked the meeting may or may not be a Cal.com user, hence we don't bother checking for their availability. But we do have the user's email, so we should look up the user and see if they're a Cal.com user, and then if so, retrieve the available times for them, and only display those when the host tries to reschedule\n\n<sub>[CAL-4531](https://linear.app/calcom/issue/CAL-4531/take-into-account-guests-availability-when-rescheduling)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/16378",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#13532",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-12-16T21:16:36.338Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:36.338Z",
            "created_at": "2025-12-16T21:16:36.338Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#13532",
              "status": "open",
              "type": "issue",
              "number": 13532,
              "title": "[CAL-3076] allow emails and invite people to a team event-type directly from \"assignment\" if not in team yet",
              "source": {
                "data": {
                  "id": "source-cal#13532",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-3076] allow emails and invite people to a team event-type directly from \"assignment\" if not in team yet",
                  "body": "- [ ] allow emails in assignment\n- [ ] invite person if its not in the team yet (as Member)\n- [ ] allow multiple emails comma separated: [user1@example.com](mailto:user1@example.com), [user2@example.com](mailto:user2@example.com)\n\n![CleanShot 2024-02-05 at 10 17 08@2x](https://uploads.linear.app/e86bf957-d82f-465e-b205-135559f4b623/598b18ab-0aed-491d-90f4-cd2afa2f1bbb/24eba054-180f-45cf-b858-4764d896572f?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiL2U4NmJmOTU3LWQ4MmYtNDY1ZS1iMjA1LTEzNTU1OWY0YjYyMy81OThiMThhYi0wYWVkLTQ5MWQtOTBmNC1jZDJhZmEyZjFiYmIvMjRlYmEwNTQtMTgwZi00NWNmLWI4NTgtNDc2NGQ4OTY1NzJmIiwiaWF0IjoxNzA3MTI4MjY3LCJleHAiOjE3MDcyMTQ2Njd9.DQDBQ-o7r2wBUIvnU0ZH2MoSkKZMbzDB9cIHtLUvV4Q)\n\n<sub>From [SyncLinear.com](https://synclinear.com) | [CAL-3076](https://linear.app/calcom/issue/CAL-3076/allow-emails-and-invite-people-to-a-team-event-type-directly-from)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/13532"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#13532",
              "body": "- [ ] allow emails in assignment\n- [ ] invite person if its not in the team yet (as Member)\n- [ ] allow multiple emails comma separated: [user1@example.com](mailto:user1@example.com), [user2@example.com](mailto:user2@example.com)\n\n![CleanShot 2024-02-05 at 10 17 08@2x](https://uploads.linear.app/e86bf957-d82f-465e-b205-135559f4b623/598b18ab-0aed-491d-90f4-cd2afa2f1bbb/24eba054-180f-45cf-b858-4764d896572f?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiL2U4NmJmOTU3LWQ4MmYtNDY1ZS1iMjA1LTEzNTU1OWY0YjYyMy81OThiMThhYi0wYWVkLTQ5MWQtOTBmNC1jZDJhZmEyZjFiYmIvMjRlYmEwNTQtMTgwZi00NWNmLWI4NTgtNDc2NGQ4OTY1NzJmIiwiaWF0IjoxNzA3MTI4MjY3LCJleHAiOjE3MDcyMTQ2Njd9.DQDBQ-o7r2wBUIvnU0ZH2MoSkKZMbzDB9cIHtLUvV4Q)\n\n<sub>From [SyncLinear.com](https://synclinear.com) | [CAL-3076](https://linear.app/calcom/issue/CAL-3076/allow-emails-and-invite-people-to-a-team-event-type-directly-from)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/13532",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#3697",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:35.111Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.111Z",
            "created_at": "2025-12-16T21:16:35.111Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#3697",
              "status": "open",
              "type": "issue",
              "number": 3697,
              "title": "Datastar requests from Endpoint",
              "source": {
                "data": {
                  "id": "source-zio#3697",
                  "user": {
                    "login": "987Nabil",
                    "id": 7283535,
                    "node_id": "MDQ6VXNlcjcyODM1MzU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/7283535?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/987Nabil",
                    "html_url": "https://github.com/987Nabil",
                    "followers_url": "https://api.github.com/users/987Nabil/followers",
                    "following_url": "https://api.github.com/users/987Nabil/following{/other_user}",
                    "gists_url": "https://api.github.com/users/987Nabil/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/987Nabil/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/987Nabil/subscriptions",
                    "organizations_url": "https://api.github.com/users/987Nabil/orgs",
                    "repos_url": "https://api.github.com/users/987Nabil/repos",
                    "events_url": "https://api.github.com/users/987Nabil/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/987Nabil/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Datastar requests from Endpoint",
                  "body": "Build Datastar expressions for request against an Endpoint from its definition\n",
                  "html_url": "https://github.com/zio/zio-http/issues/3697"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#3697",
              "body": "Build Datastar expressions for request against an Endpoint from its definition\n",
              "url": "https://github.com/zio/zio-http/issues/3697",
              "tech": [
                "go"
              ],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#709",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:35.416Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.416Z",
            "created_at": "2025-12-16T21:16:35.416Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#709",
              "status": "open",
              "type": "issue",
              "number": 709,
              "title": "Support Http Range header on request for Files",
              "source": {
                "data": {
                  "id": "source-zio#709",
                  "user": {
                    "login": "ashprakasan",
                    "id": 8946971,
                    "node_id": "MDQ6VXNlcjg5NDY5NzE=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8946971?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ashprakasan",
                    "html_url": "https://github.com/ashprakasan",
                    "followers_url": "https://api.github.com/users/ashprakasan/followers",
                    "following_url": "https://api.github.com/users/ashprakasan/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ashprakasan/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ashprakasan/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ashprakasan/subscriptions",
                    "organizations_url": "https://api.github.com/users/ashprakasan/orgs",
                    "repos_url": "https://api.github.com/users/ashprakasan/repos",
                    "events_url": "https://api.github.com/users/ashprakasan/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ashprakasan/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Support Http Range header on request for Files",
                  "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nThe Range HTTP request header indicates the part of a document that the server should return. Several parts can be requested with one Range header at once, and the server may send back these ranges in a multipart document.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExample requesting 3 ranges from files -\r\n`Range: bytes=200-1000, 2000-6576, 19000-`\r\n\r\nSend only those parts of the document in Response.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCurrently, the range is hardcoded as follows - \r\n` ctx.write(new DefaultFileRegion(raf.getChannel, 0, fileLength))`\r\nManipulate the positions as per request headers instead.\r\n\r\n**Additional context**\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Range\r\n",
                  "html_url": "https://github.com/zio/zio-http/issues/709"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#709",
              "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nThe Range HTTP request header indicates the part of a document that the server should return. Several parts can be requested with one Range header at once, and the server may send back these ranges in a multipart document.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExample requesting 3 ranges from files -\r\n`Range: bytes=200-1000, 2000-6576, 19000-`\r\n\r\nSend only those parts of the document in Response.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCurrently, the range is hardcoded as follows - \r\n` ctx.write(new DefaultFileRegion(raf.getChannel, 0, fileLength))`\r\nManipulate the positions as per request headers instead.\r\n\r\n**Additional context**\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Range\r\n",
              "url": "https://github.com/zio/zio-http/issues/709",
              "tech": [],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#3472",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:35.845Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:35.845Z",
            "created_at": "2025-12-16T21:16:35.845Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#3472",
              "status": "open",
              "type": "issue",
              "number": 3472,
              "title": "Split into multiple modules",
              "source": {
                "data": {
                  "id": "source-zio#3472",
                  "user": {
                    "login": "987Nabil",
                    "id": 7283535,
                    "node_id": "MDQ6VXNlcjcyODM1MzU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/7283535?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/987Nabil",
                    "html_url": "https://github.com/987Nabil",
                    "followers_url": "https://api.github.com/users/987Nabil/followers",
                    "following_url": "https://api.github.com/users/987Nabil/following{/other_user}",
                    "gists_url": "https://api.github.com/users/987Nabil/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/987Nabil/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/987Nabil/subscriptions",
                    "organizations_url": "https://api.github.com/users/987Nabil/orgs",
                    "repos_url": "https://api.github.com/users/987Nabil/repos",
                    "events_url": "https://api.github.com/users/987Nabil/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/987Nabil/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Split into multiple modules",
                  "body": "Currently, the zio-http artifact contains a lot of different parts of zio-http exclusively. They are not separate maven artifacts.\n\nWe want to change this, to support future changes/features.\n\nThere should be at least these modules that are published into maven. \n\n1. core\n2. endpoint\n3. netty\n\nShould we have client and server in different modules?",
                  "html_url": "https://github.com/zio/zio-http/issues/3472"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#3472",
              "body": "Currently, the zio-http artifact contains a lot of different parts of zio-http exclusively. They are not separate maven artifacts.\n\nWe want to change this, to support future changes/features.\n\nThere should be at least these modules that are published into maven. \n\n1. core\n2. endpoint\n3. netty\n\nShould we have client and server in different modules?",
              "url": "https://github.com/zio/zio-http/issues/3472",
              "tech": [],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#9810",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:36.348Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:36.348Z",
            "created_at": "2025-12-16T21:16:36.348Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#9810",
              "status": "open",
              "type": "issue",
              "number": 9810,
              "title": "ZStreams buffer(1) is buffering 2.",
              "source": {
                "data": {
                  "id": "source-zio#9810",
                  "user": {
                    "login": "douglasthomsen",
                    "id": 88000378,
                    "node_id": "MDQ6VXNlcjg4MDAwMzc4",
                    "avatar_url": "https://avatars.githubusercontent.com/u/88000378?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/douglasthomsen",
                    "html_url": "https://github.com/douglasthomsen",
                    "followers_url": "https://api.github.com/users/douglasthomsen/followers",
                    "following_url": "https://api.github.com/users/douglasthomsen/following{/other_user}",
                    "gists_url": "https://api.github.com/users/douglasthomsen/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/douglasthomsen/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/douglasthomsen/subscriptions",
                    "organizations_url": "https://api.github.com/users/douglasthomsen/orgs",
                    "repos_url": "https://api.github.com/users/douglasthomsen/repos",
                    "events_url": "https://api.github.com/users/douglasthomsen/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/douglasthomsen/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "ZStreams buffer(1) is buffering 2.",
                  "body": "I am using zio 2.1.17. When I run the following code:\n\n```scala\ndef fakeNetworkCall(n: Int): ZIO[Any, Throwable, String] = {\n  for {\n    _ <- Console.printLine(s\"Starting request $n\")\n    _ <- ZIO.sleep(1.second)\n    _ <- Console.printLine(s\"Completed request $n\")\n  } yield s\"Response for $n\"\n}\n\nval program: ZIO[Any, Throwable, Unit] =\n  ZStream\n    .fromIterator(Iterator.from(1))\n    .mapZIO(fakeNetworkCall)\n    .buffer(1)\n    .runForeach { response =>\n      for {\n        _ <- Console.printLine(s\"Press Enter to process $response...\")\n        _ <- ZIO.sleep(100.minutes)\n        _ <- Console.printLine(s\"Processing response $response\")\n        _ <- ZIO.sleep(1.second)\n        _ <- Console.printLine(s\"Done processing $response\")\n      } yield ()\n    }\n```\nThe full code is [here](https://scastie.scala-lang.org/douglasthomsen/kvRuhoAGRjarj9djF53N0g/10).\n\nWhen i get to the `_ <- ZIO.sleep(100.minutes)` line I would expect the output to be like this:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\n```\n\nbut I am getting the following:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\nStarting request 3\nCompleted request 3\n```\n\nMy goal is to only buffer one call to `fakeNetworkCall` at time. Right now it looks like it is buffering two. I am I doing something wrong or is this a bug?",
                  "html_url": "https://github.com/zio/zio/issues/9810"
                },
                "type": "github"
              },
              "hash": "zio/zio#9810",
              "body": "I am using zio 2.1.17. When I run the following code:\n\n```scala\ndef fakeNetworkCall(n: Int): ZIO[Any, Throwable, String] = {\n  for {\n    _ <- Console.printLine(s\"Starting request $n\")\n    _ <- ZIO.sleep(1.second)\n    _ <- Console.printLine(s\"Completed request $n\")\n  } yield s\"Response for $n\"\n}\n\nval program: ZIO[Any, Throwable, Unit] =\n  ZStream\n    .fromIterator(Iterator.from(1))\n    .mapZIO(fakeNetworkCall)\n    .buffer(1)\n    .runForeach { response =>\n      for {\n        _ <- Console.printLine(s\"Press Enter to process $response...\")\n        _ <- ZIO.sleep(100.minutes)\n        _ <- Console.printLine(s\"Processing response $response\")\n        _ <- ZIO.sleep(1.second)\n        _ <- Console.printLine(s\"Done processing $response\")\n      } yield ()\n    }\n```\nThe full code is [here](https://scastie.scala-lang.org/douglasthomsen/kvRuhoAGRjarj9djF53N0g/10).\n\nWhen i get to the `_ <- ZIO.sleep(100.minutes)` line I would expect the output to be like this:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\n```\n\nbut I am getting the following:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\nStarting request 3\nCompleted request 3\n```\n\nMy goal is to only buffer one call to `fakeNetworkCall` at time. Right now it looks like it is buffering two. I am I doing something wrong or is this a bug?",
              "url": "https://github.com/zio/zio/issues/9810",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#9844",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:36.765Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:36.765Z",
            "created_at": "2025-12-16T21:16:36.765Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#9844",
              "status": "open",
              "type": "issue",
              "number": 9844,
              "title": "improved `Queue` shutdown functionality",
              "source": {
                "data": {
                  "id": "source-zio#9844",
                  "user": {
                    "login": "mberndt123",
                    "id": 11650737,
                    "node_id": "MDQ6VXNlcjExNjUwNzM3",
                    "avatar_url": "https://avatars.githubusercontent.com/u/11650737?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/mberndt123",
                    "html_url": "https://github.com/mberndt123",
                    "followers_url": "https://api.github.com/users/mberndt123/followers",
                    "following_url": "https://api.github.com/users/mberndt123/following{/other_user}",
                    "gists_url": "https://api.github.com/users/mberndt123/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/mberndt123/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/mberndt123/subscriptions",
                    "organizations_url": "https://api.github.com/users/mberndt123/orgs",
                    "repos_url": "https://api.github.com/users/mberndt123/repos",
                    "events_url": "https://api.github.com/users/mberndt123/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/mberndt123/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "improved `Queue` shutdown functionality",
                  "body": "Hey, I've recently proposed something on Discord, and since feedback has been rather positive, I'm making a ticket to track the idea.\n\nI've been working with Queues recently, and I've been having some issues around `shutdown` that I would like to address.\n\nSpecifically, I find it a common pattern that I send some kind of request object through a queue because I want another fiber to perform some action on my behalf. Along with the request, I send a `Promise` to have that fiber communicate the outcome of that action to me. By and large this works fine. The issue arises when the fiber that I'm sending requests to fails. In that case, I would like it to communicate the cause of the failure back to the other fibers. This is easy enough for the requests that I've already pulled out of the queue: I simply fail those promises.\nBut I also need to deal with other cases: fibers currently blocked in an `offer` call, future attempts to `offer` to the queue, and I also need to deal with requests that have been submitted to the queue but not yet retrieved.\n\nSo my idea is as follows:\n - add an `E` type parameter to `Queue`\n - add a `shutdownCause` method that takes a type parameter of type `Cause[E]`\n - `shutdownCause` would also return the items currently buffered in the queue in order to dispose of them\n - after `shutdownCause` has been called, any attempt to interact with the queue will fail with the cause\n - methods like `take, offer` etc. should indicate errors of type `E`\n - streams created with `ZStream.fromQueue` would also fail with this cause\n - `shutdownCause` should be atomic: when multiple fibers call it at the same time, one of them wins and the others fail with the cause supplied by the winner\n\nAfaik, adding a new method is a binary compatible change, as is adding a new type parameter. Hence I think this is a source incompatible but binary compatible change. @ghostdogpr therefore suggested it could be added in a ZIO 2.2 release.\n",
                  "html_url": "https://github.com/zio/zio/issues/9844"
                },
                "type": "github"
              },
              "hash": "zio/zio#9844",
              "body": "Hey, I've recently proposed something on Discord, and since feedback has been rather positive, I'm making a ticket to track the idea.\n\nI've been working with Queues recently, and I've been having some issues around `shutdown` that I would like to address.\n\nSpecifically, I find it a common pattern that I send some kind of request object through a queue because I want another fiber to perform some action on my behalf. Along with the request, I send a `Promise` to have that fiber communicate the outcome of that action to me. By and large this works fine. The issue arises when the fiber that I'm sending requests to fails. In that case, I would like it to communicate the cause of the failure back to the other fibers. This is easy enough for the requests that I've already pulled out of the queue: I simply fail those promises.\nBut I also need to deal with other cases: fibers currently blocked in an `offer` call, future attempts to `offer` to the queue, and I also need to deal with requests that have been submitted to the queue but not yet retrieved.\n\nSo my idea is as follows:\n - add an `E` type parameter to `Queue`\n - add a `shutdownCause` method that takes a type parameter of type `Cause[E]`\n - `shutdownCause` would also return the items currently buffered in the queue in order to dispose of them\n - after `shutdownCause` has been called, any attempt to interact with the queue will fail with the cause\n - methods like `take, offer` etc. should indicate errors of type `E`\n - streams created with `ZStream.fromQueue` would also fail with this cause\n - `shutdownCause` should be atomic: when multiple fibers call it at the same time, one of them wins and the others fail with the cause supplied by the winner\n\nAfaik, adding a new method is a binary compatible change, as is adding a new type parameter. Hence I think this is a source incompatible but binary compatible change. @ghostdogpr therefore suggested it could be added in a ZIO 2.2 release.\n",
              "url": "https://github.com/zio/zio/issues/9844",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#9878",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:37.337Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:37.337Z",
            "created_at": "2025-12-16T21:16:37.337Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#9878",
              "status": "open",
              "type": "issue",
              "number": 9878,
              "title": "ZScheduler parks+unparks workers too frequently",
              "source": {
                "data": {
                  "id": "source-zio#9878",
                  "user": {
                    "login": "hearnadam",
                    "id": 22334119,
                    "node_id": "MDQ6VXNlcjIyMzM0MTE5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/22334119?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/hearnadam",
                    "html_url": "https://github.com/hearnadam",
                    "followers_url": "https://api.github.com/users/hearnadam/followers",
                    "following_url": "https://api.github.com/users/hearnadam/following{/other_user}",
                    "gists_url": "https://api.github.com/users/hearnadam/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/hearnadam/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/hearnadam/subscriptions",
                    "organizations_url": "https://api.github.com/users/hearnadam/orgs",
                    "repos_url": "https://api.github.com/users/hearnadam/repos",
                    "events_url": "https://api.github.com/users/hearnadam/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/hearnadam/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "ZScheduler parks+unparks workers too frequently",
                  "body": "Unparking workers is slow and invoked in the hotpath too often. I think we may need to trade some fairness for aggression to avoid excessive cycling.\n\n`maybeUnparkWorker` (obviously `LockSupport.unpark(worker)`) is very expensive: https://github.com/zio/zio/blob/series/2.x/core/jvm-native/src/main/scala/zio/internal/ZScheduler.scala#L443-L454",
                  "html_url": "https://github.com/zio/zio/issues/9878"
                },
                "type": "github"
              },
              "hash": "zio/zio#9878",
              "body": "Unparking workers is slow and invoked in the hotpath too often. I think we may need to trade some fairness for aggression to avoid excessive cycling.\n\n`maybeUnparkWorker` (obviously `LockSupport.unpark(worker)`) is very expensive: https://github.com/zio/zio/blob/series/2.x/core/jvm-native/src/main/scala/zio/internal/ZScheduler.scala#L443-L454",
              "url": "https://github.com/zio/zio/issues/9878",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#9877",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:38.117Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:38.117Z",
            "created_at": "2025-12-16T21:16:38.117Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#9877",
              "status": "open",
              "type": "issue",
              "number": 9877,
              "title": "Can Fiber(Runtime) and Promise be merged?",
              "source": {
                "data": {
                  "id": "source-zio#9877",
                  "user": {
                    "login": "hearnadam",
                    "id": 22334119,
                    "node_id": "MDQ6VXNlcjIyMzM0MTE5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/22334119?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/hearnadam",
                    "html_url": "https://github.com/hearnadam",
                    "followers_url": "https://api.github.com/users/hearnadam/followers",
                    "following_url": "https://api.github.com/users/hearnadam/following{/other_user}",
                    "gists_url": "https://api.github.com/users/hearnadam/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/hearnadam/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/hearnadam/subscriptions",
                    "organizations_url": "https://api.github.com/users/hearnadam/orgs",
                    "repos_url": "https://api.github.com/users/hearnadam/repos",
                    "events_url": "https://api.github.com/users/hearnadam/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/hearnadam/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Can Fiber(Runtime) and Promise be merged?",
                  "body": "A Promise awaiting completion is essentially a Fiber parked awaiting an async callback. When a Fiber is forking work (which will eventually complete a promise), then awaiting a Promise, we end up with unnecessary allocations + indirection.\n\nit would be useful to have `Promise.become` or similar to link fibers/promises.",
                  "html_url": "https://github.com/zio/zio/issues/9877"
                },
                "type": "github"
              },
              "hash": "zio/zio#9877",
              "body": "A Promise awaiting completion is essentially a Fiber parked awaiting an async callback. When a Fiber is forking work (which will eventually complete a promise), then awaiting a Promise, we end up with unnecessary allocations + indirection.\n\nit would be useful to have `Promise.become` or similar to link fibers/promises.",
              "url": "https://github.com/zio/zio/issues/9877",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#9874",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:40.942Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:40.942Z",
            "created_at": "2025-12-16T21:16:40.942Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#9874",
              "status": "open",
              "type": "issue",
              "number": 9874,
              "title": "Handling errors allows recovering from defects",
              "source": {
                "data": {
                  "id": "source-zio#9874",
                  "user": {
                    "login": "kyri-petrou",
                    "id": 67301607,
                    "node_id": "MDQ6VXNlcjY3MzAxNjA3",
                    "avatar_url": "https://avatars.githubusercontent.com/u/67301607?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/kyri-petrou",
                    "html_url": "https://github.com/kyri-petrou",
                    "followers_url": "https://api.github.com/users/kyri-petrou/followers",
                    "following_url": "https://api.github.com/users/kyri-petrou/following{/other_user}",
                    "gists_url": "https://api.github.com/users/kyri-petrou/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/kyri-petrou/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/kyri-petrou/subscriptions",
                    "organizations_url": "https://api.github.com/users/kyri-petrou/orgs",
                    "repos_url": "https://api.github.com/users/kyri-petrou/repos",
                    "events_url": "https://api.github.com/users/kyri-petrou/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/kyri-petrou/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Handling errors allows recovering from defects",
                  "body": "Sigh... Well, this is annoying. When a Cause contains both a failure and a defect (i.e., Fail & Die), failure handling assumes that the Cause does not contain any defects and therefor silently ignores them.\n\nRepro:\n\n```scala\nimport zio.*\n\nobject Foo extends ZIOAppDefault {\n  val dieCause: Cause[String] = Cause.die(new RuntimeException(\"boom\"))\n  val combinedCause = dieCause && Cause.fail(\"boom\")\n\n  def run = ZIO.failCause(combinedCause).catchAll { e =>\n    ZIO.debug(e)\n  } *> ZIO.debug(\"Success\")\n}\n```\nprints:\n```\nhandled: boom\nSuccess\n```\n\nHowever if we substituted `ZIO.failCause(combinedCause)` with `ZIO.failCause(dieCause)`:\n\n```\ntimestamp=2025-05-20T16:31:09.291104Z level=ERROR thread=#zio-fiber-1707930317 message=\"\" cause=\"java.lang.RuntimeException: boom\n\tat Foo$.<clinit>(Foo.scala:4)\n\tat Foo.main(Foo.scala)\n\tat <empty>.Foo.run(Foo.scala:7)\n\tat <empty>.Foo.run(Foo.scala:9)\n\"\n```\n\nWith both of these causes, the outcome should be the same as defects should always be prioritised over failures. This gets even worse when interruption is involved, because the failure handling will be prioritised over it.\n\nHaving said that, this has the potential to massively alter applications so I'm too scared to fix it.\n@ghostdogpr @jdegoes  @hearnadam @guizmaii I summon you all for some wisdom",
                  "html_url": "https://github.com/zio/zio/issues/9874"
                },
                "type": "github"
              },
              "hash": "zio/zio#9874",
              "body": "Sigh... Well, this is annoying. When a Cause contains both a failure and a defect (i.e., Fail & Die), failure handling assumes that the Cause does not contain any defects and therefor silently ignores them.\n\nRepro:\n\n```scala\nimport zio.*\n\nobject Foo extends ZIOAppDefault {\n  val dieCause: Cause[String] = Cause.die(new RuntimeException(\"boom\"))\n  val combinedCause = dieCause && Cause.fail(\"boom\")\n\n  def run = ZIO.failCause(combinedCause).catchAll { e =>\n    ZIO.debug(e)\n  } *> ZIO.debug(\"Success\")\n}\n```\nprints:\n```\nhandled: boom\nSuccess\n```\n\nHowever if we substituted `ZIO.failCause(combinedCause)` with `ZIO.failCause(dieCause)`:\n\n```\ntimestamp=2025-05-20T16:31:09.291104Z level=ERROR thread=#zio-fiber-1707930317 message=\"\" cause=\"java.lang.RuntimeException: boom\n\tat Foo$.<clinit>(Foo.scala:4)\n\tat Foo.main(Foo.scala)\n\tat <empty>.Foo.run(Foo.scala:7)\n\tat <empty>.Foo.run(Foo.scala:9)\n\"\n```\n\nWith both of these causes, the outcome should be the same as defects should always be prioritised over failures. This gets even worse when interruption is involved, because the failure handling will be prioritised over it.\n\nHaving said that, this has the potential to massively alter applications so I'm too scared to fix it.\n@ghostdogpr @jdegoes  @hearnadam @guizmaii I summon you all for some wisdom",
              "url": "https://github.com/zio/zio/issues/9874",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#9681",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:43.996Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:43.996Z",
            "created_at": "2025-12-16T21:16:43.996Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#9681",
              "status": "open",
              "type": "issue",
              "number": 9681,
              "title": "Scala Native `WeakConcurrentBag` NPE when forking 10K fibers",
              "source": {
                "data": {
                  "id": "source-zio#9681",
                  "user": {
                    "login": "hearnadam",
                    "id": 22334119,
                    "node_id": "MDQ6VXNlcjIyMzM0MTE5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/22334119?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/hearnadam",
                    "html_url": "https://github.com/hearnadam",
                    "followers_url": "https://api.github.com/users/hearnadam/followers",
                    "following_url": "https://api.github.com/users/hearnadam/following{/other_user}",
                    "gists_url": "https://api.github.com/users/hearnadam/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/hearnadam/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/hearnadam/subscriptions",
                    "organizations_url": "https://api.github.com/users/hearnadam/orgs",
                    "repos_url": "https://api.github.com/users/hearnadam/repos",
                    "events_url": "https://api.github.com/users/hearnadam/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/hearnadam/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Scala Native `WeakConcurrentBag` NPE when forking 10K fibers",
                  "body": "```\n[info]   - PromiseSpec - waiter stack safety\n[info]     Exception in thread \"zio-fiber-931\" java.lang.NullPointerException: null\n[info]     \tat scala.scalanative.runtime.package$.throwNullPointer(Unknown Source)\n[info]     \tat <none>.(Unknown Source)\n[info]     \tat java.util.concurrent.ConcurrentHashMap.treeifyBin(Unknown Source)\n[info]     \tat java.util.concurrent.ConcurrentHashMap.putVal(Unknown Source)\n[info]     \tat java.util.concurrent.ConcurrentHashMap$KeySetView.add(Unknown Source)\n[info]     \tat zio.internal.WeakConcurrentBag.addToLongTermStorage(Unknown Source)\n[info]     \tat zio.internal.WeakConcurrentBag.add(Unknown Source)\n[info]     \tat zio.internal.FiberScope$global$.add(Unknown Source)\n[info]     \tat zio.ZIO$unsafe$.makeChildFiber(Unknown Source)\n[info]     \tat zio.ZIO$unsafe$.fork(Unknown Source)\n[info]     \tat zio.ZIO.$anonfun$forkWithScopeOverride$2(Unknown Source)\n[info]     \tat zio.ZIO$$Lambda$280.apply(Unknown Source)\n[info]     \tat zio.PromiseSpec.spec(PromiseSpec.scala:127)\n[info]     \tat zio.PromiseSpec.spec(PromiseSpec.scala:124)\n```\nI introduced a new test in #9569 which resulted in this strange failure on Native.",
                  "html_url": "https://github.com/zio/zio/issues/9681"
                },
                "type": "github"
              },
              "hash": "zio/zio#9681",
              "body": "```\n[info]   - PromiseSpec - waiter stack safety\n[info]     Exception in thread \"zio-fiber-931\" java.lang.NullPointerException: null\n[info]     \tat scala.scalanative.runtime.package$.throwNullPointer(Unknown Source)\n[info]     \tat <none>.(Unknown Source)\n[info]     \tat java.util.concurrent.ConcurrentHashMap.treeifyBin(Unknown Source)\n[info]     \tat java.util.concurrent.ConcurrentHashMap.putVal(Unknown Source)\n[info]     \tat java.util.concurrent.ConcurrentHashMap$KeySetView.add(Unknown Source)\n[info]     \tat zio.internal.WeakConcurrentBag.addToLongTermStorage(Unknown Source)\n[info]     \tat zio.internal.WeakConcurrentBag.add(Unknown Source)\n[info]     \tat zio.internal.FiberScope$global$.add(Unknown Source)\n[info]     \tat zio.ZIO$unsafe$.makeChildFiber(Unknown Source)\n[info]     \tat zio.ZIO$unsafe$.fork(Unknown Source)\n[info]     \tat zio.ZIO.$anonfun$forkWithScopeOverride$2(Unknown Source)\n[info]     \tat zio.ZIO$$Lambda$280.apply(Unknown Source)\n[info]     \tat zio.PromiseSpec.spec(PromiseSpec.scala:127)\n[info]     \tat zio.PromiseSpec.spec(PromiseSpec.scala:124)\n```\nI introduced a new test in #9569 which resulted in this strange failure on Native.",
              "url": "https://github.com/zio/zio/issues/9681",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "zio#9922",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "zio",
              "id": "generated-zio",
              "name": "Zio",
              "description": "",
              "members": [],
              "display_name": "Zio",
              "created_at": "2025-12-16T21:16:47.207Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/zio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:47.207Z",
            "created_at": "2025-12-16T21:16:47.207Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-zio#9922",
              "status": "open",
              "type": "issue",
              "number": 9922,
              "title": "Add tests for `ZStreamAspect`",
              "source": {
                "data": {
                  "id": "source-zio#9922",
                  "user": {
                    "login": "kyri-petrou",
                    "id": 67301607,
                    "node_id": "MDQ6VXNlcjY3MzAxNjA3",
                    "avatar_url": "https://avatars.githubusercontent.com/u/67301607?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/kyri-petrou",
                    "html_url": "https://github.com/kyri-petrou",
                    "followers_url": "https://api.github.com/users/kyri-petrou/followers",
                    "following_url": "https://api.github.com/users/kyri-petrou/following{/other_user}",
                    "gists_url": "https://api.github.com/users/kyri-petrou/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/kyri-petrou/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/kyri-petrou/subscriptions",
                    "organizations_url": "https://api.github.com/users/kyri-petrou/orgs",
                    "repos_url": "https://api.github.com/users/kyri-petrou/repos",
                    "events_url": "https://api.github.com/users/kyri-petrou/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/kyri-petrou/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Add tests for `ZStreamAspect`",
                  "body": "Currently there are no tests for `ZStreamAspect`. We should add a suite that tests each of the provided aspects",
                  "html_url": "https://github.com/zio/zio/issues/9922"
                },
                "type": "github"
              },
              "hash": "zio/zio#9922",
              "body": "Currently there are no tests for `ZStreamAspect`. We should add a suite that tests each of the provided aspects",
              "url": "https://github.com/zio/zio/issues/9922",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "triggerdotdev#2654",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "triggerdotdev",
              "id": "generated-triggerdotdev",
              "name": "Triggerdotdev",
              "description": "",
              "members": [],
              "display_name": "Triggerdotdev",
              "created_at": "2025-12-16T21:16:55.468Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/triggerdotdev?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "colinhacks",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-16T21:16:55.468Z",
            "created_at": "2025-12-16T21:16:55.468Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-triggerdotdev#2654",
              "status": "open",
              "type": "issue",
              "number": 2654,
              "title": "Schema in object being inferred differently (and weirdly)",
              "source": {
                "data": {
                  "id": "source-triggerdotdev#2654",
                  "user": {
                    "login": "ericallam",
                    "id": 534,
                    "node_id": "MDQ6VXNlcjUzNA==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/534?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ericallam",
                    "html_url": "https://github.com/ericallam",
                    "followers_url": "https://api.github.com/users/ericallam/followers",
                    "following_url": "https://api.github.com/users/ericallam/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ericallam/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ericallam/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ericallam/subscriptions",
                    "organizations_url": "https://api.github.com/users/ericallam/orgs",
                    "repos_url": "https://api.github.com/users/ericallam/repos",
                    "events_url": "https://api.github.com/users/ericallam/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ericallam/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Schema in object being inferred differently (and weirdly)",
                  "body": "See the following TS snippet:\r\n\r\n```ts\r\nimport { z } from \"zod\";\r\n\r\nconst EventNameSchema = z.string().or(z.array(z.string()));\r\n\r\ntype EventName = z.infer<typeof EventNameSchema>;\r\n// EventName is string | string[]\r\n\r\nconst EventSchema = z.object({\r\n  name: z.string().or(z.array(z.string())) // this is the same as the EventNameSchema\r\n});\r\n\r\ntype EventWithName = z.infer<typeof EventSchema>;\r\ntype EventName2 = EventWithName[\"name\"];\r\n// EventName2 is (string | string[]) & (string | string[] | undefined)\r\n```\r\n\r\nAnd the TS playground: [link](https://www.typescriptlang.org/play?#code/JYWwDg9gTgLgBAbzgLzgXzgMyhEcBEyEAJvgNwBQFAxhAHYDO8AogG4CmdMAcgIYjsAytQAW7ELzgBeFADomUYHQDmACgCUs6KuSzeUKLwCeO+TEUqN69ZQowjYdnDace-JzN1LM7KAB57RwhMZw4uPgFhMQkAPkoAenjQ1winYAY4BSVlOAAfTPNsgG0AXSpaRhYwmCjxSU8tACMAK3ZqGFUECjg4OncALjksy01tXX1DE11htWt1CjQbKkCnFy4AdWAYEVTpOW9fAId2YOSuWtjKFbO3AQAmPbWYTe3Uovw+gXwShKSn1Ie6TgqhmeQKFmUpXUcAAZMDQfkZqUwQBXOjEdiYJTsYjqIA)\r\n\r\nI'm not sure if this is intended or a bug or maybe just a user error. Using zod `3.21.4` and TS `4.8.4`",
                  "html_url": "https://github.com/colinhacks/zod/issues/2654"
                },
                "type": "github"
              },
              "hash": "colinhacks/zod#2654",
              "body": "See the following TS snippet:\r\n\r\n```ts\r\nimport { z } from \"zod\";\r\n\r\nconst EventNameSchema = z.string().or(z.array(z.string()));\r\n\r\ntype EventName = z.infer<typeof EventNameSchema>;\r\n// EventName is string | string[]\r\n\r\nconst EventSchema = z.object({\r\n  name: z.string().or(z.array(z.string())) // this is the same as the EventNameSchema\r\n});\r\n\r\ntype EventWithName = z.infer<typeof EventSchema>;\r\ntype EventName2 = EventWithName[\"name\"];\r\n// EventName2 is (string | string[]) & (string | string[] | undefined)\r\n```\r\n\r\nAnd the TS playground: [link](https://www.typescriptlang.org/play?#code/JYWwDg9gTgLgBAbzgLzgXzgMyhEcBEyEAJvgNwBQFAxhAHYDO8AogG4CmdMAcgIYjsAytQAW7ELzgBeFADomUYHQDmACgCUs6KuSzeUKLwCeO+TEUqN69ZQowjYdnDace-JzN1LM7KAB57RwhMZw4uPgFhMQkAPkoAenjQ1winYAY4BSVlOAAfTPNsgG0AXSpaRhYwmCjxSU8tACMAK3ZqGFUECjg4OncALjksy01tXX1DE11htWt1CjQbKkCnFy4AdWAYEVTpOW9fAId2YOSuWtjKFbO3AQAmPbWYTe3Uovw+gXwShKSn1Ie6TgqhmeQKFmUpXUcAAZMDQfkZqUwQBXOjEdiYJTsYjqIA)\r\n\r\nI'm not sure if this is intended or a bug or maybe just a user error. Using zod `3.21.4` and TS `4.8.4`",
              "url": "https://github.com/colinhacks/zod/issues/2654",
              "tech": [
                "go"
              ],
              "repo_name": "zod",
              "repo_owner": "colinhacks",
              "forge": "github"
            },
            "timeouts_disabled": false
          }
        ],
        "next_cursor": null
      }
    }
  }
}