{
  "result": {
    "data": {
      "json": {
        "items": [
          {
            "id": "GolemCloud#275",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "GolemCloud",
              "id": "generated-GolemCloud",
              "name": "GolemCloud",
              "description": "",
              "members": [],
              "display_name": "GolemCloud",
              "created_at": "2025-11-09T14:40:46.156Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/GolemCloud?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "golemcloud",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.156Z",
            "created_at": "2025-11-09T14:40:46.156Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-GolemCloud#275",
              "status": "open",
              "type": "issue",
              "number": 275,
              "title": "Incorporate MCP Server into Golem CLI",
              "source": {
                "data": {
                  "id": "source-GolemCloud#275",
                  "user": {
                    "login": "jdegoes",
                    "id": 156745,
                    "node_id": "MDQ6VXNlcjE1Njc0NQ==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/156745?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/jdegoes",
                    "html_url": "https://github.com/jdegoes",
                    "followers_url": "https://api.github.com/users/jdegoes/followers",
                    "following_url": "https://api.github.com/users/jdegoes/following{/other_user}",
                    "gists_url": "https://api.github.com/users/jdegoes/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/jdegoes/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/jdegoes/subscriptions",
                    "organizations_url": "https://api.github.com/users/jdegoes/orgs",
                    "repos_url": "https://api.github.com/users/jdegoes/repos",
                    "events_url": "https://api.github.com/users/jdegoes/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/jdegoes/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Incorporate MCP Server into Golem CLI",
                  "body": "With a new command, Golem CLI enters serve mode:\n\n```bash\n%> golem-cli --serve --serve-port 1232\n%> golem-cli running MCP Server at port 1232\n```\n\nIn this mode, Golem CLI creates an MCP Server that exposes different commands as different tools, and exposes relevant resources (the manifest file in the current, ancestor, and children directories) as resources.\n\nWhen this ticket is completed, it should be possible to use an agent such as Claude Code to perform anything that Golem CLI can do. Moreover, all individual tools and resources must be end-to-end tested with an MCP Client that interacts with the MCP Server.\n\n[This library](https://github.com/rust-mcp-stack/rust-mcp-sdk) looks like the one to use for Rust, but perhaps there are better options available.",
                  "html_url": "https://github.com/golemcloud/golem/issues/1926"
                },
                "type": "github"
              },
              "hash": "golemcloud/golem-cli#275",
              "body": "With a new command, Golem CLI enters serve mode:\n\n```bash\n%> golem-cli --serve --serve-port 1232\n%> golem-cli running MCP Server at port 1232\n```\n\nIn this mode, Golem CLI creates an MCP Server that exposes different commands as different tools, and exposes relevant resources (the manifest file in the current, ancestor, and children directories) as resources.\n\nWhen this ticket is completed, it should be possible to use an agent such as Claude Code to perform anything that Golem CLI can do. Moreover, all individual tools and resources must be end-to-end tested with an MCP Client that interacts with the MCP Server.\n\n[This library](https://github.com/rust-mcp-stack/rust-mcp-sdk) looks like the one to use for Rust, but perhaps there are better options available.",
              "url": "https://github.com/golemcloud/golem/issues/1926",
              "tech": [
                "go"
              ],
              "repo_name": "golem-cli",
              "repo_owner": "golemcloud",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "GolemCloud#23",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "GolemCloud",
              "id": "generated-GolemCloud",
              "name": "GolemCloud",
              "description": "",
              "members": [],
              "display_name": "GolemCloud",
              "created_at": "2025-11-09T14:40:46.392Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/GolemCloud?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "golemcloud",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.392Z",
            "created_at": "2025-11-09T14:40:46.392Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-GolemCloud#23",
              "status": "open",
              "type": "issue",
              "number": 23,
              "title": "Implement Durable Text-to-Speech Provider Components for golem:tts WIT Interface",
              "source": {
                "data": {
                  "id": "source-GolemCloud#23",
                  "user": {
                    "login": "jdegoes",
                    "id": 156745,
                    "node_id": "MDQ6VXNlcjE1Njc0NQ==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/156745?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/jdegoes",
                    "html_url": "https://github.com/jdegoes",
                    "followers_url": "https://api.github.com/users/jdegoes/followers",
                    "following_url": "https://api.github.com/users/jdegoes/following{/other_user}",
                    "gists_url": "https://api.github.com/users/jdegoes/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/jdegoes/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/jdegoes/subscriptions",
                    "organizations_url": "https://api.github.com/users/jdegoes/orgs",
                    "repos_url": "https://api.github.com/users/jdegoes/repos",
                    "events_url": "https://api.github.com/users/jdegoes/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/jdegoes/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Implement Durable Text-to-Speech Provider Components for golem:tts WIT Interface",
                  "body": "I have attached to this ticket a WIT file that describes a generic interface for text-to-speech operations. This interface can be implemented by various providers, either by emulating features not present in a given provider, utilizing the provider's native support for a feature, or indicating an error if a particular combination is not natively supported by a provider.\n\nThe intent of this WIT specification is to allow developers of WASM components (on wasmCloud, Spin, or Golem) to leverage text-to-speech capabilities to build voice-powered applications, accessibility services, and audio content generation systems in a portable and provider-agnostic fashion.\n\nThis ticket involves constructing implementations of this WIT interface for the following providers:\n\n- **ElevenLabs**: The leading AI voice synthesis platform with comprehensive voice cloning, real-time streaming, voice conversion, and sound effects generation capabilities.\n- **AWS Polly**: Amazon's enterprise text-to-speech service with extensive language support, custom lexicons, speech marks, and asynchronous synthesis for long-form content.\n- **Google Cloud Text-to-Speech**: Google's neural voice synthesis service with WaveNet and Neural2 voices, device optimization profiles, and streaming synthesis capabilities.\n- **Deepgram Aura**: High-performance real-time TTS with session-based streaming, low-latency neural voices, and conversational AI optimization.\n\nThese implementations must be written in Rust and compilable to WASM Components (WASI 0.23 only, since Golem does not yet support WASI 0.3). The standard Rust toolchain for WASM component development can be employed (see cargo component and the Rust examples of components in this and other Golem repositories).\n\nAdditionally, these implementations should incorporate custom durability semantics using the Golem durability API and the Golem host API. This approach ensures that durability is managed at the level of individual TTS operations (voice synthesis, streaming session creation, voice cloning, batch processing), providing a higher-level and clearer operation log, which aids in debugging and monitoring. See golem:llm and golem:embed for more details and durable implementations in this same repository.\n\nThe final deliverables associated with this ticket are:\n\n- **ElevenLabs implementation**: A WASM Component (WASI 0.23), named `tts-elevenlabs.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n- **AWS Polly implementation**: A WASM Component (WASI 0.23), named `tts-polly.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n- **Google Cloud TTS implementation**: A WASM Component (WASI 0.23), named `tts-google.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n- **Deepgram Aura implementation**: A WASM Component (WASI 0.23), named `tts-deepgram.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n\n**Note**: If you have a strong recommendation to swap out one or two of these with other popular / common TTS providers (such as Azure Cognitive Services Speech, IBM Watson Text to Speech, or OpenAI TTS), then as long as you get permission beforehand, that's okay with me. However, we definitely need ElevenLabs and AWS Polly.\n\nThese components will require runtime configuration, notably API keys, endpoint URLs, authentication credentials, and provider-specific settings. For configuring this information, the components can use environment variables for now (in the future, they will use wasi-runtime-config, but Golem does not support this yet, whereas Golem has good support for environment variables).\n\nMoreover, the Rust components need to be tested within Golem to ensure compatibility with Golem 1.2.x.\n\nThis WIT has been designed by examining and comparing the APIs of ElevenLabs, AWS Polly, Google Cloud TTS, Azure Speech Services, OpenAI TTS, and Deepgram Aura. However, given there are no implementations, it is possible the provided WIT is not the optimal abstraction across all these providers. Therefore, deviations from the proposed design can be made. However, to be accepted, any deviation must be fully justified and deemed by Golem core contributors to be an improvement from the original specification.\n\n## Implementation Guidelines\n\nEach provider implementation should handle the following key mapping considerations:\n\n- **Voice Management**: Map the unified voice resource to provider-specific voice identifiers, handle voice discovery and metadata appropriately for each provider's voice catalog structure\n- **Audio Format Conversion**: Implement native audio format support where available, or provide format conversion for unsupported output formats using audio processing libraries\n- **Streaming Implementation**: Utilize native streaming APIs where supported (ElevenLabs, Deepgram), or implement chunk-based synthesis for providers without native streaming support\n- **Authentication Handling**: Implement appropriate authentication mechanisms (API keys, OAuth, service accounts) per provider requirements\n- **Feature Availability**: Route advanced features (voice cloning, sound effects, speech marks) through provider-native APIs where supported, or return `unsupported-operation` errors for unavailable features\n- **Error Mapping**: Map provider-specific HTTP errors and API responses to the unified `tts-error` enumeration with appropriate context preservation\n- **Rate Limiting**: Handle provider-specific rate limits and quota management, implementing appropriate retry logic and error reporting\n- **Long-form Content**: Implement efficient handling of long-form synthesis using provider-native async operations (AWS Polly) or intelligent chunking strategies\n\n## Testing Requirements\n\nEach implementation must include comprehensive test suites covering:\n- Basic synthesis operations (text-to-speech with various voices and configurations)\n- Voice discovery and metadata retrieval\n- Streaming synthesis lifecycle (session creation, chunk processing, cleanup)\n- Advanced feature testing (voice cloning, sound effects, custom pronunciations where supported)\n- Audio format validation and quality verification\n- Authentication and authorization scenarios\n- Error handling for unsupported operations and malformed inputs\n- Rate limiting and quota management behavior\n- Connection management and retry logic\n- Long-form content synthesis (>5000 characters)\n- Durability semantics verification across operation boundaries\n- Provider-specific feature utilization (lexicons for Polly, voice settings for ElevenLabs, etc.)\n\n## Configuration Requirements\n\nEach implementation should support the following environment variables:\n\n### Common Configuration\n- `TTS_PROVIDER_ENDPOINT`: Custom endpoint URL (for enterprise/regional deployments)\n- `TTS_PROVIDER_TIMEOUT`: Request timeout in seconds (default: 30)\n- `TTS_PROVIDER_MAX_RETRIES`: Maximum retry attempts (default: 3)\n- `TTS_PROVIDER_LOG_LEVEL`: Logging verbosity (debug, info, warn, error)\n\n### Provider-Specific Configuration\n- **ElevenLabs**: `ELEVENLABS_API_KEY`, `ELEVENLABS_MODEL_VERSION`\n- **AWS Polly**: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`, `AWS_SESSION_TOKEN`\n- **Google Cloud**: `GOOGLE_APPLICATION_CREDENTIALS`, `GOOGLE_CLOUD_PROJECT`\n- **Deepgram**: `DEEPGRAM_API_KEY`, `DEEPGRAM_API_VERSION`\n\n```wit\npackage golem:tts@1.0.0;\n\n/// Core types and error handling for universal text-to-speech\ninterface types {\n    /// Comprehensive error types covering all TTS operations\n    variant tts-error {\n        /// Input validation errors\n        invalid-text(string),\n        text-too-long(u32),\n        invalid-ssml(string),\n        unsupported-language(string),\n        \n        /// Voice and model errors\n        voice-not-found(string),\n        model-not-found(string),\n        voice-unavailable(string),\n        \n        /// Authentication and authorization\n        unauthorized(string),\n        access-denied(string),\n        \n        /// Resource and quota limits\n        quota-exceeded(quota-info),\n        rate-limited(u32),\n        insufficient-credits,\n        \n        /// Operation errors\n        synthesis-failed(string),\n        unsupported-operation(string),\n        invalid-configuration(string),\n        \n        /// Service errors\n        service-unavailable(string),\n        network-error(string),\n        internal-error(string),\n        \n        /// Storage errors (for async operations)\n        invalid-storage-location(string),\n        storage-access-denied(string),\n    }\n\n    record quota-info {\n        used: u32,\n        limit: u32,\n        reset-time: u64,\n        unit: quota-unit,\n    }\n\n    enum quota-unit {\n        characters,\n        requests,\n        seconds,\n        credits,\n    }\n\n    /// Language identification using BCP 47 codes\n    type language-code = string;\n\n    /// Voice gender classification\n    enum voice-gender {\n        male,\n        female,\n        neutral,\n    }\n\n    /// Voice quality tiers\n    enum voice-quality {\n        standard,\n        premium,\n        neural,\n        studio,\n    }\n\n    /// Text input types\n    enum text-type {\n        plain,\n        ssml,\n    }\n\n    /// Audio output formats\n    enum audio-format {\n        mp3,\n        wav,\n        pcm,\n        ogg-opus,\n        aac,\n        flac,\n        mulaw,\n        alaw,\n    }\n\n    /// Audio quality settings\n    record audio-config {\n        format: audio-format,\n        sample-rate: option<u32>,\n        bit-rate: option<u32>,\n        channels: option<u8>,\n    }\n\n    /// Voice synthesis parameters\n    record voice-settings {\n        /// Speaking rate (0.25 to 4.0, default 1.0)\n        speed: option<f32>,\n        /// Pitch adjustment in semitones (-20.0 to 20.0, default 0.0)\n        pitch: option<f32>,\n        /// Volume gain in dB (-96.0 to 16.0, default 0.0)\n        volume: option<f32>,\n        /// Voice stability (0.0 to 1.0, provider-specific)\n        stability: option<f32>,\n        /// Similarity to original (0.0 to 1.0, provider-specific)\n        similarity: option<f32>,\n        /// Style exaggeration (0.0 to 1.0, provider-specific)\n        style: option<f32>,\n    }\n\n    /// Audio effects and device optimization\n    flags audio-effects {\n        telephone-quality,\n        headphone-optimized,\n        speaker-optimized,\n        car-audio-optimized,\n        noise-reduction,\n        bass-boost,\n        treble-boost,\n    }\n\n    /// Input text with metadata\n    record text-input {\n        content: string,\n        text-type: text-type,\n        language: option<language-code>,\n    }\n\n    /// Complete synthesis result\n    record synthesis-result {\n        audio-data: list<u8>,\n        metadata: synthesis-metadata,\n    }\n\n    /// Metadata about synthesized audio\n    record synthesis-metadata {\n        duration-seconds: f32,\n        character-count: u32,\n        word-count: u32,\n        audio-size-bytes: u32,\n        request-id: string,\n        provider-info: option<string>,\n    }\n\n    /// Streaming audio chunk\n    record audio-chunk {\n        data: list<u8>,\n        sequence-number: u32,\n        is-final: bool,\n        timing-info: option<timing-info>,\n    }\n\n    /// Timing and synchronization information\n    record timing-info {\n        start-time-seconds: f32,\n        end-time-seconds: option<f32>,\n        text-offset: option<u32>,\n        mark-type: option<timing-mark-type>,\n    }\n\n    enum timing-mark-type {\n        word,\n        sentence,\n        paragraph,\n        ssml-mark,\n        viseme,\n    }\n\n\n}\n\n/// Voice discovery and management\ninterface voices {\n    use types.{tts-error, language-code, voice-gender, voice-quality};\n\n    /// Represents a voice that can be used for speech synthesis\n    resource voice {\n        /// Get voice identification\n        get-id: func() -> string;\n        get-name: func() -> string;\n        get-provider-id: func() -> option<string>;\n        \n        /// Get voice characteristics\n        get-language: func() -> language-code;\n        get-additional-languages: func() -> list<language-code>;\n        get-gender: func() -> voice-gender;\n        get-quality: func() -> voice-quality;\n        get-description: func() -> option<string>;\n        \n        /// Voice capabilities\n        supports-ssml: func() -> bool;\n        get-sample-rates: func() -> list<u32>;\n        get-supported-formats: func() -> list<types.audio-format>;\n        \n        /// Voice management (may return unsupported-operation)\n        update-settings: func(settings: types.voice-settings) -> result<_, tts-error>;\n        delete: func() -> result<_, tts-error>;\n        clone: func() -> result<voice, tts-error>;\n        \n        /// Preview voice with sample text\n        preview: func(text: string) -> result<list<u8>, tts-error>;\n    }\n\n    /// Voice search and filtering\n    record voice-filter {\n        language: option<language-code>,\n        gender: option<voice-gender>,\n        quality: option<voice-quality>,\n        supports-ssml: option<bool>,\n        provider: option<string>,\n        search-query: option<string>,\n    }\n\n    /// Detailed voice information\n    record voice-info {\n        id: string,\n        name: string,\n        language: language-code,\n        additional-languages: list<language-code>,\n        gender: voice-gender,\n        quality: voice-quality,\n        description: option<string>,\n        provider: string,\n        sample-rate: u32,\n        is-custom: bool,\n        is-cloned: bool,\n        preview-url: option<string>,\n        use-cases: list<string>,\n    }\n\n    /// Resource-based iterator for voice results\n    resource voice-results {\n        /// Check if more voices are available\n        has-more: func() -> bool;\n        \n        /// Get next batch of voices\n        get-next: func() -> result<list<voice-info>, tts-error>;\n        \n        /// Get total count if available\n        get-total-count: func() -> option<u32>;\n    }\n\n    /// List available voices with filtering and pagination\n    list-voices: func(\n        filter: option<voice-filter>\n    ) -> result<voice-results, tts-error>;\n\n    /// Get specific voice by ID\n    get-voice: func(voice-id: string) -> result<voice, tts-error>;\n\n    /// Search voices by characteristics\n    search-voices: func(\n        query: string,\n        filter: option<voice-filter>\n    ) -> result<list<voice-info>, tts-error>;\n\n    /// Get supported languages\n    list-languages: func() -> result<list<language-info>, tts-error>;\n\n    record language-info {\n        code: language-code,\n        name: string,\n        native-name: string,\n        voice-count: u32,\n    }\n}\n\n/// Core text-to-speech synthesis operations\ninterface synthesis {\n    use types.{\n        text-input, audio-config, voice-settings, audio-effects,\n        synthesis-result, tts-error, timing-info\n    };\n    use voices.{voice};\n\n    /// Synthesis configuration options\n    record synthesis-options {\n        audio-config: option<audio-config>,\n        voice-settings: option<voice-settings>,\n        audio-effects: option<audio-effects>,\n        enable-timing: option<bool>,\n        enable-word-timing: option<bool>,\n        seed: option<u32>,\n        model-version: option<string>,\n        context: option<synthesis-context>,\n    }\n\n    /// Context for better synthesis quality\n    record synthesis-context {\n        previous-text: option<string>,\n        next-text: option<string>,\n        topic: option<string>,\n        emotion: option<string>,\n        speaking-style: option<string>,\n    }\n\n    /// Convert text to speech (removed async)\n    synthesize: func(\n        input: text-input,\n        voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<synthesis-result, tts-error>;\n\n    /// Batch synthesis for multiple inputs (removed async)\n    synthesize-batch: func(\n        inputs: list<text-input>,\n        voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<list<synthesis-result>, tts-error>;\n\n    /// Get timing information without audio synthesis\n    get-timing-marks: func(\n        input: text-input,\n        voice: borrow<voice>\n    ) -> result<list<timing-info>, tts-error>;\n\n    /// Validate text before synthesis\n    validate-input: func(\n        input: text-input,\n        voice: borrow<voice>\n    ) -> result<validation-result, tts-error>;\n\n    record validation-result {\n        is-valid: bool,\n        character-count: u32,\n        estimated-duration: option<f32>,\n        warnings: list<string>,\n        errors: list<string>,\n    }\n}\n\n/// Real-time streaming synthesis\ninterface streaming {\n    use types.{\n        text-input, audio-config, voice-settings, audio-chunk,\n        tts-error, timing-info\n    };\n    use voices.{voice};\n    use synthesis.{synthesis-options};\n\n    /// Streaming synthesis session\n    resource synthesis-stream {\n        /// Send text for synthesis (can be called multiple times)\n        send-text: func(input: text-input) -> result<_, tts-error>;\n        \n        /// Signal end of input and flush remaining audio\n        finish: func() -> result<_, tts-error>;\n        \n        /// Receive next audio chunk (non-blocking)\n        receive-chunk: func() -> result<option<audio-chunk>, tts-error>;\n        \n        /// Check if more chunks are available\n        has-pending-audio: func() -> bool;\n        \n        /// Get current stream status\n        get-status: func() -> stream-status;\n        \n        /// Close stream and clean up resources\n        close: func();\n    }\n\n    enum stream-status {\n        ready,\n        processing,\n        finished,\n        error,\n        closed,\n    }\n\n    /// Create streaming synthesis session\n    create-stream: func(\n        voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<synthesis-stream, tts-error>;\n\n    /// Real-time voice conversion streaming\n    create-voice-conversion-stream: func(\n        target-voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<voice-conversion-stream, tts-error>;\n\n    resource voice-conversion-stream {\n        /// Send input audio chunks\n        send-audio: func(audio-data: list<u8>) -> result<_, tts-error>;\n        \n        /// Receive converted audio chunks\n        receive-converted: func() -> result<option<audio-chunk>, tts-error>;\n        \n        finish: func() -> result<_, tts-error>;\n        close: func();\n    }\n}\n\n/// Advanced TTS features and voice manipulation\ninterface advanced {\n    use types.{tts-error, audio-config, language-code};\n    use voices.{voice};\n\n    /// Voice cloning and creation (removed async)\n    create-voice-clone: func(\n        name: string,\n        audio-samples: list<audio-sample>,\n        description: option<string>\n    ) -> result<voice, tts-error>;\n\n    record audio-sample {\n        data: list<u8>,\n        transcript: option<string>,\n        quality-rating: option<u8>,\n    }\n\n    /// Design synthetic voice (removed async)\n    design-voice: func(\n        name: string,\n        characteristics: voice-design-params\n    ) -> result<voice, tts-error>;\n\n    record voice-design-params {\n        gender: types.voice-gender,\n        age-category: age-category,\n        accent: string,\n        personality-traits: list<string>,\n        reference-voice: option<string>,\n    }\n\n    enum age-category {\n        child,\n        young-adult,\n        middle-aged,\n        elderly,\n    }\n\n    /// Voice-to-voice conversion (removed async)\n    convert-voice: func(\n        input-audio: list<u8>,\n        target-voice: borrow<voice>,\n        preserve-timing: option<bool>\n    ) -> result<list<u8>, tts-error>;\n\n    /// Generate sound effects from text description (removed async)\n    generate-sound-effect: func(\n        description: string,\n        duration-seconds: option<f32>,\n        style-influence: option<f32>\n    ) -> result<list<u8>, tts-error>;\n\n    /// Custom pronunciation management\n    resource pronunciation-lexicon {\n        get-name: func() -> string;\n        get-language: func() -> language-code;\n        get-entry-count: func() -> u32;\n        \n        /// Add pronunciation rule\n        add-entry: func(word: string, pronunciation: string) -> result<_, tts-error>;\n        \n        /// Remove pronunciation rule\n        remove-entry: func(word: string) -> result<_, tts-error>;\n        \n        /// Export lexicon content\n        export-content: func() -> result<string, tts-error>;\n    }\n\n    /// Create custom pronunciation lexicon\n    create-lexicon: func(\n        name: string,\n        language: language-code,\n        entries: option<list<pronunciation-entry>>\n    ) -> result<pronunciation-lexicon, tts-error>;\n\n    record pronunciation-entry {\n        word: string,\n        pronunciation: string,\n        part-of-speech: option<string>,\n    }\n\n    /// Long-form content synthesis with optimization (removed async)\n    synthesize-long-form: func(\n        content: string,\n        voice: borrow<voice>,\n        output-location: string,\n        chapter-breaks: option<list<u32>>\n    ) -> result<long-form-operation, tts-error>;\n\n    resource long-form-operation {\n        get-status: func() -> operation-status;\n        get-progress: func() -> f32;\n        cancel: func() -> result<_, tts-error>;\n        get-result: func() -> result<long-form-result, tts-error>;\n    }\n\n    enum operation-status {\n        pending,\n        processing,\n        completed,\n        failed,\n        cancelled,\n    }\n\n    record long-form-result {\n        output-location: string,\n        total-duration: f32,\n        chapter-durations: option<list<f32>>,\n        metadata: types.synthesis-metadata,\n    }\n}\n```",
                  "html_url": "https://github.com/golemcloud/golem-ai/issues/23"
                },
                "type": "github"
              },
              "hash": "golemcloud/golem-ai#23",
              "body": "I have attached to this ticket a WIT file that describes a generic interface for text-to-speech operations. This interface can be implemented by various providers, either by emulating features not present in a given provider, utilizing the provider's native support for a feature, or indicating an error if a particular combination is not natively supported by a provider.\n\nThe intent of this WIT specification is to allow developers of WASM components (on wasmCloud, Spin, or Golem) to leverage text-to-speech capabilities to build voice-powered applications, accessibility services, and audio content generation systems in a portable and provider-agnostic fashion.\n\nThis ticket involves constructing implementations of this WIT interface for the following providers:\n\n- **ElevenLabs**: The leading AI voice synthesis platform with comprehensive voice cloning, real-time streaming, voice conversion, and sound effects generation capabilities.\n- **AWS Polly**: Amazon's enterprise text-to-speech service with extensive language support, custom lexicons, speech marks, and asynchronous synthesis for long-form content.\n- **Google Cloud Text-to-Speech**: Google's neural voice synthesis service with WaveNet and Neural2 voices, device optimization profiles, and streaming synthesis capabilities.\n- **Deepgram Aura**: High-performance real-time TTS with session-based streaming, low-latency neural voices, and conversational AI optimization.\n\nThese implementations must be written in Rust and compilable to WASM Components (WASI 0.23 only, since Golem does not yet support WASI 0.3). The standard Rust toolchain for WASM component development can be employed (see cargo component and the Rust examples of components in this and other Golem repositories).\n\nAdditionally, these implementations should incorporate custom durability semantics using the Golem durability API and the Golem host API. This approach ensures that durability is managed at the level of individual TTS operations (voice synthesis, streaming session creation, voice cloning, batch processing), providing a higher-level and clearer operation log, which aids in debugging and monitoring. See golem:llm and golem:embed for more details and durable implementations in this same repository.\n\nThe final deliverables associated with this ticket are:\n\n- **ElevenLabs implementation**: A WASM Component (WASI 0.23), named `tts-elevenlabs.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n- **AWS Polly implementation**: A WASM Component (WASI 0.23), named `tts-polly.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n- **Google Cloud TTS implementation**: A WASM Component (WASI 0.23), named `tts-google.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n- **Deepgram Aura implementation**: A WASM Component (WASI 0.23), named `tts-deepgram.wasm`, with a full test suite and custom durability implementation at the level of TTS operations.\n\n**Note**: If you have a strong recommendation to swap out one or two of these with other popular / common TTS providers (such as Azure Cognitive Services Speech, IBM Watson Text to Speech, or OpenAI TTS), then as long as you get permission beforehand, that's okay with me. However, we definitely need ElevenLabs and AWS Polly.\n\nThese components will require runtime configuration, notably API keys, endpoint URLs, authentication credentials, and provider-specific settings. For configuring this information, the components can use environment variables for now (in the future, they will use wasi-runtime-config, but Golem does not support this yet, whereas Golem has good support for environment variables).\n\nMoreover, the Rust components need to be tested within Golem to ensure compatibility with Golem 1.2.x.\n\nThis WIT has been designed by examining and comparing the APIs of ElevenLabs, AWS Polly, Google Cloud TTS, Azure Speech Services, OpenAI TTS, and Deepgram Aura. However, given there are no implementations, it is possible the provided WIT is not the optimal abstraction across all these providers. Therefore, deviations from the proposed design can be made. However, to be accepted, any deviation must be fully justified and deemed by Golem core contributors to be an improvement from the original specification.\n\n## Implementation Guidelines\n\nEach provider implementation should handle the following key mapping considerations:\n\n- **Voice Management**: Map the unified voice resource to provider-specific voice identifiers, handle voice discovery and metadata appropriately for each provider's voice catalog structure\n- **Audio Format Conversion**: Implement native audio format support where available, or provide format conversion for unsupported output formats using audio processing libraries\n- **Streaming Implementation**: Utilize native streaming APIs where supported (ElevenLabs, Deepgram), or implement chunk-based synthesis for providers without native streaming support\n- **Authentication Handling**: Implement appropriate authentication mechanisms (API keys, OAuth, service accounts) per provider requirements\n- **Feature Availability**: Route advanced features (voice cloning, sound effects, speech marks) through provider-native APIs where supported, or return `unsupported-operation` errors for unavailable features\n- **Error Mapping**: Map provider-specific HTTP errors and API responses to the unified `tts-error` enumeration with appropriate context preservation\n- **Rate Limiting**: Handle provider-specific rate limits and quota management, implementing appropriate retry logic and error reporting\n- **Long-form Content**: Implement efficient handling of long-form synthesis using provider-native async operations (AWS Polly) or intelligent chunking strategies\n\n## Testing Requirements\n\nEach implementation must include comprehensive test suites covering:\n- Basic synthesis operations (text-to-speech with various voices and configurations)\n- Voice discovery and metadata retrieval\n- Streaming synthesis lifecycle (session creation, chunk processing, cleanup)\n- Advanced feature testing (voice cloning, sound effects, custom pronunciations where supported)\n- Audio format validation and quality verification\n- Authentication and authorization scenarios\n- Error handling for unsupported operations and malformed inputs\n- Rate limiting and quota management behavior\n- Connection management and retry logic\n- Long-form content synthesis (>5000 characters)\n- Durability semantics verification across operation boundaries\n- Provider-specific feature utilization (lexicons for Polly, voice settings for ElevenLabs, etc.)\n\n## Configuration Requirements\n\nEach implementation should support the following environment variables:\n\n### Common Configuration\n- `TTS_PROVIDER_ENDPOINT`: Custom endpoint URL (for enterprise/regional deployments)\n- `TTS_PROVIDER_TIMEOUT`: Request timeout in seconds (default: 30)\n- `TTS_PROVIDER_MAX_RETRIES`: Maximum retry attempts (default: 3)\n- `TTS_PROVIDER_LOG_LEVEL`: Logging verbosity (debug, info, warn, error)\n\n### Provider-Specific Configuration\n- **ElevenLabs**: `ELEVENLABS_API_KEY`, `ELEVENLABS_MODEL_VERSION`\n- **AWS Polly**: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`, `AWS_SESSION_TOKEN`\n- **Google Cloud**: `GOOGLE_APPLICATION_CREDENTIALS`, `GOOGLE_CLOUD_PROJECT`\n- **Deepgram**: `DEEPGRAM_API_KEY`, `DEEPGRAM_API_VERSION`\n\n```wit\npackage golem:tts@1.0.0;\n\n/// Core types and error handling for universal text-to-speech\ninterface types {\n    /// Comprehensive error types covering all TTS operations\n    variant tts-error {\n        /// Input validation errors\n        invalid-text(string),\n        text-too-long(u32),\n        invalid-ssml(string),\n        unsupported-language(string),\n        \n        /// Voice and model errors\n        voice-not-found(string),\n        model-not-found(string),\n        voice-unavailable(string),\n        \n        /// Authentication and authorization\n        unauthorized(string),\n        access-denied(string),\n        \n        /// Resource and quota limits\n        quota-exceeded(quota-info),\n        rate-limited(u32),\n        insufficient-credits,\n        \n        /// Operation errors\n        synthesis-failed(string),\n        unsupported-operation(string),\n        invalid-configuration(string),\n        \n        /// Service errors\n        service-unavailable(string),\n        network-error(string),\n        internal-error(string),\n        \n        /// Storage errors (for async operations)\n        invalid-storage-location(string),\n        storage-access-denied(string),\n    }\n\n    record quota-info {\n        used: u32,\n        limit: u32,\n        reset-time: u64,\n        unit: quota-unit,\n    }\n\n    enum quota-unit {\n        characters,\n        requests,\n        seconds,\n        credits,\n    }\n\n    /// Language identification using BCP 47 codes\n    type language-code = string;\n\n    /// Voice gender classification\n    enum voice-gender {\n        male,\n        female,\n        neutral,\n    }\n\n    /// Voice quality tiers\n    enum voice-quality {\n        standard,\n        premium,\n        neural,\n        studio,\n    }\n\n    /// Text input types\n    enum text-type {\n        plain,\n        ssml,\n    }\n\n    /// Audio output formats\n    enum audio-format {\n        mp3,\n        wav,\n        pcm,\n        ogg-opus,\n        aac,\n        flac,\n        mulaw,\n        alaw,\n    }\n\n    /// Audio quality settings\n    record audio-config {\n        format: audio-format,\n        sample-rate: option<u32>,\n        bit-rate: option<u32>,\n        channels: option<u8>,\n    }\n\n    /// Voice synthesis parameters\n    record voice-settings {\n        /// Speaking rate (0.25 to 4.0, default 1.0)\n        speed: option<f32>,\n        /// Pitch adjustment in semitones (-20.0 to 20.0, default 0.0)\n        pitch: option<f32>,\n        /// Volume gain in dB (-96.0 to 16.0, default 0.0)\n        volume: option<f32>,\n        /// Voice stability (0.0 to 1.0, provider-specific)\n        stability: option<f32>,\n        /// Similarity to original (0.0 to 1.0, provider-specific)\n        similarity: option<f32>,\n        /// Style exaggeration (0.0 to 1.0, provider-specific)\n        style: option<f32>,\n    }\n\n    /// Audio effects and device optimization\n    flags audio-effects {\n        telephone-quality,\n        headphone-optimized,\n        speaker-optimized,\n        car-audio-optimized,\n        noise-reduction,\n        bass-boost,\n        treble-boost,\n    }\n\n    /// Input text with metadata\n    record text-input {\n        content: string,\n        text-type: text-type,\n        language: option<language-code>,\n    }\n\n    /// Complete synthesis result\n    record synthesis-result {\n        audio-data: list<u8>,\n        metadata: synthesis-metadata,\n    }\n\n    /// Metadata about synthesized audio\n    record synthesis-metadata {\n        duration-seconds: f32,\n        character-count: u32,\n        word-count: u32,\n        audio-size-bytes: u32,\n        request-id: string,\n        provider-info: option<string>,\n    }\n\n    /// Streaming audio chunk\n    record audio-chunk {\n        data: list<u8>,\n        sequence-number: u32,\n        is-final: bool,\n        timing-info: option<timing-info>,\n    }\n\n    /// Timing and synchronization information\n    record timing-info {\n        start-time-seconds: f32,\n        end-time-seconds: option<f32>,\n        text-offset: option<u32>,\n        mark-type: option<timing-mark-type>,\n    }\n\n    enum timing-mark-type {\n        word,\n        sentence,\n        paragraph,\n        ssml-mark,\n        viseme,\n    }\n\n\n}\n\n/// Voice discovery and management\ninterface voices {\n    use types.{tts-error, language-code, voice-gender, voice-quality};\n\n    /// Represents a voice that can be used for speech synthesis\n    resource voice {\n        /// Get voice identification\n        get-id: func() -> string;\n        get-name: func() -> string;\n        get-provider-id: func() -> option<string>;\n        \n        /// Get voice characteristics\n        get-language: func() -> language-code;\n        get-additional-languages: func() -> list<language-code>;\n        get-gender: func() -> voice-gender;\n        get-quality: func() -> voice-quality;\n        get-description: func() -> option<string>;\n        \n        /// Voice capabilities\n        supports-ssml: func() -> bool;\n        get-sample-rates: func() -> list<u32>;\n        get-supported-formats: func() -> list<types.audio-format>;\n        \n        /// Voice management (may return unsupported-operation)\n        update-settings: func(settings: types.voice-settings) -> result<_, tts-error>;\n        delete: func() -> result<_, tts-error>;\n        clone: func() -> result<voice, tts-error>;\n        \n        /// Preview voice with sample text\n        preview: func(text: string) -> result<list<u8>, tts-error>;\n    }\n\n    /// Voice search and filtering\n    record voice-filter {\n        language: option<language-code>,\n        gender: option<voice-gender>,\n        quality: option<voice-quality>,\n        supports-ssml: option<bool>,\n        provider: option<string>,\n        search-query: option<string>,\n    }\n\n    /// Detailed voice information\n    record voice-info {\n        id: string,\n        name: string,\n        language: language-code,\n        additional-languages: list<language-code>,\n        gender: voice-gender,\n        quality: voice-quality,\n        description: option<string>,\n        provider: string,\n        sample-rate: u32,\n        is-custom: bool,\n        is-cloned: bool,\n        preview-url: option<string>,\n        use-cases: list<string>,\n    }\n\n    /// Resource-based iterator for voice results\n    resource voice-results {\n        /// Check if more voices are available\n        has-more: func() -> bool;\n        \n        /// Get next batch of voices\n        get-next: func() -> result<list<voice-info>, tts-error>;\n        \n        /// Get total count if available\n        get-total-count: func() -> option<u32>;\n    }\n\n    /// List available voices with filtering and pagination\n    list-voices: func(\n        filter: option<voice-filter>\n    ) -> result<voice-results, tts-error>;\n\n    /// Get specific voice by ID\n    get-voice: func(voice-id: string) -> result<voice, tts-error>;\n\n    /// Search voices by characteristics\n    search-voices: func(\n        query: string,\n        filter: option<voice-filter>\n    ) -> result<list<voice-info>, tts-error>;\n\n    /// Get supported languages\n    list-languages: func() -> result<list<language-info>, tts-error>;\n\n    record language-info {\n        code: language-code,\n        name: string,\n        native-name: string,\n        voice-count: u32,\n    }\n}\n\n/// Core text-to-speech synthesis operations\ninterface synthesis {\n    use types.{\n        text-input, audio-config, voice-settings, audio-effects,\n        synthesis-result, tts-error, timing-info\n    };\n    use voices.{voice};\n\n    /// Synthesis configuration options\n    record synthesis-options {\n        audio-config: option<audio-config>,\n        voice-settings: option<voice-settings>,\n        audio-effects: option<audio-effects>,\n        enable-timing: option<bool>,\n        enable-word-timing: option<bool>,\n        seed: option<u32>,\n        model-version: option<string>,\n        context: option<synthesis-context>,\n    }\n\n    /// Context for better synthesis quality\n    record synthesis-context {\n        previous-text: option<string>,\n        next-text: option<string>,\n        topic: option<string>,\n        emotion: option<string>,\n        speaking-style: option<string>,\n    }\n\n    /// Convert text to speech (removed async)\n    synthesize: func(\n        input: text-input,\n        voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<synthesis-result, tts-error>;\n\n    /// Batch synthesis for multiple inputs (removed async)\n    synthesize-batch: func(\n        inputs: list<text-input>,\n        voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<list<synthesis-result>, tts-error>;\n\n    /// Get timing information without audio synthesis\n    get-timing-marks: func(\n        input: text-input,\n        voice: borrow<voice>\n    ) -> result<list<timing-info>, tts-error>;\n\n    /// Validate text before synthesis\n    validate-input: func(\n        input: text-input,\n        voice: borrow<voice>\n    ) -> result<validation-result, tts-error>;\n\n    record validation-result {\n        is-valid: bool,\n        character-count: u32,\n        estimated-duration: option<f32>,\n        warnings: list<string>,\n        errors: list<string>,\n    }\n}\n\n/// Real-time streaming synthesis\ninterface streaming {\n    use types.{\n        text-input, audio-config, voice-settings, audio-chunk,\n        tts-error, timing-info\n    };\n    use voices.{voice};\n    use synthesis.{synthesis-options};\n\n    /// Streaming synthesis session\n    resource synthesis-stream {\n        /// Send text for synthesis (can be called multiple times)\n        send-text: func(input: text-input) -> result<_, tts-error>;\n        \n        /// Signal end of input and flush remaining audio\n        finish: func() -> result<_, tts-error>;\n        \n        /// Receive next audio chunk (non-blocking)\n        receive-chunk: func() -> result<option<audio-chunk>, tts-error>;\n        \n        /// Check if more chunks are available\n        has-pending-audio: func() -> bool;\n        \n        /// Get current stream status\n        get-status: func() -> stream-status;\n        \n        /// Close stream and clean up resources\n        close: func();\n    }\n\n    enum stream-status {\n        ready,\n        processing,\n        finished,\n        error,\n        closed,\n    }\n\n    /// Create streaming synthesis session\n    create-stream: func(\n        voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<synthesis-stream, tts-error>;\n\n    /// Real-time voice conversion streaming\n    create-voice-conversion-stream: func(\n        target-voice: borrow<voice>,\n        options: option<synthesis-options>\n    ) -> result<voice-conversion-stream, tts-error>;\n\n    resource voice-conversion-stream {\n        /// Send input audio chunks\n        send-audio: func(audio-data: list<u8>) -> result<_, tts-error>;\n        \n        /// Receive converted audio chunks\n        receive-converted: func() -> result<option<audio-chunk>, tts-error>;\n        \n        finish: func() -> result<_, tts-error>;\n        close: func();\n    }\n}\n\n/// Advanced TTS features and voice manipulation\ninterface advanced {\n    use types.{tts-error, audio-config, language-code};\n    use voices.{voice};\n\n    /// Voice cloning and creation (removed async)\n    create-voice-clone: func(\n        name: string,\n        audio-samples: list<audio-sample>,\n        description: option<string>\n    ) -> result<voice, tts-error>;\n\n    record audio-sample {\n        data: list<u8>,\n        transcript: option<string>,\n        quality-rating: option<u8>,\n    }\n\n    /// Design synthetic voice (removed async)\n    design-voice: func(\n        name: string,\n        characteristics: voice-design-params\n    ) -> result<voice, tts-error>;\n\n    record voice-design-params {\n        gender: types.voice-gender,\n        age-category: age-category,\n        accent: string,\n        personality-traits: list<string>,\n        reference-voice: option<string>,\n    }\n\n    enum age-category {\n        child,\n        young-adult,\n        middle-aged,\n        elderly,\n    }\n\n    /// Voice-to-voice conversion (removed async)\n    convert-voice: func(\n        input-audio: list<u8>,\n        target-voice: borrow<voice>,\n        preserve-timing: option<bool>\n    ) -> result<list<u8>, tts-error>;\n\n    /// Generate sound effects from text description (removed async)\n    generate-sound-effect: func(\n        description: string,\n        duration-seconds: option<f32>,\n        style-influence: option<f32>\n    ) -> result<list<u8>, tts-error>;\n\n    /// Custom pronunciation management\n    resource pronunciation-lexicon {\n        get-name: func() -> string;\n        get-language: func() -> language-code;\n        get-entry-count: func() -> u32;\n        \n        /// Add pronunciation rule\n        add-entry: func(word: string, pronunciation: string) -> result<_, tts-error>;\n        \n        /// Remove pronunciation rule\n        remove-entry: func(word: string) -> result<_, tts-error>;\n        \n        /// Export lexicon content\n        export-content: func() -> result<string, tts-error>;\n    }\n\n    /// Create custom pronunciation lexicon\n    create-lexicon: func(\n        name: string,\n        language: language-code,\n        entries: option<list<pronunciation-entry>>\n    ) -> result<pronunciation-lexicon, tts-error>;\n\n    record pronunciation-entry {\n        word: string,\n        pronunciation: string,\n        part-of-speech: option<string>,\n    }\n\n    /// Long-form content synthesis with optimization (removed async)\n    synthesize-long-form: func(\n        content: string,\n        voice: borrow<voice>,\n        output-location: string,\n        chapter-breaks: option<list<u32>>\n    ) -> result<long-form-operation, tts-error>;\n\n    resource long-form-operation {\n        get-status: func() -> operation-status;\n        get-progress: func() -> f32;\n        cancel: func() -> result<_, tts-error>;\n        get-result: func() -> result<long-form-result, tts-error>;\n    }\n\n    enum operation-status {\n        pending,\n        processing,\n        completed,\n        failed,\n        cancelled,\n    }\n\n    record long-form-result {\n        output-location: string,\n        total-duration: f32,\n        chapter-durations: option<list<f32>>,\n        metadata: types.synthesis-metadata,\n    }\n}\n```",
              "url": "https://github.com/golemcloud/golem-ai/issues/23",
              "tech": [
                "go"
              ],
              "repo_name": "golem-ai",
              "repo_owner": "golemcloud",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "GolemCloud#21",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "GolemCloud",
              "id": "generated-GolemCloud",
              "name": "GolemCloud",
              "description": "",
              "members": [],
              "display_name": "GolemCloud",
              "created_at": "2025-11-09T14:40:46.669Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/GolemCloud?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "golemcloud",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.669Z",
            "created_at": "2025-11-09T14:40:46.669Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-GolemCloud#21",
              "status": "open",
              "type": "issue",
              "number": 21,
              "title": "Implement Durable Vector Database Provider Components for golem:vector WIT Interface",
              "source": {
                "data": {
                  "id": "source-GolemCloud#21",
                  "user": {
                    "login": "jdegoes",
                    "id": 156745,
                    "node_id": "MDQ6VXNlcjE1Njc0NQ==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/156745?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/jdegoes",
                    "html_url": "https://github.com/jdegoes",
                    "followers_url": "https://api.github.com/users/jdegoes/followers",
                    "following_url": "https://api.github.com/users/jdegoes/following{/other_user}",
                    "gists_url": "https://api.github.com/users/jdegoes/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/jdegoes/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/jdegoes/subscriptions",
                    "organizations_url": "https://api.github.com/users/jdegoes/orgs",
                    "repos_url": "https://api.github.com/users/jdegoes/repos",
                    "events_url": "https://api.github.com/users/jdegoes/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/jdegoes/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Implement Durable Vector Database Provider Components for golem:vector WIT Interface",
                  "body": "I have attached to this ticket a WIT file that describes a generic interface for vector database operations. This interface can be implemented by various providers, either by emulating features not present in a given provider, utilizing the provider's native support for a feature, or indicating an error if a particular combination is not natively supported by a provider.\n\nThe intent of this WIT specification is to allow developers of WASM components (on wasmCloud, Spin, or Golem) to leverage vector database capabilities to build agents and services in a portable and provider-agnostic fashion.\n\nThis ticket involves constructing implementations of this WIT interface for the following providers:\n\n- **Qdrant**: Offers advanced vector similarity search with support for sparse vectors, recommendations, and discovery operations.\n- **Pinecone**: Provides managed vector database services with hybrid vector support and namespace organization.\n- **Milvus**: Supplies enterprise-grade vector database with comprehensive data types and clustering capabilities.\n- **pgvector**: Extends PostgreSQL with vector operations, binary vectors, and mathematical functions.\n\nThese implementations must be written in Rust and compilable to WASM Components (WASI 0.23 only, since Golem does not yet support WASI 0.3). The standard Rust toolchain for WASM component development can be employed (see cargo component and the Rust examples of components in this and other Golem repositories).\n\nAdditionally, these implementations should incorporate custom durability semantics using the Golem durability API and the Golem host API. This approach ensures that durability is managed at the level of individual vector operations (upsert, search, delete), providing a higher-level and clearer operation log, which aids in debugging and monitoring. See `golem:llm` and `golem:embed` for more details and durable implementations in this same repository.\n\nThe final deliverables associated with this ticket are:\n\n- **Qdrant implementation**: A WASM Component (WASI 0.2), named `vector-qdrant.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n- **Pinecone implementation**: A WASM Component (WASI 0.2), named `vector-pinecone.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n- **Milvus implementation**: A WASM Component (WASI 0.2), named `vector-milvus.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n- **pgvector implementation**: A WASM Component (WASI 0.2), named `vector-pgvector.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n\nNote: If you have a strong recommendation to swap out one or two of these with other popular / common vector databases, then as long as you get permission beforehand, that's okay with me. However, we definitely need Pinecone, pgvector, and Qdrant. \n\nThese components will require runtime configuration, notably API keys, connection strings, and database credentials. For configuring this information, the components can use environment variables for now (in the future, they will use wasi-runtime-config, but Golem does not support this yet, whereas Golem has good support for environment variables).\n\nMoreover, the Rust components need to be tested within Golem to ensure compatibility with Golem 1.2.x.\n\nThis WIT has been designed by examining and comparing the APIs of Qdrant, Pinecone, Chroma, Weaviate, Milvus, and pgvector. However, given there are no implementations, it is possible the provided WIT is not the optimal abstraction across all these providers. Therefore, deviations from the proposed design can be made. However, to be accepted, any deviation must be fully justified and deemed by Golem core contributors to be an improvement from the original specification.\n\n```wit\npackage golem:vector@1.0.0;\n\n/// Core types and fundamental data structures for vector operations\ninterface types {\n    /// Unique identifier for vectors and collections\n    type id = string;\n    \n    /// Standard dense vector representation\n    type dense-vector = list<f32>;\n    \n    /// Sparse vector with explicit indices\n    record sparse-vector {\n        /// Zero-based indices of non-zero elements\n        indices: list<u32>,\n        /// Values corresponding to the indices\n        values: list<f32>,\n        /// Total dimensionality of the vector space\n        total-dimensions: u32,\n    }\n    \n    /// Binary vector representation\n    record binary-vector {\n        /// Packed binary data\n        data: list<u8>,\n        /// Number of bits/dimensions\n        dimensions: u32,\n    }\n    \n    /// Half-precision vector (16-bit floats)\n    record half-vector {\n        /// Half-precision values (represented as f32 for compatibility)\n        data: list<f32>,\n        /// Number of dimensions\n        dimensions: u32,\n    }\n    \n    /// Vector data supporting multiple representations\n    variant vector-data {\n        /// Standard 32-bit floating point vector\n        dense(dense-vector),\n        /// Sparse vector representation\n        sparse(sparse-vector),\n        /// Binary/bit vector\n        binary(binary-vector),\n        /// Half-precision vector\n        half(half-vector),\n        /// Named vectors for multi-vector collections\n        named(list<tuple<string, dense-vector>>),\n        /// Hybrid dense + sparse combination\n        hybrid(tuple<dense-vector, sparse-vector>),\n    }\n    \n    /// Supported distance metrics\n    enum distance-metric {\n        /// Cosine similarity (1 - cosine distance)\n        cosine,\n        /// Euclidean (L2) distance\n        euclidean,\n        /// Dot product / inner product\n        dot-product,\n        /// Manhattan (L1) distance\n        manhattan,\n        /// Hamming distance (for binary vectors)\n        hamming,\n        /// Jaccard distance (for binary/sparse vectors)\n        jaccard,\n    }\n    \n    /// Metadata value types\n    variant metadata-value {\n        string-val(string),\n        number-val(f64),\n        integer-val(s64),\n        boolean-val(bool),\n        array-val(list<metadata-value>),\n        object-val(list<tuple<string, metadata-value>>),\n        null-val,\n        /// Geographic coordinates\n        geo-val(geo-coordinates),\n        /// ISO 8601 datetime string\n        datetime-val(string),\n        /// Binary data\n        blob-val(list<u8>),\n    }\n    \n    /// Geographic coordinates\n    record geo-coordinates {\n        latitude: f64,\n        longitude: f64,\n    }\n    \n    /// Key-value metadata\n    type metadata = list<tuple<string, metadata-value>>;\n    \n    /// Filter operators for metadata queries\n    enum filter-operator {\n        /// Equal to\n        eq,\n        /// Not equal to\n        ne,\n        /// Greater than\n        gt,\n        /// Greater than or equal\n        gte,\n        /// Less than\n        lt,\n        /// Less than or equal\n        lte,\n        /// Value is in list\n        %in,\n        /// Value is not in list\n        nin,\n        /// Text contains substring (case insensitive)\n        contains,\n        /// Text doesn't contain substring\n        not-contains,\n        /// Regular expression match\n        regex,\n        /// Geographic distance within radius\n        geo-within,\n        /// Geographic bounding box\n        geo-bbox,\n    }\n    \n    /// Basic filter condition\n    record filter-condition {\n        /// Field path (supports nested fields with dot notation)\n        field: string,\n        /// Filter operator\n        operator: filter-operator,\n        /// Value to compare against\n        value: metadata-value,\n    }\n    \n    /// Complex filter expressions with boolean logic\n    variant filter-expression {\n        /// Simple condition\n        condition(filter-condition),\n        /// Logical AND of multiple expressions\n        and(list<filter-expression>),\n        /// Logical OR of multiple expressions\n        or(list<filter-expression>),\n        /// Logical NOT of expression\n        not(filter-expression),\n    }\n    \n    /// Vector record for storage operations\n    record vector-record {\n        /// Unique identifier\n        id: id,\n        /// Vector data\n        vector: vector-data,\n        /// Associated metadata\n        metadata: option<metadata>,\n    }\n    \n    /// Search result with similarity score\n    record search-result {\n        /// Vector identifier\n        id: id,\n        /// Similarity score (higher = more similar)\n        score: f32,\n        /// Distance from query vector (lower = more similar)\n        distance: f32,\n        /// Vector data (if requested)\n        vector: option<vector-data>,\n        /// Associated metadata (if requested)\n        metadata: option<metadata>,\n    }\n    \n    /// Standard error types\n    variant vector-error {\n        /// Resource not found\n        not-found(string),\n        /// Resource already exists\n        already-exists(string),\n        /// Invalid parameters or configuration\n        invalid-params(string),\n        /// Feature not supported by this provider\n        unsupported-feature(string),\n        /// Vector dimension mismatch\n        dimension-mismatch(string),\n        /// Invalid vector format or data\n        invalid-vector(string),\n        /// Authentication/authorization failure\n        unauthorized(string),\n        /// Rate limit exceeded\n        rate-limited(string),\n        /// Internal provider error\n        provider-error(string),\n        /// Network/connection issues\n        connection-error(string),\n    }\n    \n\n}\n\n/// Collection/index management and configuration\ninterface collections {\n    use types.{id, distance-metric, vector-error};\n    \n    /// Index configuration parameters\n    record index-config {\n        index-type: option<string>,\n        parameters: list<tuple<string, string>>,\n    }\n    \n    /// Collection information and statistics\n    record collection-info {\n        name: string,\n        description: option<string>,\n        dimension: u32,\n        metric: distance-metric,\n        vector-count: u64,\n        size-bytes: option<u64>,\n        index-ready: bool,\n        created-at: option<u64>,\n        updated-at: option<u64>,\n        provider-stats: option<types.metadata>,\n    }\n    \n    /// Create or update collection (upsert)\n    upsert-collection: func(\n        name: string,\n        description: option<string>,\n        dimension: u32,\n        metric: distance-metric,\n        index-config: option<index-config>,\n        metadata: option<types.metadata>\n    ) -> result<collection-info, vector-error>;\n    \n    /// List all collections\n    list-collections: func() -> result<list<collection-info>, vector-error>;\n    \n    /// Get collection information\n    get-collection: func(name: string) -> result<collection-info, vector-error>;\n    \n    /// Update collection metadata only\n    update-collection: func(\n        name: string,\n        description: option<string>,\n        metadata: option<types.metadata>\n    ) -> result<collection-info, vector-error>;\n    \n    /// Delete collection and all vectors\n    delete-collection: func(name: string) -> result<_, vector-error>;\n    \n    /// Check if collection exists\n    collection-exists: func(name: string) -> result<bool, vector-error>;\n}\n\n/// Core vector operations (CRUD)\ninterface vectors {\n    use types.{id, vector-record, vector-data, metadata, filter-expression, vector-error};\n    \n    /// Batch operation result\n    record batch-result {\n        success-count: u32,\n        failure-count: u32,\n        errors: list<tuple<u32, vector-error>>,\n    }\n    \n    /// List response with pagination\n    record list-response {\n        vectors: list<vector-record>,\n        next-cursor: option<string>,\n        total-count: option<u64>,\n    }\n    \n    /// Upsert vectors into collection\n    upsert-vectors: func(\n        collection: string,\n        vectors: list<vector-record>,\n        namespace: option<string>\n    ) -> result<batch-result, vector-error>;\n    \n    /// Upsert single vector (convenience)\n    upsert-vector: func(\n        collection: string,\n        id: id,\n        vector: vector-data,\n        metadata: option<metadata>,\n        namespace: option<string>\n    ) -> result<_, vector-error>;\n    \n    /// Get vectors by IDs\n    get-vectors: func(\n        collection: string,\n        ids: list<id>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<vector-record>, vector-error>;\n    \n    /// Get single vector by ID (convenience)\n    get-vector: func(\n        collection: string,\n        id: id,\n        namespace: option<string>\n    ) -> result<option<vector-record>, vector-error>;\n    \n    /// Update vector in place\n    update-vector: func(\n        collection: string,\n        id: id,\n        vector: option<vector-data>,\n        metadata: option<metadata>,\n        namespace: option<string>,\n        merge-metadata: option<bool>\n    ) -> result<_, vector-error>;\n    \n    /// Delete vectors by IDs\n    delete-vectors: func(\n        collection: string,\n        ids: list<id>,\n        namespace: option<string>\n    ) -> result<u32, vector-error>;\n    \n    /// Delete vectors by filter\n    delete-by-filter: func(\n        collection: string,\n        filter: filter-expression,\n        namespace: option<string>\n    ) -> result<u32, vector-error>;\n    \n    /// Delete all vectors in namespace\n    delete-namespace: func(\n        collection: string,\n        namespace: string\n    ) -> result<u32, vector-error>;\n    \n    /// List vectors with filtering and pagination\n    list-vectors: func(\n        collection: string,\n        namespace: option<string>,\n        filter: option<filter-expression>,\n        limit: option<u32>,\n        cursor: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list-response, vector-error>;\n    \n    /// Count vectors matching filter\n    count-vectors: func(\n        collection: string,\n        filter: option<filter-expression>,\n        namespace: option<string>\n    ) -> result<u64, vector-error>;\n}\n\n/// Core similarity search operations\ninterface search {\n    use types.{id, vector-data, search-result, filter-expression, vector-error};\n    \n    /// Search query variants\n    variant search-query {\n        vector(vector-data),\n        by-id(id),\n        multi-vector(list<tuple<string, vector-data>>),\n    }\n    \n    /// Similarity search\n    search-vectors: func(\n        collection: string,\n        query: search-query,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>,\n        min-score: option<f32>,\n        max-distance: option<f32>,\n        search-params: option<list<tuple<string, string>>>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Simple vector similarity search (convenience)\n    find-similar: func(\n        collection: string,\n        vector: vector-data,\n        limit: u32,\n        namespace: option<string>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Batch similarity search\n    batch-search: func(\n        collection: string,\n        queries: list<search-query>,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>,\n        search-params: option<list<tuple<string, string>>>\n    ) -> result<list<list<search-result>>, vector-error>;\n}\n}\n\n/// Extended search capabilities (provider-dependent)\ninterface search-extended {\n    use types.{id, vector-data, search-result, filter-expression, vector-error, metadata-value};\n    \n    /// Recommendation example types\n    variant recommendation-example {\n        vector-id(id),\n        vector-data(vector-data),\n    }\n    \n    enum recommendation-strategy {\n        average-vector,\n        best-score,\n        centroid,\n    }\n    \n    /// Context pair for discovery\n    record context-pair {\n        positive: recommendation-example,\n        negative: recommendation-example,\n    }\n    \n    /// Grouped search result\n    record grouped-search-result {\n        group-value: metadata-value,\n        results: list<search-result>,\n        group-count: u32,\n    }\n    \n    /// Recommendation-based search\n    recommend-vectors: func(\n        collection: string,\n        positive: list<recommendation-example>,\n        negative: option<list<recommendation-example>>,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        strategy: option<recommendation-strategy>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Discovery/context-based search\n    discover-vectors: func(\n        collection: string,\n        context-pairs: list<context-pair>,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Grouped search for diverse results\n    search-groups: func(\n        collection: string,\n        query: search.search-query,\n        group-by: string,\n        group-size: u32,\n        max-groups: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<grouped-search-result>, vector-error>;\n    \n    /// Range search within distance bounds\n    search-range: func(\n        collection: string,\n        vector: vector-data,\n        min-distance: option<f32>,\n        max-distance: f32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        limit: option<u32>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Text/document search (auto-embedding)\n    search-text: func(\n        collection: string,\n        query-text: string,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>\n    ) -> result<list<search-result>, vector-error>;\n}\n\n/// Analytics and statistics\ninterface analytics {\n    use types.{vector-error, metadata-value, filter-expression};\n    \n    /// Collection statistics\n    record collection-stats {\n        vector-count: u64,\n        dimension: u32,\n        size-bytes: u64,\n        index-size-bytes: option<u64>,\n        namespace-stats: list<tuple<string, namespace-stats>>,\n        distance-distribution: option<distance-stats>,\n    }\n    \n    record namespace-stats {\n        vector-count: u64,\n        size-bytes: u64,\n    }\n    \n    record distance-stats {\n        min-distance: f32,\n        max-distance: f32,\n        avg-distance: f32,\n        percentiles: list<tuple<f32, f32>>,\n    }\n    \n    /// Field statistics for metadata\n    record field-stats {\n        field-name: string,\n        value-count: u64,\n        unique-values: u64,\n        null-count: u64,\n        data-type: string,\n        sample-values: list<metadata-value>,\n    }\n    \n    /// Get collection statistics\n    get-collection-stats: func(\n        collection: string,\n        namespace: option<string>\n    ) -> result<collection-stats, vector-error>;\n    \n    /// Get field statistics\n    get-field-stats: func(\n        collection: string,\n        field: string,\n        namespace: option<string>\n    ) -> result<field-stats, vector-error>;\n    \n    /// Get value distribution for a field\n    get-field-distribution: func(\n        collection: string,\n        field: string,\n        limit: option<u32>,\n        namespace: option<string>\n    ) -> result<list<tuple<metadata-value, u64>>, vector-error>;\n}\n\n/// Namespace/partition management\ninterface namespaces {\n    use types.{vector-error, metadata};\n    \n    /// Namespace information\n    record namespace-info {\n        name: string,\n        collection: string,\n        vector-count: u64,\n        size-bytes: u64,\n        created-at: option<u64>,\n        metadata: option<metadata>,\n    }\n    \n    /// Create or update namespace (upsert)\n    upsert-namespace: func(\n        collection: string,\n        namespace: string,\n        metadata: option<metadata>\n    ) -> result<namespace-info, vector-error>;\n    \n    /// List namespaces in collection\n    list-namespaces: func(collection: string) -> result<list<namespace-info>, vector-error>;\n    \n    /// Get namespace information\n    get-namespace: func(\n        collection: string,\n        namespace: string\n    ) -> result<namespace-info, vector-error>;\n    \n    /// Delete namespace and all vectors within it\n    delete-namespace: func(\n        collection: string,\n        namespace: string\n    ) -> result<_, vector-error>;\n    \n    /// Check if namespace exists\n    namespace-exists: func(\n        collection: string,\n        namespace: string\n    ) -> result<bool, vector-error>;\n}\n\n/// Connection and configuration management\ninterface connection {\n    use types.{vector-error, metadata};\n    \n    variant credentials {\n        api-key(string),\n        username-password(tuple<string, string>),\n        token(string),\n        certificate(list<u8>),\n        oauth(oauth-config),\n    }\n    \n    record oauth-config {\n        client-id: string,\n        client-secret: option<string>,\n        token-url: string,\n        scope: option<string>,\n    }\n    \n    /// Connection status\n    record connection-status {\n        connected: bool,\n        provider: option<string>,\n        endpoint: option<string>,\n        last-activity: option<u64>,\n        connection-id: option<string>,\n    }\n    \n    /// Establish connection to vector database\n    connect: func(\n        endpoint: string,\n        credentials: option<credentials>,\n        timeout-ms: option<u32>,\n        options: option<metadata>\n    ) -> result<_, vector-error>;\n    \n    /// Close connection\n    disconnect: func() -> result<_, vector-error>;\n    \n    /// Get current connection status\n    get-connection-status: func() -> result<connection-status, vector-error>;\n    \n    /// Test connection without modifying state\n    test-connection: func(\n        endpoint: string,\n        credentials: option<credentials>,\n        timeout-ms: option<u32>,\n        options: option<metadata>\n    ) -> result<bool, vector-error>;\n}",
                  "html_url": "https://github.com/golemcloud/golem-ai/issues/21"
                },
                "type": "github"
              },
              "hash": "golemcloud/golem-ai#21",
              "body": "I have attached to this ticket a WIT file that describes a generic interface for vector database operations. This interface can be implemented by various providers, either by emulating features not present in a given provider, utilizing the provider's native support for a feature, or indicating an error if a particular combination is not natively supported by a provider.\n\nThe intent of this WIT specification is to allow developers of WASM components (on wasmCloud, Spin, or Golem) to leverage vector database capabilities to build agents and services in a portable and provider-agnostic fashion.\n\nThis ticket involves constructing implementations of this WIT interface for the following providers:\n\n- **Qdrant**: Offers advanced vector similarity search with support for sparse vectors, recommendations, and discovery operations.\n- **Pinecone**: Provides managed vector database services with hybrid vector support and namespace organization.\n- **Milvus**: Supplies enterprise-grade vector database with comprehensive data types and clustering capabilities.\n- **pgvector**: Extends PostgreSQL with vector operations, binary vectors, and mathematical functions.\n\nThese implementations must be written in Rust and compilable to WASM Components (WASI 0.23 only, since Golem does not yet support WASI 0.3). The standard Rust toolchain for WASM component development can be employed (see cargo component and the Rust examples of components in this and other Golem repositories).\n\nAdditionally, these implementations should incorporate custom durability semantics using the Golem durability API and the Golem host API. This approach ensures that durability is managed at the level of individual vector operations (upsert, search, delete), providing a higher-level and clearer operation log, which aids in debugging and monitoring. See `golem:llm` and `golem:embed` for more details and durable implementations in this same repository.\n\nThe final deliverables associated with this ticket are:\n\n- **Qdrant implementation**: A WASM Component (WASI 0.2), named `vector-qdrant.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n- **Pinecone implementation**: A WASM Component (WASI 0.2), named `vector-pinecone.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n- **Milvus implementation**: A WASM Component (WASI 0.2), named `vector-milvus.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n- **pgvector implementation**: A WASM Component (WASI 0.2), named `vector-pgvector.wasm`, with a full test suite and custom durability implementation at the level of vector operations.\n\nNote: If you have a strong recommendation to swap out one or two of these with other popular / common vector databases, then as long as you get permission beforehand, that's okay with me. However, we definitely need Pinecone, pgvector, and Qdrant. \n\nThese components will require runtime configuration, notably API keys, connection strings, and database credentials. For configuring this information, the components can use environment variables for now (in the future, they will use wasi-runtime-config, but Golem does not support this yet, whereas Golem has good support for environment variables).\n\nMoreover, the Rust components need to be tested within Golem to ensure compatibility with Golem 1.2.x.\n\nThis WIT has been designed by examining and comparing the APIs of Qdrant, Pinecone, Chroma, Weaviate, Milvus, and pgvector. However, given there are no implementations, it is possible the provided WIT is not the optimal abstraction across all these providers. Therefore, deviations from the proposed design can be made. However, to be accepted, any deviation must be fully justified and deemed by Golem core contributors to be an improvement from the original specification.\n\n```wit\npackage golem:vector@1.0.0;\n\n/// Core types and fundamental data structures for vector operations\ninterface types {\n    /// Unique identifier for vectors and collections\n    type id = string;\n    \n    /// Standard dense vector representation\n    type dense-vector = list<f32>;\n    \n    /// Sparse vector with explicit indices\n    record sparse-vector {\n        /// Zero-based indices of non-zero elements\n        indices: list<u32>,\n        /// Values corresponding to the indices\n        values: list<f32>,\n        /// Total dimensionality of the vector space\n        total-dimensions: u32,\n    }\n    \n    /// Binary vector representation\n    record binary-vector {\n        /// Packed binary data\n        data: list<u8>,\n        /// Number of bits/dimensions\n        dimensions: u32,\n    }\n    \n    /// Half-precision vector (16-bit floats)\n    record half-vector {\n        /// Half-precision values (represented as f32 for compatibility)\n        data: list<f32>,\n        /// Number of dimensions\n        dimensions: u32,\n    }\n    \n    /// Vector data supporting multiple representations\n    variant vector-data {\n        /// Standard 32-bit floating point vector\n        dense(dense-vector),\n        /// Sparse vector representation\n        sparse(sparse-vector),\n        /// Binary/bit vector\n        binary(binary-vector),\n        /// Half-precision vector\n        half(half-vector),\n        /// Named vectors for multi-vector collections\n        named(list<tuple<string, dense-vector>>),\n        /// Hybrid dense + sparse combination\n        hybrid(tuple<dense-vector, sparse-vector>),\n    }\n    \n    /// Supported distance metrics\n    enum distance-metric {\n        /// Cosine similarity (1 - cosine distance)\n        cosine,\n        /// Euclidean (L2) distance\n        euclidean,\n        /// Dot product / inner product\n        dot-product,\n        /// Manhattan (L1) distance\n        manhattan,\n        /// Hamming distance (for binary vectors)\n        hamming,\n        /// Jaccard distance (for binary/sparse vectors)\n        jaccard,\n    }\n    \n    /// Metadata value types\n    variant metadata-value {\n        string-val(string),\n        number-val(f64),\n        integer-val(s64),\n        boolean-val(bool),\n        array-val(list<metadata-value>),\n        object-val(list<tuple<string, metadata-value>>),\n        null-val,\n        /// Geographic coordinates\n        geo-val(geo-coordinates),\n        /// ISO 8601 datetime string\n        datetime-val(string),\n        /// Binary data\n        blob-val(list<u8>),\n    }\n    \n    /// Geographic coordinates\n    record geo-coordinates {\n        latitude: f64,\n        longitude: f64,\n    }\n    \n    /// Key-value metadata\n    type metadata = list<tuple<string, metadata-value>>;\n    \n    /// Filter operators for metadata queries\n    enum filter-operator {\n        /// Equal to\n        eq,\n        /// Not equal to\n        ne,\n        /// Greater than\n        gt,\n        /// Greater than or equal\n        gte,\n        /// Less than\n        lt,\n        /// Less than or equal\n        lte,\n        /// Value is in list\n        %in,\n        /// Value is not in list\n        nin,\n        /// Text contains substring (case insensitive)\n        contains,\n        /// Text doesn't contain substring\n        not-contains,\n        /// Regular expression match\n        regex,\n        /// Geographic distance within radius\n        geo-within,\n        /// Geographic bounding box\n        geo-bbox,\n    }\n    \n    /// Basic filter condition\n    record filter-condition {\n        /// Field path (supports nested fields with dot notation)\n        field: string,\n        /// Filter operator\n        operator: filter-operator,\n        /// Value to compare against\n        value: metadata-value,\n    }\n    \n    /// Complex filter expressions with boolean logic\n    variant filter-expression {\n        /// Simple condition\n        condition(filter-condition),\n        /// Logical AND of multiple expressions\n        and(list<filter-expression>),\n        /// Logical OR of multiple expressions\n        or(list<filter-expression>),\n        /// Logical NOT of expression\n        not(filter-expression),\n    }\n    \n    /// Vector record for storage operations\n    record vector-record {\n        /// Unique identifier\n        id: id,\n        /// Vector data\n        vector: vector-data,\n        /// Associated metadata\n        metadata: option<metadata>,\n    }\n    \n    /// Search result with similarity score\n    record search-result {\n        /// Vector identifier\n        id: id,\n        /// Similarity score (higher = more similar)\n        score: f32,\n        /// Distance from query vector (lower = more similar)\n        distance: f32,\n        /// Vector data (if requested)\n        vector: option<vector-data>,\n        /// Associated metadata (if requested)\n        metadata: option<metadata>,\n    }\n    \n    /// Standard error types\n    variant vector-error {\n        /// Resource not found\n        not-found(string),\n        /// Resource already exists\n        already-exists(string),\n        /// Invalid parameters or configuration\n        invalid-params(string),\n        /// Feature not supported by this provider\n        unsupported-feature(string),\n        /// Vector dimension mismatch\n        dimension-mismatch(string),\n        /// Invalid vector format or data\n        invalid-vector(string),\n        /// Authentication/authorization failure\n        unauthorized(string),\n        /// Rate limit exceeded\n        rate-limited(string),\n        /// Internal provider error\n        provider-error(string),\n        /// Network/connection issues\n        connection-error(string),\n    }\n    \n\n}\n\n/// Collection/index management and configuration\ninterface collections {\n    use types.{id, distance-metric, vector-error};\n    \n    /// Index configuration parameters\n    record index-config {\n        index-type: option<string>,\n        parameters: list<tuple<string, string>>,\n    }\n    \n    /// Collection information and statistics\n    record collection-info {\n        name: string,\n        description: option<string>,\n        dimension: u32,\n        metric: distance-metric,\n        vector-count: u64,\n        size-bytes: option<u64>,\n        index-ready: bool,\n        created-at: option<u64>,\n        updated-at: option<u64>,\n        provider-stats: option<types.metadata>,\n    }\n    \n    /// Create or update collection (upsert)\n    upsert-collection: func(\n        name: string,\n        description: option<string>,\n        dimension: u32,\n        metric: distance-metric,\n        index-config: option<index-config>,\n        metadata: option<types.metadata>\n    ) -> result<collection-info, vector-error>;\n    \n    /// List all collections\n    list-collections: func() -> result<list<collection-info>, vector-error>;\n    \n    /// Get collection information\n    get-collection: func(name: string) -> result<collection-info, vector-error>;\n    \n    /// Update collection metadata only\n    update-collection: func(\n        name: string,\n        description: option<string>,\n        metadata: option<types.metadata>\n    ) -> result<collection-info, vector-error>;\n    \n    /// Delete collection and all vectors\n    delete-collection: func(name: string) -> result<_, vector-error>;\n    \n    /// Check if collection exists\n    collection-exists: func(name: string) -> result<bool, vector-error>;\n}\n\n/// Core vector operations (CRUD)\ninterface vectors {\n    use types.{id, vector-record, vector-data, metadata, filter-expression, vector-error};\n    \n    /// Batch operation result\n    record batch-result {\n        success-count: u32,\n        failure-count: u32,\n        errors: list<tuple<u32, vector-error>>,\n    }\n    \n    /// List response with pagination\n    record list-response {\n        vectors: list<vector-record>,\n        next-cursor: option<string>,\n        total-count: option<u64>,\n    }\n    \n    /// Upsert vectors into collection\n    upsert-vectors: func(\n        collection: string,\n        vectors: list<vector-record>,\n        namespace: option<string>\n    ) -> result<batch-result, vector-error>;\n    \n    /// Upsert single vector (convenience)\n    upsert-vector: func(\n        collection: string,\n        id: id,\n        vector: vector-data,\n        metadata: option<metadata>,\n        namespace: option<string>\n    ) -> result<_, vector-error>;\n    \n    /// Get vectors by IDs\n    get-vectors: func(\n        collection: string,\n        ids: list<id>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<vector-record>, vector-error>;\n    \n    /// Get single vector by ID (convenience)\n    get-vector: func(\n        collection: string,\n        id: id,\n        namespace: option<string>\n    ) -> result<option<vector-record>, vector-error>;\n    \n    /// Update vector in place\n    update-vector: func(\n        collection: string,\n        id: id,\n        vector: option<vector-data>,\n        metadata: option<metadata>,\n        namespace: option<string>,\n        merge-metadata: option<bool>\n    ) -> result<_, vector-error>;\n    \n    /// Delete vectors by IDs\n    delete-vectors: func(\n        collection: string,\n        ids: list<id>,\n        namespace: option<string>\n    ) -> result<u32, vector-error>;\n    \n    /// Delete vectors by filter\n    delete-by-filter: func(\n        collection: string,\n        filter: filter-expression,\n        namespace: option<string>\n    ) -> result<u32, vector-error>;\n    \n    /// Delete all vectors in namespace\n    delete-namespace: func(\n        collection: string,\n        namespace: string\n    ) -> result<u32, vector-error>;\n    \n    /// List vectors with filtering and pagination\n    list-vectors: func(\n        collection: string,\n        namespace: option<string>,\n        filter: option<filter-expression>,\n        limit: option<u32>,\n        cursor: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list-response, vector-error>;\n    \n    /// Count vectors matching filter\n    count-vectors: func(\n        collection: string,\n        filter: option<filter-expression>,\n        namespace: option<string>\n    ) -> result<u64, vector-error>;\n}\n\n/// Core similarity search operations\ninterface search {\n    use types.{id, vector-data, search-result, filter-expression, vector-error};\n    \n    /// Search query variants\n    variant search-query {\n        vector(vector-data),\n        by-id(id),\n        multi-vector(list<tuple<string, vector-data>>),\n    }\n    \n    /// Similarity search\n    search-vectors: func(\n        collection: string,\n        query: search-query,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>,\n        min-score: option<f32>,\n        max-distance: option<f32>,\n        search-params: option<list<tuple<string, string>>>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Simple vector similarity search (convenience)\n    find-similar: func(\n        collection: string,\n        vector: vector-data,\n        limit: u32,\n        namespace: option<string>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Batch similarity search\n    batch-search: func(\n        collection: string,\n        queries: list<search-query>,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>,\n        search-params: option<list<tuple<string, string>>>\n    ) -> result<list<list<search-result>>, vector-error>;\n}\n}\n\n/// Extended search capabilities (provider-dependent)\ninterface search-extended {\n    use types.{id, vector-data, search-result, filter-expression, vector-error, metadata-value};\n    \n    /// Recommendation example types\n    variant recommendation-example {\n        vector-id(id),\n        vector-data(vector-data),\n    }\n    \n    enum recommendation-strategy {\n        average-vector,\n        best-score,\n        centroid,\n    }\n    \n    /// Context pair for discovery\n    record context-pair {\n        positive: recommendation-example,\n        negative: recommendation-example,\n    }\n    \n    /// Grouped search result\n    record grouped-search-result {\n        group-value: metadata-value,\n        results: list<search-result>,\n        group-count: u32,\n    }\n    \n    /// Recommendation-based search\n    recommend-vectors: func(\n        collection: string,\n        positive: list<recommendation-example>,\n        negative: option<list<recommendation-example>>,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        strategy: option<recommendation-strategy>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Discovery/context-based search\n    discover-vectors: func(\n        collection: string,\n        context-pairs: list<context-pair>,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Grouped search for diverse results\n    search-groups: func(\n        collection: string,\n        query: search.search-query,\n        group-by: string,\n        group-size: u32,\n        max-groups: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<grouped-search-result>, vector-error>;\n    \n    /// Range search within distance bounds\n    search-range: func(\n        collection: string,\n        vector: vector-data,\n        min-distance: option<f32>,\n        max-distance: f32,\n        filter: option<filter-expression>,\n        namespace: option<string>,\n        limit: option<u32>,\n        include-vectors: option<bool>,\n        include-metadata: option<bool>\n    ) -> result<list<search-result>, vector-error>;\n    \n    /// Text/document search (auto-embedding)\n    search-text: func(\n        collection: string,\n        query-text: string,\n        limit: u32,\n        filter: option<filter-expression>,\n        namespace: option<string>\n    ) -> result<list<search-result>, vector-error>;\n}\n\n/// Analytics and statistics\ninterface analytics {\n    use types.{vector-error, metadata-value, filter-expression};\n    \n    /// Collection statistics\n    record collection-stats {\n        vector-count: u64,\n        dimension: u32,\n        size-bytes: u64,\n        index-size-bytes: option<u64>,\n        namespace-stats: list<tuple<string, namespace-stats>>,\n        distance-distribution: option<distance-stats>,\n    }\n    \n    record namespace-stats {\n        vector-count: u64,\n        size-bytes: u64,\n    }\n    \n    record distance-stats {\n        min-distance: f32,\n        max-distance: f32,\n        avg-distance: f32,\n        percentiles: list<tuple<f32, f32>>,\n    }\n    \n    /// Field statistics for metadata\n    record field-stats {\n        field-name: string,\n        value-count: u64,\n        unique-values: u64,\n        null-count: u64,\n        data-type: string,\n        sample-values: list<metadata-value>,\n    }\n    \n    /// Get collection statistics\n    get-collection-stats: func(\n        collection: string,\n        namespace: option<string>\n    ) -> result<collection-stats, vector-error>;\n    \n    /// Get field statistics\n    get-field-stats: func(\n        collection: string,\n        field: string,\n        namespace: option<string>\n    ) -> result<field-stats, vector-error>;\n    \n    /// Get value distribution for a field\n    get-field-distribution: func(\n        collection: string,\n        field: string,\n        limit: option<u32>,\n        namespace: option<string>\n    ) -> result<list<tuple<metadata-value, u64>>, vector-error>;\n}\n\n/// Namespace/partition management\ninterface namespaces {\n    use types.{vector-error, metadata};\n    \n    /// Namespace information\n    record namespace-info {\n        name: string,\n        collection: string,\n        vector-count: u64,\n        size-bytes: u64,\n        created-at: option<u64>,\n        metadata: option<metadata>,\n    }\n    \n    /// Create or update namespace (upsert)\n    upsert-namespace: func(\n        collection: string,\n        namespace: string,\n        metadata: option<metadata>\n    ) -> result<namespace-info, vector-error>;\n    \n    /// List namespaces in collection\n    list-namespaces: func(collection: string) -> result<list<namespace-info>, vector-error>;\n    \n    /// Get namespace information\n    get-namespace: func(\n        collection: string,\n        namespace: string\n    ) -> result<namespace-info, vector-error>;\n    \n    /// Delete namespace and all vectors within it\n    delete-namespace: func(\n        collection: string,\n        namespace: string\n    ) -> result<_, vector-error>;\n    \n    /// Check if namespace exists\n    namespace-exists: func(\n        collection: string,\n        namespace: string\n    ) -> result<bool, vector-error>;\n}\n\n/// Connection and configuration management\ninterface connection {\n    use types.{vector-error, metadata};\n    \n    variant credentials {\n        api-key(string),\n        username-password(tuple<string, string>),\n        token(string),\n        certificate(list<u8>),\n        oauth(oauth-config),\n    }\n    \n    record oauth-config {\n        client-id: string,\n        client-secret: option<string>,\n        token-url: string,\n        scope: option<string>,\n    }\n    \n    /// Connection status\n    record connection-status {\n        connected: bool,\n        provider: option<string>,\n        endpoint: option<string>,\n        last-activity: option<u64>,\n        connection-id: option<string>,\n    }\n    \n    /// Establish connection to vector database\n    connect: func(\n        endpoint: string,\n        credentials: option<credentials>,\n        timeout-ms: option<u32>,\n        options: option<metadata>\n    ) -> result<_, vector-error>;\n    \n    /// Close connection\n    disconnect: func() -> result<_, vector-error>;\n    \n    /// Get current connection status\n    get-connection-status: func() -> result<connection-status, vector-error>;\n    \n    /// Test connection without modifying state\n    test-connection: func(\n        endpoint: string,\n        credentials: option<credentials>,\n        timeout-ms: option<u32>,\n        options: option<metadata>\n    ) -> result<bool, vector-error>;\n}",
              "url": "https://github.com/golemcloud/golem-ai/issues/21",
              "tech": [
                "go"
              ],
              "repo_name": "golem-ai",
              "repo_owner": "golemcloud",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#315",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:45.811Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:45.811Z",
            "created_at": "2025-11-09T14:40:45.811Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#315",
              "status": "open",
              "type": "issue",
              "number": 315,
              "title": "[bounty] test and showcase the new gemini computer use model with terminator MCP",
              "source": {
                "data": {
                  "id": "source-mediar-ai#315",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] test and showcase the new gemini computer use model with terminator MCP",
                  "body": "basic idea: try the new gemini CU model:\n\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/computer-use\n\nand try to build some workflows like reading PDFs or spreadsheets on file system and doing data entry in another software \n\n\ngoal: evaluate if their model adds worthwhile performance gain by using both vision and accessibility to build workflow compared to claude etc\n\n\nrequirements:\n- record yourself building the workflow \n- use the new model somehow (gemini CLI + terminator MCP + gemini model configured for example, or build a custom MCP client in python/js, high preference for JS)\n- use terminator MCP server\n\n\nbonus:\n- professional video level that can be published on social\n\n\nplease share your plan before building things, one-prompt PRs will be banned \n\n/bounty 200\n",
                  "html_url": "https://github.com/mediar-ai/terminator/issues/315"
                },
                "type": "github"
              },
              "hash": "mediar-ai/terminator#315",
              "body": "basic idea: try the new gemini CU model:\n\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/computer-use\n\nand try to build some workflows like reading PDFs or spreadsheets on file system and doing data entry in another software \n\n\ngoal: evaluate if their model adds worthwhile performance gain by using both vision and accessibility to build workflow compared to claude etc\n\n\nrequirements:\n- record yourself building the workflow \n- use the new model somehow (gemini CLI + terminator MCP + gemini model configured for example, or build a custom MCP client in python/js, high preference for JS)\n- use terminator MCP server\n\n\nbonus:\n- professional video level that can be published on social\n\n\nplease share your plan before building things, one-prompt PRs will be banned \n\n/bounty 200\n",
              "url": "https://github.com/mediar-ai/terminator/issues/315",
              "tech": [
                "go"
              ],
              "repo_name": "terminator",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1626",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:46.020Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.020Z",
            "created_at": "2025-11-09T14:40:46.020Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1626",
              "status": "open",
              "type": "issue",
              "number": 1626,
              "title": "[bounty] $400 fix audio device randomly stopping sometimes",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1626",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] $400 fix audio device randomly stopping sometimes",
                  "body": "\ndisplay audio / microphone randomly stops on mac sometimes \n\nhad it running for 48h now and display audio somehow stopped a bit later after 50h\n\n/bounty 400 \n\n@EzraEllette any idea?\n\n\ni suggest finding way to reproduce and then some automated test and a fix ",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1626"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1626",
              "body": "\ndisplay audio / microphone randomly stops on mac sometimes \n\nhad it running for 48h now and display audio somehow stopped a bit later after 50h\n\n/bounty 400 \n\n@EzraEllette any idea?\n\n\ni suggest finding way to reproduce and then some automated test and a fix ",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1626",
              "tech": [
                "go"
              ],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1560",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:46.298Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.298Z",
            "created_at": "2025-11-09T14:40:46.298Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1560",
              "status": "open",
              "type": "issue",
              "number": 1560,
              "title": "[feature]  Implement Session Tracking for Application and Window Usage",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1560",
                  "user": {
                    "login": "rodgomesc",
                    "id": 4893591,
                    "node_id": "MDQ6VXNlcjQ4OTM1OTE=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/4893591?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/rodgomesc",
                    "html_url": "https://github.com/rodgomesc",
                    "followers_url": "https://api.github.com/users/rodgomesc/followers",
                    "following_url": "https://api.github.com/users/rodgomesc/following{/other_user}",
                    "gists_url": "https://api.github.com/users/rodgomesc/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/rodgomesc/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/rodgomesc/subscriptions",
                    "organizations_url": "https://api.github.com/users/rodgomesc/orgs",
                    "repos_url": "https://api.github.com/users/rodgomesc/repos",
                    "events_url": "https://api.github.com/users/rodgomesc/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/rodgomesc/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[feature]  Implement Session Tracking for Application and Window Usage",
                  "body": "\n## Challenge\n\nI'm working on an application that requires accurate session tracking to analyze user behavior patterns,  and provide meaningful insights across different applications and windows. \n\nIn the existing Screenpipe architecture, OCR data is stored with only timestamp information, lacking precise session boundaries (start/end times) for application usage tracking. This limitation prevents us from properly segmenting user activity into coherent sessions.\n\n## 1. Current Workaround\ninternally I implementing a suboptimal solution that:\n1. Queries all OCR database entries chronologically\n2. Manually identifies session boundaries by analyzing timestamp sequences for each application\n3. Determines session end when detecting a different application in subsequent timestamps\n4. Assumes a session is ongoing if no clear delimiter exists in future timestamps\n\nWith that said I would love to hear any feedbacks that could help solve this problem with a minimal overhead on code changes, and make sure I'm not overthinking the solution: cc @louis030195 , i'm planning to start playing with some day next week\n\n## 2. Requirements\n\n1.  **Session Definition:** A \"session\" should be defined as a continuous period of user interaction with a specific application and window.  A break in activity (e.g., switching to a different application or window) should end the current session and potentially start a new one.  A configurable inactivity timeout (e.g., 5 minutes) should also end a session.\n\n2.  **Data Storage:** The database schema must be modified to store session start and end times, associated with specific applications and windows.  It must maintain links to the relevant OCR and audio data chunks.\n\n3.  **API Endpoint:**  A new or modified API endpoint (e.g., `/sessions`) is needed to query for session data.  This endpoint should support filtering by:\n    *   Date range (start and end timestamps).\n    *   Application name.\n    *   Window name.\n    *   Minimum and maximum session duration.\n    *   Associated tags.\n    *   Pagination (limit and offset).\n\n4.  **Real-time Capability:** The system should be able to detect session boundaries in near real-time as data is being ingested. This implies integration with the existing data capture and processing pipeline.\n\n5. **Configurability:** The inactivity timeout for ending a session should be configurable.\n\n## 3. Proposed Solution\n\n### 3.1. Database Schema Changes\n\nWe'll introduce a new table `sessions` and modify the existing tables.\n\n**New Table: `sessions`**\n\n| Column Name      | Data Type  | Constraints                                  | Description                                                                  |\n| :--------------- | :--------- | :------------------------------------------- | :--------------------------------------------------------------------------- |\n| `id`             | INTEGER    | PRIMARY KEY AUTOINCREMENT                    | Unique identifier for the session.                                       |\n| `app_name`       | TEXT       | NOT NULL                                     | Name of the application.                                                 |\n| `window_name`    | TEXT       |                                               | Title of the window.                                                     |\n| `start_time`     | DATETIME   | NOT NULL                                     | Timestamp of the session start.                                            |\n| `end_time`       | DATETIME   | NOT NULL                                     | Timestamp of the session end.                                              |\n| `duration_secs`  | REAL       | NOT NULL                                     | Duration of the session in seconds.                                        |\n\n**Modified Tables:**\n\n*   **`ocr_text`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`audio_transcriptions`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`ui_monitoring`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n\n**Indexes:**\n\n*   Create indexes on `sessions` table columns (`app_name`, `window_name`, `start_time`, `end_time`) for efficient querying.\n*   Create indexes on the `session_id` columns in `ocr_text`, `audio_transcriptions`, and `ui_monitoring`.\n\n### 3.2. API Enhancements\n\nWe'll modify the existing `/search` endpoint to also support session queries.\n\n**Request:**\n\n```\nGET /search?content_type=session&app_name=Cursor&start_time=2024-11-27T00:00:00Z&end_time=2024-11-28T00:00:00Z&min_duration=60&limit=10&offset=0\n```\n\n*   `content_type=session`: Indicates a session-based search.\n*   `app_name`, `window_name`: Filter by application and window (optional).\n*   `start_time`, `end_time`: Date range for the session (optional, defaults to last 24 hours).\n*   `min_duration`, `max_duration`: Filter by session duration in seconds (optional).\n*   `limit`, `offset`: Pagination parameters.\n\n\nafter discussing with @louis030195 on discord he proposed something like this as the output\n\n**Response:**\n\n```json\n{\n  \"data\": [\n    {\n      \"app_name\": \"Cursor\",\n      \"total_usage_time\": 3600,\n      \"windows\": [\n        {\n          \"window_name\": \"page.tsx\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T12:00:00Z\",\n              \"end_time\": \"2024-07-16T12:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [123, 124, 125],\n              \"tags\": [\"coding\", \"frontend\"]\n            },\n            {\n              \"start_time\": \"2024-07-16T14:00:00Z\",\n              \"end_time\": \"2024-07-16T14:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [223, 224, 225],\n              \"tags\": [\"coding\", \"frontend\"]\n            }\n          ],\n          \"total_usage_time\": 3600\n        }\n      ]\n    },\n    {\n      \"app_name\": \"Arc\",\n      \"total_usage_time\": 1200,\n      \"windows\": [\n        {\n          \"window_name\": \"x.com\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T10:30:00Z\",\n              \"end_time\": \"2024-07-16T10:50:00Z\",\n              \"duration\": 1200,\n              \"content_ids\": [126, 127, 128],\n              \"tags\": [\"browsing\", \"social-media\"]\n            }\n          ],\n          \"total_usage_time\": 1200\n        }\n      ]\n    }\n  ],\n  \"pagination\": {\n    \"limit\": 50,\n    \"offset\": 0,\n    \"total\": 2\n  }\n}\n```\n\n### 3.3. Session Boundary Detection Logic\n\n1.  **Real-time Monitoring:**  The existing event listeners for OCR, audio, and UI events will be leveraged.\n\n2.  **Session Tracking:**  Maintain a data structure (likely in memory, possibly augmented with periodic database writes for persistence) to track active sessions.  This structure should map:\n    *   `(app_name, window_name)` -> `(session_id, last_activity_timestamp)`\n\n3.  **New Event Handling:**\n    *   When a new event (OCR, audio, UI) arrives:\n        *   Extract `app_name` and `window_name`.\n        *   Check if an active session exists for this (app, window) combination.\n        *   If a session exists:\n            *   Check if `now() - last_activity_timestamp` exceeds the inactivity timeout.\n            *   If timeout exceeded, end the existing session and create a new one.\n            *   If not timed out, update `last_activity_timestamp` to the event's timestamp.\n        *   If no session exists:\n            *   Create a new session with `start_time` and `end_time` set to the event's timestamp.\n            *   Generate a new `session_id`.\n            *   Insert a new row into the `sessions` table.\n        *   Insert the new data (OCR, audio, UI) into the relevant table, linking it to the active `session_id`.\n\n4.  **Inactivity Timeout:** A configurable timeout (e.g., 5 minutes) will be used to determine session end due to inactivity.\n\n5.  **Periodic Flushing:** To avoid excessive in-memory state, periodically flush closed sessions (those that have exceeded the inactivity timeout) to the database.\n\n6. **Data Migration:** A migration script will be needed to analyze existing data and create initial session records based on timestamp proximity and app/window changes.\n\n### 3.4. Implementation Details\n\n*   **Language/Framework:** Continue using Rust for core logic and database interaction.\n*   **Database:** Continue using SQLite.\n*   **Concurrency:** Use asynchronous operations with `tokio` for handling concurrent events and database interactions.  Leverage `Arc` and `Mutex` for shared state, and channels for communication between tasks.\n*   **Configuration:** Add a new setting for the inactivity timeout.\n*   **Error Handling:** Implement robust error handling with `anyhow` and propagate errors appropriately. Log errors using `tracing`.\n* **Testing:** Thoroughly test session boundary detection with various scenarios, including rapid switching, long inactivity periods, and concurrent events.\n\n## 4. Diagrams\n\n### 4.1. Current Data Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Database\n    User->>Screen: Interacts with screen\n    Screen->>Screenpipe: Captures screen content (OCR)\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Database: Stores OCR data with timestamps\n    Screenpipe->>Database: Stores audio transcriptions with timestamps\n    User->>Screenpipe: Requests data\n    Screenpipe->>Database: Queries data by time range\n    Database->>Screenpipe: Returns data\n    Screenpipe->>User: Returns data\n```\n\n### 4.2. Proposed Database Schema Changes\n\n```mermaid\nclassDiagram\n    class frames {\n        +id : INTEGER [PK]\n        +video_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +name : TEXT\n    }\n    class video_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +device_name : TEXT\n    }\n    class ocr_text {\n        +frame_id : INTEGER [FK]\n        +text : TEXT\n        +text_json : TEXT\n        +app_name : TEXT\n        +window_name : TEXT\n        +ocr_engine : TEXT\n        +focused : BOOLEAN\n        +session_id : INTEGER [FK]\n    }\n    class audio_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +timestamp : TIMESTAMP\n    }\n    class audio_transcriptions {\n        +id : INTEGER [PK]\n        +audio_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +transcription : TEXT\n        +device : TEXT\n        +is_input_device : BOOLEAN\n        +speaker_id : INTEGER\n        +transcription_engine : TEXT\n        +start_time : REAL\n        +end_time : REAL\n        +text_length : INTEGER\n        +session_id : INTEGER [FK]\n    }\n    class ui_monitoring {\n      +id : INTEGER [PK]\n      +text_output : TEXT\n      +timestamp : DATETIME\n      +app : TEXT\n      +window : TEXT\n      +initial_traversal_at : DATETIME\n      +session_id : INTEGER [FK]\n    }\n  class sessions {\n    +id : INTEGER [PK]\n    +app_name : TEXT\n    +window_name : TEXT\n    +start_time : DATETIME\n    +end_time : DATETIME\n    +duration_secs : REAL\n  }\n  frames --|> video_chunks : video_chunk_id\n  ocr_text --|> frames : frame_id\n  audio_transcriptions --|> audio_chunks : audio_chunk_id\n  ocr_text --|> sessions : session_id\n  audio_transcriptions --|> sessions : session_id\n  ui_monitoring --|> sessions : session_id\n```\n\n### 4.3. Modified Data Flow with Session Tracking\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Session Manager\n    participant Database\n\n    User->>Screen: Interacts with App A, Window 1\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to session_id\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: Active session found (session_id)\n    Screenpipe->>Database: Store audio data, link to session_id\n\n    Note over User,Screenpipe: User switches to App B, Window 2\n    User->>Screen: Interacts with App B, Window 2\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App B, Window 2)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: End previous session (App A, Window 1)\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to new session_id\n\n    Note over User,Screenpipe: Inactivity Timeout\n    Screenpipe->>Session Manager: Check for activity\n    Session Manager->>Database: End current session (App B, Window 2)\n```\n\n### 4.4. New API Endpoint (Modified /search)\n\nThe existing `/search` endpoint will be extended to support session-based queries.  No new endpoint is needed.\n\n## 5. Performance Implications\n\n*   **Database:**  The addition of the `sessions` table and foreign keys will add some overhead to write operations.  Proper indexing is crucial to mitigate this.  Read performance for session-based queries should be good with appropriate indexes.\n*   **Memory:**  The in-memory session tracking will require additional memory.  The size of this will depend on the number of concurrent users and the configured timeout.  This should be monitored and potentially optimized (e.g., using a more efficient data structure than a simple `HashMap`).\n*   **CPU:**  The session boundary detection logic will add some CPU overhead.  This should be minimized by efficient checks and the use of asynchronous operations.\n\n## 6. Migration Path\n\n1.  **Add new `sessions` table.**\n2.  **Add `session_id` columns** to `ocr_text`, `audio_transcriptions`, and `ui_monitoring` tables.\n3.  **Backfill `session_id`:**  A migration script will be needed to analyze existing data and create initial session records.  This script will need to:\n    *   Iterate through existing OCR, audio, and UI data in chronological order.\n    *   Apply the session boundary detection logic (described above) to group data into sessions.\n    *   Insert corresponding rows into the `sessions` table.\n    *   Update the `session_id` foreign key in the existing tables.\n\n\n## 7. Alternative Approaches\n\n1.  **Session Table per Content Type:** Instead of a single `sessions` table, we could have separate session tables for each content type (e.g., `ocr_sessions`, `audio_sessions`, `ui_sessions`).  This might simplify queries for a specific content type but could make cross-content type analysis more complex.\n2.  **No Session Table (Denormalized):** We could add `start_time` and `end_time` directly to the existing tables (`ocr_text`, `audio_transcriptions`, `ui_monitoring`). This would avoid the need for joins when querying for sessions, but could lead to data redundancy and potential inconsistencies.\n3.  any other suggestions ????\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1560"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1560",
              "body": "\n## Challenge\n\nI'm working on an application that requires accurate session tracking to analyze user behavior patterns,  and provide meaningful insights across different applications and windows. \n\nIn the existing Screenpipe architecture, OCR data is stored with only timestamp information, lacking precise session boundaries (start/end times) for application usage tracking. This limitation prevents us from properly segmenting user activity into coherent sessions.\n\n## 1. Current Workaround\ninternally I implementing a suboptimal solution that:\n1. Queries all OCR database entries chronologically\n2. Manually identifies session boundaries by analyzing timestamp sequences for each application\n3. Determines session end when detecting a different application in subsequent timestamps\n4. Assumes a session is ongoing if no clear delimiter exists in future timestamps\n\nWith that said I would love to hear any feedbacks that could help solve this problem with a minimal overhead on code changes, and make sure I'm not overthinking the solution: cc @louis030195 , i'm planning to start playing with some day next week\n\n## 2. Requirements\n\n1.  **Session Definition:** A \"session\" should be defined as a continuous period of user interaction with a specific application and window.  A break in activity (e.g., switching to a different application or window) should end the current session and potentially start a new one.  A configurable inactivity timeout (e.g., 5 minutes) should also end a session.\n\n2.  **Data Storage:** The database schema must be modified to store session start and end times, associated with specific applications and windows.  It must maintain links to the relevant OCR and audio data chunks.\n\n3.  **API Endpoint:**  A new or modified API endpoint (e.g., `/sessions`) is needed to query for session data.  This endpoint should support filtering by:\n    *   Date range (start and end timestamps).\n    *   Application name.\n    *   Window name.\n    *   Minimum and maximum session duration.\n    *   Associated tags.\n    *   Pagination (limit and offset).\n\n4.  **Real-time Capability:** The system should be able to detect session boundaries in near real-time as data is being ingested. This implies integration with the existing data capture and processing pipeline.\n\n5. **Configurability:** The inactivity timeout for ending a session should be configurable.\n\n## 3. Proposed Solution\n\n### 3.1. Database Schema Changes\n\nWe'll introduce a new table `sessions` and modify the existing tables.\n\n**New Table: `sessions`**\n\n| Column Name      | Data Type  | Constraints                                  | Description                                                                  |\n| :--------------- | :--------- | :------------------------------------------- | :--------------------------------------------------------------------------- |\n| `id`             | INTEGER    | PRIMARY KEY AUTOINCREMENT                    | Unique identifier for the session.                                       |\n| `app_name`       | TEXT       | NOT NULL                                     | Name of the application.                                                 |\n| `window_name`    | TEXT       |                                               | Title of the window.                                                     |\n| `start_time`     | DATETIME   | NOT NULL                                     | Timestamp of the session start.                                            |\n| `end_time`       | DATETIME   | NOT NULL                                     | Timestamp of the session end.                                              |\n| `duration_secs`  | REAL       | NOT NULL                                     | Duration of the session in seconds.                                        |\n\n**Modified Tables:**\n\n*   **`ocr_text`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`audio_transcriptions`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n*   **`ui_monitoring`**: Add a `session_id` column (INTEGER, FOREIGN KEY referencing `sessions.id`).\n\n**Indexes:**\n\n*   Create indexes on `sessions` table columns (`app_name`, `window_name`, `start_time`, `end_time`) for efficient querying.\n*   Create indexes on the `session_id` columns in `ocr_text`, `audio_transcriptions`, and `ui_monitoring`.\n\n### 3.2. API Enhancements\n\nWe'll modify the existing `/search` endpoint to also support session queries.\n\n**Request:**\n\n```\nGET /search?content_type=session&app_name=Cursor&start_time=2024-11-27T00:00:00Z&end_time=2024-11-28T00:00:00Z&min_duration=60&limit=10&offset=0\n```\n\n*   `content_type=session`: Indicates a session-based search.\n*   `app_name`, `window_name`: Filter by application and window (optional).\n*   `start_time`, `end_time`: Date range for the session (optional, defaults to last 24 hours).\n*   `min_duration`, `max_duration`: Filter by session duration in seconds (optional).\n*   `limit`, `offset`: Pagination parameters.\n\n\nafter discussing with @louis030195 on discord he proposed something like this as the output\n\n**Response:**\n\n```json\n{\n  \"data\": [\n    {\n      \"app_name\": \"Cursor\",\n      \"total_usage_time\": 3600,\n      \"windows\": [\n        {\n          \"window_name\": \"page.tsx\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T12:00:00Z\",\n              \"end_time\": \"2024-07-16T12:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [123, 124, 125],\n              \"tags\": [\"coding\", \"frontend\"]\n            },\n            {\n              \"start_time\": \"2024-07-16T14:00:00Z\",\n              \"end_time\": \"2024-07-16T14:30:00Z\",\n              \"duration\": 1800,\n              \"content_ids\": [223, 224, 225],\n              \"tags\": [\"coding\", \"frontend\"]\n            }\n          ],\n          \"total_usage_time\": 3600\n        }\n      ]\n    },\n    {\n      \"app_name\": \"Arc\",\n      \"total_usage_time\": 1200,\n      \"windows\": [\n        {\n          \"window_name\": \"x.com\",\n          \"sessions\": [\n            {\n              \"start_time\": \"2024-07-16T10:30:00Z\",\n              \"end_time\": \"2024-07-16T10:50:00Z\",\n              \"duration\": 1200,\n              \"content_ids\": [126, 127, 128],\n              \"tags\": [\"browsing\", \"social-media\"]\n            }\n          ],\n          \"total_usage_time\": 1200\n        }\n      ]\n    }\n  ],\n  \"pagination\": {\n    \"limit\": 50,\n    \"offset\": 0,\n    \"total\": 2\n  }\n}\n```\n\n### 3.3. Session Boundary Detection Logic\n\n1.  **Real-time Monitoring:**  The existing event listeners for OCR, audio, and UI events will be leveraged.\n\n2.  **Session Tracking:**  Maintain a data structure (likely in memory, possibly augmented with periodic database writes for persistence) to track active sessions.  This structure should map:\n    *   `(app_name, window_name)` -> `(session_id, last_activity_timestamp)`\n\n3.  **New Event Handling:**\n    *   When a new event (OCR, audio, UI) arrives:\n        *   Extract `app_name` and `window_name`.\n        *   Check if an active session exists for this (app, window) combination.\n        *   If a session exists:\n            *   Check if `now() - last_activity_timestamp` exceeds the inactivity timeout.\n            *   If timeout exceeded, end the existing session and create a new one.\n            *   If not timed out, update `last_activity_timestamp` to the event's timestamp.\n        *   If no session exists:\n            *   Create a new session with `start_time` and `end_time` set to the event's timestamp.\n            *   Generate a new `session_id`.\n            *   Insert a new row into the `sessions` table.\n        *   Insert the new data (OCR, audio, UI) into the relevant table, linking it to the active `session_id`.\n\n4.  **Inactivity Timeout:** A configurable timeout (e.g., 5 minutes) will be used to determine session end due to inactivity.\n\n5.  **Periodic Flushing:** To avoid excessive in-memory state, periodically flush closed sessions (those that have exceeded the inactivity timeout) to the database.\n\n6. **Data Migration:** A migration script will be needed to analyze existing data and create initial session records based on timestamp proximity and app/window changes.\n\n### 3.4. Implementation Details\n\n*   **Language/Framework:** Continue using Rust for core logic and database interaction.\n*   **Database:** Continue using SQLite.\n*   **Concurrency:** Use asynchronous operations with `tokio` for handling concurrent events and database interactions.  Leverage `Arc` and `Mutex` for shared state, and channels for communication between tasks.\n*   **Configuration:** Add a new setting for the inactivity timeout.\n*   **Error Handling:** Implement robust error handling with `anyhow` and propagate errors appropriately. Log errors using `tracing`.\n* **Testing:** Thoroughly test session boundary detection with various scenarios, including rapid switching, long inactivity periods, and concurrent events.\n\n## 4. Diagrams\n\n### 4.1. Current Data Flow\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Database\n    User->>Screen: Interacts with screen\n    Screen->>Screenpipe: Captures screen content (OCR)\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Database: Stores OCR data with timestamps\n    Screenpipe->>Database: Stores audio transcriptions with timestamps\n    User->>Screenpipe: Requests data\n    Screenpipe->>Database: Queries data by time range\n    Database->>Screenpipe: Returns data\n    Screenpipe->>User: Returns data\n```\n\n### 4.2. Proposed Database Schema Changes\n\n```mermaid\nclassDiagram\n    class frames {\n        +id : INTEGER [PK]\n        +video_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +name : TEXT\n    }\n    class video_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +device_name : TEXT\n    }\n    class ocr_text {\n        +frame_id : INTEGER [FK]\n        +text : TEXT\n        +text_json : TEXT\n        +app_name : TEXT\n        +window_name : TEXT\n        +ocr_engine : TEXT\n        +focused : BOOLEAN\n        +session_id : INTEGER [FK]\n    }\n    class audio_chunks {\n        +id : INTEGER [PK]\n        +file_path : TEXT\n        +timestamp : TIMESTAMP\n    }\n    class audio_transcriptions {\n        +id : INTEGER [PK]\n        +audio_chunk_id : INTEGER [FK]\n        +offset_index : INTEGER\n        +timestamp : DATETIME\n        +transcription : TEXT\n        +device : TEXT\n        +is_input_device : BOOLEAN\n        +speaker_id : INTEGER\n        +transcription_engine : TEXT\n        +start_time : REAL\n        +end_time : REAL\n        +text_length : INTEGER\n        +session_id : INTEGER [FK]\n    }\n    class ui_monitoring {\n      +id : INTEGER [PK]\n      +text_output : TEXT\n      +timestamp : DATETIME\n      +app : TEXT\n      +window : TEXT\n      +initial_traversal_at : DATETIME\n      +session_id : INTEGER [FK]\n    }\n  class sessions {\n    +id : INTEGER [PK]\n    +app_name : TEXT\n    +window_name : TEXT\n    +start_time : DATETIME\n    +end_time : DATETIME\n    +duration_secs : REAL\n  }\n  frames --|> video_chunks : video_chunk_id\n  ocr_text --|> frames : frame_id\n  audio_transcriptions --|> audio_chunks : audio_chunk_id\n  ocr_text --|> sessions : session_id\n  audio_transcriptions --|> sessions : session_id\n  ui_monitoring --|> sessions : session_id\n```\n\n### 4.3. Modified Data Flow with Session Tracking\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Screen\n    participant Microphone\n    participant Screenpipe\n    participant Session Manager\n    participant Database\n\n    User->>Screen: Interacts with App A, Window 1\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to session_id\n    User->>Microphone: Speaks\n    Microphone->>Screenpipe: Captures audio\n    Screenpipe->>Session Manager: Check for existing session (App A, Window 1)\n    Session Manager-->>Screenpipe: Active session found (session_id)\n    Screenpipe->>Database: Store audio data, link to session_id\n\n    Note over User,Screenpipe: User switches to App B, Window 2\n    User->>Screen: Interacts with App B, Window 2\n    Screen->>Screenpipe: Captures screen content (OCR)\n    Screenpipe->>Session Manager: Check for existing session (App B, Window 2)\n    Session Manager-->>Screenpipe: No active session\n    Session Manager->>Database: End previous session (App A, Window 1)\n    Session Manager->>Database: Create new session, get session_id\n    Screenpipe->>Database: Store OCR data, link to new session_id\n\n    Note over User,Screenpipe: Inactivity Timeout\n    Screenpipe->>Session Manager: Check for activity\n    Session Manager->>Database: End current session (App B, Window 2)\n```\n\n### 4.4. New API Endpoint (Modified /search)\n\nThe existing `/search` endpoint will be extended to support session-based queries.  No new endpoint is needed.\n\n## 5. Performance Implications\n\n*   **Database:**  The addition of the `sessions` table and foreign keys will add some overhead to write operations.  Proper indexing is crucial to mitigate this.  Read performance for session-based queries should be good with appropriate indexes.\n*   **Memory:**  The in-memory session tracking will require additional memory.  The size of this will depend on the number of concurrent users and the configured timeout.  This should be monitored and potentially optimized (e.g., using a more efficient data structure than a simple `HashMap`).\n*   **CPU:**  The session boundary detection logic will add some CPU overhead.  This should be minimized by efficient checks and the use of asynchronous operations.\n\n## 6. Migration Path\n\n1.  **Add new `sessions` table.**\n2.  **Add `session_id` columns** to `ocr_text`, `audio_transcriptions`, and `ui_monitoring` tables.\n3.  **Backfill `session_id`:**  A migration script will be needed to analyze existing data and create initial session records.  This script will need to:\n    *   Iterate through existing OCR, audio, and UI data in chronological order.\n    *   Apply the session boundary detection logic (described above) to group data into sessions.\n    *   Insert corresponding rows into the `sessions` table.\n    *   Update the `session_id` foreign key in the existing tables.\n\n\n## 7. Alternative Approaches\n\n1.  **Session Table per Content Type:** Instead of a single `sessions` table, we could have separate session tables for each content type (e.g., `ocr_sessions`, `audio_sessions`, `ui_sessions`).  This might simplify queries for a specific content type but could make cross-content type analysis more complex.\n2.  **No Session Table (Denormalized):** We could add `start_time` and `end_time` directly to the existing tables (`ocr_text`, `audio_transcriptions`, `ui_monitoring`). This would avoid the need for joins when querying for sessions, but could lead to data redundancy and potential inconsistencies.\n3.  any other suggestions ????\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1560",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1441",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:46.588Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.588Z",
            "created_at": "2025-11-09T14:40:46.588Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1441",
              "status": "open",
              "type": "issue",
              "number": 1441,
              "title": "[bounty] general purpose scrapper",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1441",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] general purpose scrapper",
                  "body": "\nv0\n\nshould be able to scrap whatsapp\n\nideally you can use the keyboard / mouse api \n\nhttps://docs.screenpi.pe/docs/sdk-reference#input-control-api\n\n\nbasically many ppl would like to scrap data from desktop app and it's hard, like whatsapp, imessage, or even web but without getting banned or because it's local authentication is already done by user usually and stays local, and much less likely detected \n\n\n\n/bounty 200\n\nplease suggest how you would implement a very simple first version that work for whatsapp and UX\n\n\n\n\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1441"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1441",
              "body": "\nv0\n\nshould be able to scrap whatsapp\n\nideally you can use the keyboard / mouse api \n\nhttps://docs.screenpi.pe/docs/sdk-reference#input-control-api\n\n\nbasically many ppl would like to scrap data from desktop app and it's hard, like whatsapp, imessage, or even web but without getting banned or because it's local authentication is already done by user usually and stays local, and much less likely detected \n\n\n\n/bounty 200\n\nplease suggest how you would implement a very simple first version that work for whatsapp and UX\n\n\n\n\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1441",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1298",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:46.790Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.790Z",
            "created_at": "2025-11-09T14:40:46.790Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1298",
              "status": "open",
              "type": "issue",
              "number": 1298,
              "title": "[docs] document env var somewhere somehow ",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1298",
                  "user": {
                    "login": "dassio",
                    "id": 733502,
                    "node_id": "MDQ6VXNlcjczMzUwMg==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/733502?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/dassio",
                    "html_url": "https://github.com/dassio",
                    "followers_url": "https://api.github.com/users/dassio/followers",
                    "following_url": "https://api.github.com/users/dassio/following{/other_user}",
                    "gists_url": "https://api.github.com/users/dassio/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/dassio/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/dassio/subscriptions",
                    "organizations_url": "https://api.github.com/users/dassio/orgs",
                    "repos_url": "https://api.github.com/users/dassio/repos",
                    "events_url": "https://api.github.com/users/dassio/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/dassio/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[docs] document env var somewhere somehow ",
                  "body": "**describe the feature**\nin the desktop app, you can configure hugging face mirror, but for the screenpipe server cli ,there is no such option , when try to use whisper for audio transcription, need to download mode from hugging face \n\n**why is this needed?**\nhugging face is banned in China\n\n\n**additional context**\n```\nyou are using local processing. all your data stays on your computer.\n\nwarning: telemetry is enabled. only error-level data will be sent to highlight.io.\nto disable, use the --disable-telemetry flag.\n\ncheck latest changes here: https://github.com/mediar-ai/screenpipe/releases\n2025-02-07T14:15:37.861529Z  INFO screenpipe: starting pipes\nfailed to start pipe timeline: No pipe.js/pipe.ts found in the pipe/dist directory\n2025-02-07T14:15:37.864597Z  INFO screenpipe_server::server: Server starting on 127.0.0.1:3030\n2025-02-07T14:15:45.839709Z  INFO screenpipe_server::video: Starting FFmpeg process for file: C:\\Users\\dassi\\.screenpipe\\data\\monitor_490607759_2025-02-07_14-15-45.mp4\n2025-02-07T14:15:48.283937Z  INFO screenpipe_server::resource_monitor: Runtime: 10s, Total Memory: 2% (0.25 GB / 16.09 GB), Total CPU: 88%\n2025-02-07T14:15:58.322953Z  INFO screenpipe_server::resource_monitor: Runtime: 20s, Total Memory: 2% (0.26 GB / 16.09 GB), Total CPU: 92%\n2025-02-07T14:16:00.374953Z ERROR screenpipe: continuous recording error: request error: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n\nCaused by:\n    0: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n    1: connection timed out\n```\n\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1298"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1298",
              "body": "**describe the feature**\nin the desktop app, you can configure hugging face mirror, but for the screenpipe server cli ,there is no such option , when try to use whisper for audio transcription, need to download mode from hugging face \n\n**why is this needed?**\nhugging face is banned in China\n\n\n**additional context**\n```\nyou are using local processing. all your data stays on your computer.\n\nwarning: telemetry is enabled. only error-level data will be sent to highlight.io.\nto disable, use the --disable-telemetry flag.\n\ncheck latest changes here: https://github.com/mediar-ai/screenpipe/releases\n2025-02-07T14:15:37.861529Z  INFO screenpipe: starting pipes\nfailed to start pipe timeline: No pipe.js/pipe.ts found in the pipe/dist directory\n2025-02-07T14:15:37.864597Z  INFO screenpipe_server::server: Server starting on 127.0.0.1:3030\n2025-02-07T14:15:45.839709Z  INFO screenpipe_server::video: Starting FFmpeg process for file: C:\\Users\\dassi\\.screenpipe\\data\\monitor_490607759_2025-02-07_14-15-45.mp4\n2025-02-07T14:15:48.283937Z  INFO screenpipe_server::resource_monitor: Runtime: 10s, Total Memory: 2% (0.25 GB / 16.09 GB), Total CPU: 88%\n2025-02-07T14:15:58.322953Z  INFO screenpipe_server::resource_monitor: Runtime: 20s, Total Memory: 2% (0.26 GB / 16.09 GB), Total CPU: 92%\n2025-02-07T14:16:00.374953Z ERROR screenpipe: continuous recording error: request error: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n\nCaused by:\n    0: https://huggingface.co/distil-whisper/distil-large-v3/resolve/main/config.json: Connection Failed: Connect error: connection timed out\n    1: connection timed out\n```\n\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1298",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1383",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:46.929Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:46.929Z",
            "created_at": "2025-11-09T14:40:46.929Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1383",
              "status": "open",
              "type": "issue",
              "number": 1383,
              "title": "[bounty] implement deep research in search pipe",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1383",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] implement deep research in search pipe",
                  "body": "\n\ni love perplexity deep research \n\ni think we should have something like this in screenpipe search, which would be something that go beyond surface level, really in depth on your data \n\nnot sure about the UX yet, suggest any design first \n\nshould allow async (eg ask something and come back later to it, stored in some kind of history, maybe related to #1382 check perplexity UI too)\n\n/bounty 200 \n\n\n\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1383"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1383",
              "body": "\n\ni love perplexity deep research \n\ni think we should have something like this in screenpipe search, which would be something that go beyond surface level, really in depth on your data \n\nnot sure about the UX yet, suggest any design first \n\nshould allow async (eg ask something and come back later to it, stored in some kind of history, maybe related to #1382 check perplexity UI too)\n\n/bounty 200 \n\n\n\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1383",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1382",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:47.248Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:47.248Z",
            "created_at": "2025-11-09T14:40:47.248Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1382",
              "status": "open",
              "type": "issue",
              "number": 1382,
              "title": "[bounty] implement history for search pipe",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1382",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] implement history for search pipe",
                  "body": "like chatgpt history a bit \n\nwe had this but did not work well\n\ncheck shadcn sidebar and stuff \n\n\n/bounty 100 ",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1382"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1382",
              "body": "like chatgpt history a bit \n\nwe had this but did not work well\n\ncheck shadcn sidebar and stuff \n\n\n/bounty 100 ",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1382",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1380",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:47.786Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:47.786Z",
            "created_at": "2025-11-09T14:40:47.786Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1380",
              "status": "open",
              "type": "issue",
              "number": 1380,
              "title": "[bounty] implement device control and make --use-all-monitors work",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1380",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] implement device control and make --use-all-monitors work",
                  "body": "\nthis feature is useful to improve the experience for:\n1. people switching audio device and monitor devices regularly (commuting for work etc.) (e.g. `--use-all-monitors` argument to always record all monitors for example)\n2. privacy preserving use cases \n3. control resource usage dynamically \n4. other use cases\n\ndefinition of done:\n- `--use-all-monitors` always record all monitor available, and stop recording when it's unplugged \n- running screenpipe for 3 days, while having meetings, working, etc. and the memory stays under 2.5 gb, and CPU roughly the same (10-30% on my mac book pro m4 max)\n- /vision/start, /vision/stop, /vision/list\n\nmake this work reliably and prove it's not memory leaking \n\n\n\n/bounty 400 \n\n\n\nbonus bounty:\n- `--use-all-default-audio-devices` always record all default audio devices\n- very good tests, good benchmarks of memory over time somehow, idk\n\n\n\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1380"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1380",
              "body": "\nthis feature is useful to improve the experience for:\n1. people switching audio device and monitor devices regularly (commuting for work etc.) (e.g. `--use-all-monitors` argument to always record all monitors for example)\n2. privacy preserving use cases \n3. control resource usage dynamically \n4. other use cases\n\ndefinition of done:\n- `--use-all-monitors` always record all monitor available, and stop recording when it's unplugged \n- running screenpipe for 3 days, while having meetings, working, etc. and the memory stays under 2.5 gb, and CPU roughly the same (10-30% on my mac book pro m4 max)\n- /vision/start, /vision/stop, /vision/list\n\nmake this work reliably and prove it's not memory leaking \n\n\n\n/bounty 400 \n\n\n\nbonus bounty:\n- `--use-all-default-audio-devices` always record all default audio devices\n- very good tests, good benchmarks of memory over time somehow, idk\n\n\n\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1380",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1160",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:48.943Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:48.943Z",
            "created_at": "2025-11-09T14:40:48.943Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1160",
              "status": "open",
              "type": "issue",
              "number": 1160,
              "title": "[bounty] list webcam, iphone, etc. in list-monitors",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1160",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] list webcam, iphone, etc. in list-monitors",
                  "body": "we should be able to record webcam, iphone cameras, etc. easily in the same way than monitor device\n\n```\nlouisbeaumont@MacBook-Pro-9:~/Documents/brain$ ffmpeg -f avfoundation -list_devices true -i \"\"\nffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1_4 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.100 / 61. 19.100\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n  libpostproc    58.  3.100 / 58.  3.100\n2025-01-16 09:03:00.993 ffmpeg[23450:7179261] WARNING: Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.\n2025-01-16 09:03:01.177 ffmpeg[23450:7179261] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n[AVFoundation indev @ 0x1426314d0] AVFoundation video devices:\n[AVFoundation indev @ 0x1426314d0] [0] MacBook Pro Camera\n[AVFoundation indev @ 0x1426314d0] [1] louisbeaumont.me iphone Camera\n[AVFoundation indev @ 0x1426314d0] [2] louisbeaumont.me iphone Desk View Camera\n[AVFoundation indev @ 0x1426314d0] [3] MacBook Pro Desk View Camera\n[AVFoundation indev @ 0x1426314d0] [4] louisbeaumont.me iphone Desk View Camera\n[AVFoundation indev @ 0x1426314d0] [5] louisbeaumont.me iphone Camera\n[AVFoundation indev @ 0x1426314d0] [6] Capture screen 0\n[AVFoundation indev @ 0x1426314d0] AVFoundation audio devices:\n[AVFoundation indev @ 0x1426314d0] [0] louisbeaumont.me iphone Microphone\n[AVFoundation indev @ 0x1426314d0] [1] Immersed\n[AVFoundation indev @ 0x1426314d0] [2] BlackHole 2ch\n[AVFoundation indev @ 0x1426314d0] [3] louisbeaumont.me iphone Microphone\n[AVFoundation indev @ 0x1426314d0] [4] MacBook Pro Microphone\n[AVFoundation indev @ 0x1426314d0] [5] BlackHole 16ch\n[AVFoundation indev @ 0x1426314d0] [6] Aggregate Device\n[AVFoundation indev @ 0x1426314d0] [7] input\n[AVFoundation indev @ 0x1426314d0] [8] EpocCam Microphone\n[in#0 @ 0x142630ef0] Error opening input: Input/output error\nError opening input file .\nError opening input files: Input/output error\nlouisbeaumont@MacBook-Pro-9:~/Documents/brain$ \n```\n\n/bounty 120",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1160"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1160",
              "body": "we should be able to record webcam, iphone cameras, etc. easily in the same way than monitor device\n\n```\nlouisbeaumont@MacBook-Pro-9:~/Documents/brain$ ffmpeg -f avfoundation -list_devices true -i \"\"\nffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n  configuration: --prefix=/opt/homebrew/Cellar/ffmpeg/7.1_4 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox --enable-neon\n  libavutil      59. 39.100 / 59. 39.100\n  libavcodec     61. 19.100 / 61. 19.100\n  libavformat    61.  7.100 / 61.  7.100\n  libavdevice    61.  3.100 / 61.  3.100\n  libavfilter    10.  4.100 / 10.  4.100\n  libswscale      8.  3.100 /  8.  3.100\n  libswresample   5.  3.100 /  5.  3.100\n  libpostproc    58.  3.100 / 58.  3.100\n2025-01-16 09:03:00.993 ffmpeg[23450:7179261] WARNING: Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.\n2025-01-16 09:03:01.177 ffmpeg[23450:7179261] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n[AVFoundation indev @ 0x1426314d0] AVFoundation video devices:\n[AVFoundation indev @ 0x1426314d0] [0] MacBook Pro Camera\n[AVFoundation indev @ 0x1426314d0] [1] louisbeaumont.me iphone Camera\n[AVFoundation indev @ 0x1426314d0] [2] louisbeaumont.me iphone Desk View Camera\n[AVFoundation indev @ 0x1426314d0] [3] MacBook Pro Desk View Camera\n[AVFoundation indev @ 0x1426314d0] [4] louisbeaumont.me iphone Desk View Camera\n[AVFoundation indev @ 0x1426314d0] [5] louisbeaumont.me iphone Camera\n[AVFoundation indev @ 0x1426314d0] [6] Capture screen 0\n[AVFoundation indev @ 0x1426314d0] AVFoundation audio devices:\n[AVFoundation indev @ 0x1426314d0] [0] louisbeaumont.me iphone Microphone\n[AVFoundation indev @ 0x1426314d0] [1] Immersed\n[AVFoundation indev @ 0x1426314d0] [2] BlackHole 2ch\n[AVFoundation indev @ 0x1426314d0] [3] louisbeaumont.me iphone Microphone\n[AVFoundation indev @ 0x1426314d0] [4] MacBook Pro Microphone\n[AVFoundation indev @ 0x1426314d0] [5] BlackHole 16ch\n[AVFoundation indev @ 0x1426314d0] [6] Aggregate Device\n[AVFoundation indev @ 0x1426314d0] [7] input\n[AVFoundation indev @ 0x1426314d0] [8] EpocCam Microphone\n[in#0 @ 0x142630ef0] Error opening input: Input/output error\nError opening input file .\nError opening input files: Input/output error\nlouisbeaumont@MacBook-Pro-9:~/Documents/brain$ \n```\n\n/bounty 120",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1160",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "mediar-ai#1142",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "mediar-ai",
              "id": "generated-mediar-ai",
              "name": "Mediar-ai",
              "description": "",
              "members": [],
              "display_name": "Mediar-ai",
              "created_at": "2025-11-09T14:40:50.882Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/mediar-ai?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "mediar-ai",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:40:50.882Z",
            "created_at": "2025-11-09T14:40:50.882Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-mediar-ai#1142",
              "status": "open",
              "type": "issue",
              "number": 1142,
              "title": "[bounty] support for video and voice LLM in search, timeline, meeting",
              "source": {
                "data": {
                  "id": "source-mediar-ai#1142",
                  "user": {
                    "login": "louis030195",
                    "id": 25003283,
                    "node_id": "MDQ6VXNlcjI1MDAzMjgz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25003283?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/louis030195",
                    "html_url": "https://github.com/louis030195",
                    "followers_url": "https://api.github.com/users/louis030195/followers",
                    "following_url": "https://api.github.com/users/louis030195/following{/other_user}",
                    "gists_url": "https://api.github.com/users/louis030195/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/louis030195/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/louis030195/subscriptions",
                    "organizations_url": "https://api.github.com/users/louis030195/orgs",
                    "repos_url": "https://api.github.com/users/louis030195/repos",
                    "events_url": "https://api.github.com/users/louis030195/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/louis030195/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[bounty] support for video and voice LLM in search, timeline, meeting",
                  "body": "likely need to break down in multiple bounties\r\n\r\n\r\n\r\n\r\n/bounty 400 \r\n\r\n\r\neg\r\n- meeting: use voice LLM to transcribe or summarize audio would increase a lot quality - 10x better than granola etc\r\n- search: use video LLM would be much more powerful and different context windows \r\n- timeline: same\r\n\r\n\r\nsuggest rough design, might create other issues \r\n\r\n\r\n\r\n",
                  "html_url": "https://github.com/mediar-ai/screenpipe/issues/1142"
                },
                "type": "github"
              },
              "hash": "mediar-ai/screenpipe#1142",
              "body": "likely need to break down in multiple bounties\r\n\r\n\r\n\r\n\r\n/bounty 400 \r\n\r\n\r\neg\r\n- meeting: use voice LLM to transcribe or summarize audio would increase a lot quality - 10x better than granola etc\r\n- search: use video LLM would be much more powerful and different context windows \r\n- timeline: same\r\n\r\n\r\nsuggest rough design, might create other issues \r\n\r\n\r\n\r\n",
              "url": "https://github.com/mediar-ai/screenpipe/issues/1142",
              "tech": [],
              "repo_name": "screenpipe",
              "repo_owner": "mediar-ai",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#3697",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.084Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.084Z",
            "created_at": "2025-11-09T14:41:11.084Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#3697",
              "status": "open",
              "type": "issue",
              "number": 3697,
              "title": "Datastar requests from Endpoint",
              "source": {
                "data": {
                  "id": "source-ZIO#3697",
                  "user": {
                    "login": "987Nabil",
                    "id": 7283535,
                    "node_id": "MDQ6VXNlcjcyODM1MzU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/7283535?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/987Nabil",
                    "html_url": "https://github.com/987Nabil",
                    "followers_url": "https://api.github.com/users/987Nabil/followers",
                    "following_url": "https://api.github.com/users/987Nabil/following{/other_user}",
                    "gists_url": "https://api.github.com/users/987Nabil/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/987Nabil/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/987Nabil/subscriptions",
                    "organizations_url": "https://api.github.com/users/987Nabil/orgs",
                    "repos_url": "https://api.github.com/users/987Nabil/repos",
                    "events_url": "https://api.github.com/users/987Nabil/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/987Nabil/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Datastar requests from Endpoint",
                  "body": "Build Datastar expressions for request against an Endpoint from its definition\n",
                  "html_url": "https://github.com/zio/zio-http/issues/3697"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#3697",
              "body": "Build Datastar expressions for request against an Endpoint from its definition\n",
              "url": "https://github.com/zio/zio-http/issues/3697",
              "tech": [
                "go"
              ],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#709",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.193Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.193Z",
            "created_at": "2025-11-09T14:41:11.193Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#709",
              "status": "open",
              "type": "issue",
              "number": 709,
              "title": "Support Http Range header on request for Files",
              "source": {
                "data": {
                  "id": "source-ZIO#709",
                  "user": {
                    "login": "ashprakasan",
                    "id": 8946971,
                    "node_id": "MDQ6VXNlcjg5NDY5NzE=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8946971?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ashprakasan",
                    "html_url": "https://github.com/ashprakasan",
                    "followers_url": "https://api.github.com/users/ashprakasan/followers",
                    "following_url": "https://api.github.com/users/ashprakasan/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ashprakasan/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ashprakasan/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ashprakasan/subscriptions",
                    "organizations_url": "https://api.github.com/users/ashprakasan/orgs",
                    "repos_url": "https://api.github.com/users/ashprakasan/repos",
                    "events_url": "https://api.github.com/users/ashprakasan/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ashprakasan/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Support Http Range header on request for Files",
                  "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nThe Range HTTP request header indicates the part of a document that the server should return. Several parts can be requested with one Range header at once, and the server may send back these ranges in a multipart document.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExample requesting 3 ranges from files -\r\n`Range: bytes=200-1000, 2000-6576, 19000-`\r\n\r\nSend only those parts of the document in Response.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCurrently, the range is hardcoded as follows - \r\n` ctx.write(new DefaultFileRegion(raf.getChannel, 0, fileLength))`\r\nManipulate the positions as per request headers instead.\r\n\r\n**Additional context**\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Range\r\n",
                  "html_url": "https://github.com/zio/zio-http/issues/709"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#709",
              "body": "**Is your feature request related to a problem? Please describe.**\r\n\r\nThe Range HTTP request header indicates the part of a document that the server should return. Several parts can be requested with one Range header at once, and the server may send back these ranges in a multipart document.\r\n\r\n**Describe the solution you'd like**\r\n\r\nExample requesting 3 ranges from files -\r\n`Range: bytes=200-1000, 2000-6576, 19000-`\r\n\r\nSend only those parts of the document in Response.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nCurrently, the range is hardcoded as follows - \r\n` ctx.write(new DefaultFileRegion(raf.getChannel, 0, fileLength))`\r\nManipulate the positions as per request headers instead.\r\n\r\n**Additional context**\r\n\r\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Range\r\n",
              "url": "https://github.com/zio/zio-http/issues/709",
              "tech": [],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#595",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.293Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.294Z",
            "created_at": "2025-11-09T14:41:11.294Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#595",
              "status": "open",
              "type": "issue",
              "number": 595,
              "title": "STOMP support",
              "source": {
                "data": {
                  "id": "source-ZIO#595",
                  "user": {
                    "login": "octavz",
                    "id": 23629302,
                    "node_id": "MDQ6VXNlcjIzNjI5MzAy",
                    "avatar_url": "https://avatars.githubusercontent.com/u/23629302?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/octavz",
                    "html_url": "https://github.com/octavz",
                    "followers_url": "https://api.github.com/users/octavz/followers",
                    "following_url": "https://api.github.com/users/octavz/following{/other_user}",
                    "gists_url": "https://api.github.com/users/octavz/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/octavz/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/octavz/subscriptions",
                    "organizations_url": "https://api.github.com/users/octavz/orgs",
                    "repos_url": "https://api.github.com/users/octavz/repos",
                    "events_url": "https://api.github.com/users/octavz/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/octavz/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "STOMP support",
                  "body": "Would be nice to support STOMP, probably not a priority for authors, but I thought about adding it, perhaps someone can contribute.\r\n\r\nNetty already implements support for STOMP,  `zio-http` should \"decode/encode\" frames as proper \"Stomp\" and add a typed API to work with it. \r\n\r\nThanks",
                  "html_url": "https://github.com/zio/zio-http/issues/595"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#595",
              "body": "Would be nice to support STOMP, probably not a priority for authors, but I thought about adding it, perhaps someone can contribute.\r\n\r\nNetty already implements support for STOMP,  `zio-http` should \"decode/encode\" frames as proper \"Stomp\" and add a typed API to work with it. \r\n\r\nThanks",
              "url": "https://github.com/zio/zio-http/issues/595",
              "tech": [],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#3645",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.382Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.382Z",
            "created_at": "2025-11-09T14:41:11.382Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#3645",
              "status": "open",
              "type": "issue",
              "number": 3645,
              "title": "Default 404 page with available routes for dev mode",
              "source": {
                "data": {
                  "id": "source-ZIO#3645",
                  "user": {
                    "login": "987Nabil",
                    "id": 7283535,
                    "node_id": "MDQ6VXNlcjcyODM1MzU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/7283535?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/987Nabil",
                    "html_url": "https://github.com/987Nabil",
                    "followers_url": "https://api.github.com/users/987Nabil/followers",
                    "following_url": "https://api.github.com/users/987Nabil/following{/other_user}",
                    "gists_url": "https://api.github.com/users/987Nabil/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/987Nabil/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/987Nabil/subscriptions",
                    "organizations_url": "https://api.github.com/users/987Nabil/orgs",
                    "repos_url": "https://api.github.com/users/987Nabil/repos",
                    "events_url": "https://api.github.com/users/987Nabil/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/987Nabil/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Default 404 page with available routes for dev mode",
                  "body": "Depends on dev mode being implemented \n",
                  "html_url": "https://github.com/zio/zio-http/issues/3645"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#3645",
              "body": "Depends on dev mode being implemented \n",
              "url": "https://github.com/zio/zio-http/issues/3645",
              "tech": [],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#3472",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.476Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.476Z",
            "created_at": "2025-11-09T14:41:11.476Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#3472",
              "status": "open",
              "type": "issue",
              "number": 3472,
              "title": "Split into multiple modules",
              "source": {
                "data": {
                  "id": "source-ZIO#3472",
                  "user": {
                    "login": "987Nabil",
                    "id": 7283535,
                    "node_id": "MDQ6VXNlcjcyODM1MzU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/7283535?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/987Nabil",
                    "html_url": "https://github.com/987Nabil",
                    "followers_url": "https://api.github.com/users/987Nabil/followers",
                    "following_url": "https://api.github.com/users/987Nabil/following{/other_user}",
                    "gists_url": "https://api.github.com/users/987Nabil/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/987Nabil/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/987Nabil/subscriptions",
                    "organizations_url": "https://api.github.com/users/987Nabil/orgs",
                    "repos_url": "https://api.github.com/users/987Nabil/repos",
                    "events_url": "https://api.github.com/users/987Nabil/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/987Nabil/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Split into multiple modules",
                  "body": "Currently, the zio-http artifact contains a lot of different parts of zio-http exclusively. They are not separate maven artifacts.\n\nWe want to change this, to support future changes/features.\n\nThere should be at least these modules that are published into maven. \n\n1. core\n2. endpoint\n3. netty\n\nShould we have client and server in different modules?",
                  "html_url": "https://github.com/zio/zio-http/issues/3472"
                },
                "type": "github"
              },
              "hash": "zio/zio-http#3472",
              "body": "Currently, the zio-http artifact contains a lot of different parts of zio-http exclusively. They are not separate maven artifacts.\n\nWe want to change this, to support future changes/features.\n\nThere should be at least these modules that are published into maven. \n\n1. core\n2. endpoint\n3. netty\n\nShould we have client and server in different modules?",
              "url": "https://github.com/zio/zio-http/issues/3472",
              "tech": [],
              "repo_name": "zio-http",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#9810",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.564Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.564Z",
            "created_at": "2025-11-09T14:41:11.564Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#9810",
              "status": "open",
              "type": "issue",
              "number": 9810,
              "title": "ZStreams buffer(1) is buffering 2.",
              "source": {
                "data": {
                  "id": "source-ZIO#9810",
                  "user": {
                    "login": "douglasthomsen",
                    "id": 88000378,
                    "node_id": "MDQ6VXNlcjg4MDAwMzc4",
                    "avatar_url": "https://avatars.githubusercontent.com/u/88000378?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/douglasthomsen",
                    "html_url": "https://github.com/douglasthomsen",
                    "followers_url": "https://api.github.com/users/douglasthomsen/followers",
                    "following_url": "https://api.github.com/users/douglasthomsen/following{/other_user}",
                    "gists_url": "https://api.github.com/users/douglasthomsen/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/douglasthomsen/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/douglasthomsen/subscriptions",
                    "organizations_url": "https://api.github.com/users/douglasthomsen/orgs",
                    "repos_url": "https://api.github.com/users/douglasthomsen/repos",
                    "events_url": "https://api.github.com/users/douglasthomsen/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/douglasthomsen/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "ZStreams buffer(1) is buffering 2.",
                  "body": "I am using zio 2.1.17. When I run the following code:\n\n```scala\ndef fakeNetworkCall(n: Int): ZIO[Any, Throwable, String] = {\n  for {\n    _ <- Console.printLine(s\"Starting request $n\")\n    _ <- ZIO.sleep(1.second)\n    _ <- Console.printLine(s\"Completed request $n\")\n  } yield s\"Response for $n\"\n}\n\nval program: ZIO[Any, Throwable, Unit] =\n  ZStream\n    .fromIterator(Iterator.from(1))\n    .mapZIO(fakeNetworkCall)\n    .buffer(1)\n    .runForeach { response =>\n      for {\n        _ <- Console.printLine(s\"Press Enter to process $response...\")\n        _ <- ZIO.sleep(100.minutes)\n        _ <- Console.printLine(s\"Processing response $response\")\n        _ <- ZIO.sleep(1.second)\n        _ <- Console.printLine(s\"Done processing $response\")\n      } yield ()\n    }\n```\nThe full code is [here](https://scastie.scala-lang.org/douglasthomsen/kvRuhoAGRjarj9djF53N0g/10).\n\nWhen i get to the `_ <- ZIO.sleep(100.minutes)` line I would expect the output to be like this:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\n```\n\nbut I am getting the following:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\nStarting request 3\nCompleted request 3\n```\n\nMy goal is to only buffer one call to `fakeNetworkCall` at time. Right now it looks like it is buffering two. I am I doing something wrong or is this a bug?",
                  "html_url": "https://github.com/zio/zio/issues/9810"
                },
                "type": "github"
              },
              "hash": "zio/zio#9810",
              "body": "I am using zio 2.1.17. When I run the following code:\n\n```scala\ndef fakeNetworkCall(n: Int): ZIO[Any, Throwable, String] = {\n  for {\n    _ <- Console.printLine(s\"Starting request $n\")\n    _ <- ZIO.sleep(1.second)\n    _ <- Console.printLine(s\"Completed request $n\")\n  } yield s\"Response for $n\"\n}\n\nval program: ZIO[Any, Throwable, Unit] =\n  ZStream\n    .fromIterator(Iterator.from(1))\n    .mapZIO(fakeNetworkCall)\n    .buffer(1)\n    .runForeach { response =>\n      for {\n        _ <- Console.printLine(s\"Press Enter to process $response...\")\n        _ <- ZIO.sleep(100.minutes)\n        _ <- Console.printLine(s\"Processing response $response\")\n        _ <- ZIO.sleep(1.second)\n        _ <- Console.printLine(s\"Done processing $response\")\n      } yield ()\n    }\n```\nThe full code is [here](https://scastie.scala-lang.org/douglasthomsen/kvRuhoAGRjarj9djF53N0g/10).\n\nWhen i get to the `_ <- ZIO.sleep(100.minutes)` line I would expect the output to be like this:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\n```\n\nbut I am getting the following:\n```\nStarting request 1\nCompleted request 1\nStarting request 2\nPress Enter to process Response for 1...\nCompleted request 2\nStarting request 3\nCompleted request 3\n```\n\nMy goal is to only buffer one call to `fakeNetworkCall` at time. Right now it looks like it is buffering two. I am I doing something wrong or is this a bug?",
              "url": "https://github.com/zio/zio/issues/9810",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#9844",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.662Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.662Z",
            "created_at": "2025-11-09T14:41:11.662Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#9844",
              "status": "open",
              "type": "issue",
              "number": 9844,
              "title": "improved `Queue` shutdown functionality",
              "source": {
                "data": {
                  "id": "source-ZIO#9844",
                  "user": {
                    "login": "mberndt123",
                    "id": 11650737,
                    "node_id": "MDQ6VXNlcjExNjUwNzM3",
                    "avatar_url": "https://avatars.githubusercontent.com/u/11650737?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/mberndt123",
                    "html_url": "https://github.com/mberndt123",
                    "followers_url": "https://api.github.com/users/mberndt123/followers",
                    "following_url": "https://api.github.com/users/mberndt123/following{/other_user}",
                    "gists_url": "https://api.github.com/users/mberndt123/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/mberndt123/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/mberndt123/subscriptions",
                    "organizations_url": "https://api.github.com/users/mberndt123/orgs",
                    "repos_url": "https://api.github.com/users/mberndt123/repos",
                    "events_url": "https://api.github.com/users/mberndt123/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/mberndt123/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "improved `Queue` shutdown functionality",
                  "body": "Hey, I've recently proposed something on Discord, and since feedback has been rather positive, I'm making a ticket to track the idea.\n\nI've been working with Queues recently, and I've been having some issues around `shutdown` that I would like to address.\n\nSpecifically, I find it a common pattern that I send some kind of request object through a queue because I want another fiber to perform some action on my behalf. Along with the request, I send a `Promise` to have that fiber communicate the outcome of that action to me. By and large this works fine. The issue arises when the fiber that I'm sending requests to fails. In that case, I would like it to communicate the cause of the failure back to the other fibers. This is easy enough for the requests that I've already pulled out of the queue: I simply fail those promises.\nBut I also need to deal with other cases: fibers currently blocked in an `offer` call, future attempts to `offer` to the queue, and I also need to deal with requests that have been submitted to the queue but not yet retrieved.\n\nSo my idea is as follows:\n - add an `E` type parameter to `Queue`\n - add a `shutdownCause` method that takes a type parameter of type `Cause[E]`\n - `shutdownCause` would also return the items currently buffered in the queue in order to dispose of them\n - after `shutdownCause` has been called, any attempt to interact with the queue will fail with the cause\n - methods like `take, offer` etc. should indicate errors of type `E`\n - streams created with `ZStream.fromQueue` would also fail with this cause\n - `shutdownCause` should be atomic: when multiple fibers call it at the same time, one of them wins and the others fail with the cause supplied by the winner\n\nAfaik, adding a new method is a binary compatible change, as is adding a new type parameter. Hence I think this is a source incompatible but binary compatible change. @ghostdogpr therefore suggested it could be added in a ZIO 2.2 release.\n",
                  "html_url": "https://github.com/zio/zio/issues/9844"
                },
                "type": "github"
              },
              "hash": "zio/zio#9844",
              "body": "Hey, I've recently proposed something on Discord, and since feedback has been rather positive, I'm making a ticket to track the idea.\n\nI've been working with Queues recently, and I've been having some issues around `shutdown` that I would like to address.\n\nSpecifically, I find it a common pattern that I send some kind of request object through a queue because I want another fiber to perform some action on my behalf. Along with the request, I send a `Promise` to have that fiber communicate the outcome of that action to me. By and large this works fine. The issue arises when the fiber that I'm sending requests to fails. In that case, I would like it to communicate the cause of the failure back to the other fibers. This is easy enough for the requests that I've already pulled out of the queue: I simply fail those promises.\nBut I also need to deal with other cases: fibers currently blocked in an `offer` call, future attempts to `offer` to the queue, and I also need to deal with requests that have been submitted to the queue but not yet retrieved.\n\nSo my idea is as follows:\n - add an `E` type parameter to `Queue`\n - add a `shutdownCause` method that takes a type parameter of type `Cause[E]`\n - `shutdownCause` would also return the items currently buffered in the queue in order to dispose of them\n - after `shutdownCause` has been called, any attempt to interact with the queue will fail with the cause\n - methods like `take, offer` etc. should indicate errors of type `E`\n - streams created with `ZStream.fromQueue` would also fail with this cause\n - `shutdownCause` should be atomic: when multiple fibers call it at the same time, one of them wins and the others fail with the cause supplied by the winner\n\nAfaik, adding a new method is a binary compatible change, as is adding a new type parameter. Hence I think this is a source incompatible but binary compatible change. @ghostdogpr therefore suggested it could be added in a ZIO 2.2 release.\n",
              "url": "https://github.com/zio/zio/issues/9844",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#9878",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.779Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.779Z",
            "created_at": "2025-11-09T14:41:11.779Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#9878",
              "status": "open",
              "type": "issue",
              "number": 9878,
              "title": "ZScheduler parks+unparks workers too frequently",
              "source": {
                "data": {
                  "id": "source-ZIO#9878",
                  "user": {
                    "login": "hearnadam",
                    "id": 22334119,
                    "node_id": "MDQ6VXNlcjIyMzM0MTE5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/22334119?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/hearnadam",
                    "html_url": "https://github.com/hearnadam",
                    "followers_url": "https://api.github.com/users/hearnadam/followers",
                    "following_url": "https://api.github.com/users/hearnadam/following{/other_user}",
                    "gists_url": "https://api.github.com/users/hearnadam/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/hearnadam/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/hearnadam/subscriptions",
                    "organizations_url": "https://api.github.com/users/hearnadam/orgs",
                    "repos_url": "https://api.github.com/users/hearnadam/repos",
                    "events_url": "https://api.github.com/users/hearnadam/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/hearnadam/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "ZScheduler parks+unparks workers too frequently",
                  "body": "Unparking workers is slow and invoked in the hotpath too often. I think we may need to trade some fairness for aggression to avoid excessive cycling.\n\n`maybeUnparkWorker` (obviously `LockSupport.unpark(worker)`) is very expensive: https://github.com/zio/zio/blob/series/2.x/core/jvm-native/src/main/scala/zio/internal/ZScheduler.scala#L443-L454",
                  "html_url": "https://github.com/zio/zio/issues/9878"
                },
                "type": "github"
              },
              "hash": "zio/zio#9878",
              "body": "Unparking workers is slow and invoked in the hotpath too often. I think we may need to trade some fairness for aggression to avoid excessive cycling.\n\n`maybeUnparkWorker` (obviously `LockSupport.unpark(worker)`) is very expensive: https://github.com/zio/zio/blob/series/2.x/core/jvm-native/src/main/scala/zio/internal/ZScheduler.scala#L443-L454",
              "url": "https://github.com/zio/zio/issues/9878",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#9877",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.884Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.884Z",
            "created_at": "2025-11-09T14:41:11.884Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#9877",
              "status": "open",
              "type": "issue",
              "number": 9877,
              "title": "Can Fiber(Runtime) and Promise be merged?",
              "source": {
                "data": {
                  "id": "source-ZIO#9877",
                  "user": {
                    "login": "hearnadam",
                    "id": 22334119,
                    "node_id": "MDQ6VXNlcjIyMzM0MTE5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/22334119?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/hearnadam",
                    "html_url": "https://github.com/hearnadam",
                    "followers_url": "https://api.github.com/users/hearnadam/followers",
                    "following_url": "https://api.github.com/users/hearnadam/following{/other_user}",
                    "gists_url": "https://api.github.com/users/hearnadam/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/hearnadam/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/hearnadam/subscriptions",
                    "organizations_url": "https://api.github.com/users/hearnadam/orgs",
                    "repos_url": "https://api.github.com/users/hearnadam/repos",
                    "events_url": "https://api.github.com/users/hearnadam/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/hearnadam/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Can Fiber(Runtime) and Promise be merged?",
                  "body": "A Promise awaiting completion is essentially a Fiber parked awaiting an async callback. When a Fiber is forking work (which will eventually complete a promise), then awaiting a Promise, we end up with unnecessary allocations + indirection.\n\nit would be useful to have `Promise.become` or similar to link fibers/promises.",
                  "html_url": "https://github.com/zio/zio/issues/9877"
                },
                "type": "github"
              },
              "hash": "zio/zio#9877",
              "body": "A Promise awaiting completion is essentially a Fiber parked awaiting an async callback. When a Fiber is forking work (which will eventually complete a promise), then awaiting a Promise, we end up with unnecessary allocations + indirection.\n\nit would be useful to have `Promise.become` or similar to link fibers/promises.",
              "url": "https://github.com/zio/zio/issues/9877",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "ZIO#9874",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "ZIO",
              "id": "generated-ZIO",
              "name": "ZIO",
              "description": "",
              "members": [],
              "display_name": "ZIO",
              "created_at": "2025-11-09T14:41:11.966Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/ZIO?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "zio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.966Z",
            "created_at": "2025-11-09T14:41:11.966Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-ZIO#9874",
              "status": "open",
              "type": "issue",
              "number": 9874,
              "title": "Handling errors allows recovering from defects",
              "source": {
                "data": {
                  "id": "source-ZIO#9874",
                  "user": {
                    "login": "kyri-petrou",
                    "id": 67301607,
                    "node_id": "MDQ6VXNlcjY3MzAxNjA3",
                    "avatar_url": "https://avatars.githubusercontent.com/u/67301607?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/kyri-petrou",
                    "html_url": "https://github.com/kyri-petrou",
                    "followers_url": "https://api.github.com/users/kyri-petrou/followers",
                    "following_url": "https://api.github.com/users/kyri-petrou/following{/other_user}",
                    "gists_url": "https://api.github.com/users/kyri-petrou/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/kyri-petrou/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/kyri-petrou/subscriptions",
                    "organizations_url": "https://api.github.com/users/kyri-petrou/orgs",
                    "repos_url": "https://api.github.com/users/kyri-petrou/repos",
                    "events_url": "https://api.github.com/users/kyri-petrou/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/kyri-petrou/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Handling errors allows recovering from defects",
                  "body": "Sigh... Well, this is annoying. When a Cause contains both a failure and a defect (i.e., Fail & Die), failure handling assumes that the Cause does not contain any defects and therefor silently ignores them.\n\nRepro:\n\n```scala\nimport zio.*\n\nobject Foo extends ZIOAppDefault {\n  val dieCause: Cause[String] = Cause.die(new RuntimeException(\"boom\"))\n  val combinedCause = dieCause && Cause.fail(\"boom\")\n\n  def run = ZIO.failCause(combinedCause).catchAll { e =>\n    ZIO.debug(e)\n  } *> ZIO.debug(\"Success\")\n}\n```\nprints:\n```\nhandled: boom\nSuccess\n```\n\nHowever if we substituted `ZIO.failCause(combinedCause)` with `ZIO.failCause(dieCause)`:\n\n```\ntimestamp=2025-05-20T16:31:09.291104Z level=ERROR thread=#zio-fiber-1707930317 message=\"\" cause=\"java.lang.RuntimeException: boom\n\tat Foo$.<clinit>(Foo.scala:4)\n\tat Foo.main(Foo.scala)\n\tat <empty>.Foo.run(Foo.scala:7)\n\tat <empty>.Foo.run(Foo.scala:9)\n\"\n```\n\nWith both of these causes, the outcome should be the same as defects should always be prioritised over failures. This gets even worse when interruption is involved, because the failure handling will be prioritised over it.\n\nHaving said that, this has the potential to massively alter applications so I'm too scared to fix it.\n@ghostdogpr @jdegoes  @hearnadam @guizmaii I summon you all for some wisdom",
                  "html_url": "https://github.com/zio/zio/issues/9874"
                },
                "type": "github"
              },
              "hash": "zio/zio#9874",
              "body": "Sigh... Well, this is annoying. When a Cause contains both a failure and a defect (i.e., Fail & Die), failure handling assumes that the Cause does not contain any defects and therefor silently ignores them.\n\nRepro:\n\n```scala\nimport zio.*\n\nobject Foo extends ZIOAppDefault {\n  val dieCause: Cause[String] = Cause.die(new RuntimeException(\"boom\"))\n  val combinedCause = dieCause && Cause.fail(\"boom\")\n\n  def run = ZIO.failCause(combinedCause).catchAll { e =>\n    ZIO.debug(e)\n  } *> ZIO.debug(\"Success\")\n}\n```\nprints:\n```\nhandled: boom\nSuccess\n```\n\nHowever if we substituted `ZIO.failCause(combinedCause)` with `ZIO.failCause(dieCause)`:\n\n```\ntimestamp=2025-05-20T16:31:09.291104Z level=ERROR thread=#zio-fiber-1707930317 message=\"\" cause=\"java.lang.RuntimeException: boom\n\tat Foo$.<clinit>(Foo.scala:4)\n\tat Foo.main(Foo.scala)\n\tat <empty>.Foo.run(Foo.scala:7)\n\tat <empty>.Foo.run(Foo.scala:9)\n\"\n```\n\nWith both of these causes, the outcome should be the same as defects should always be prioritised over failures. This gets even worse when interruption is involved, because the failure handling will be prioritised over it.\n\nHaving said that, this has the potential to massively alter applications so I'm too scared to fix it.\n@ghostdogpr @jdegoes  @hearnadam @guizmaii I summon you all for some wisdom",
              "url": "https://github.com/zio/zio/issues/9874",
              "tech": [],
              "repo_name": "zio",
              "repo_owner": "zio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#23104",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.149Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.149Z",
            "created_at": "2025-11-09T14:41:11.149Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#23104",
              "status": "open",
              "type": "issue",
              "number": 23104,
              "title": "Local dev crazy slow",
              "source": {
                "data": {
                  "id": "source-cal#23104",
                  "user": {
                    "login": "keithwillcode",
                    "id": 2538462,
                    "node_id": "MDQ6VXNlcjI1Mzg0NjI=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/2538462?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/keithwillcode",
                    "html_url": "https://github.com/keithwillcode",
                    "followers_url": "https://api.github.com/users/keithwillcode/followers",
                    "following_url": "https://api.github.com/users/keithwillcode/following{/other_user}",
                    "gists_url": "https://api.github.com/users/keithwillcode/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/keithwillcode/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/keithwillcode/subscriptions",
                    "organizations_url": "https://api.github.com/users/keithwillcode/orgs",
                    "repos_url": "https://api.github.com/users/keithwillcode/repos",
                    "events_url": "https://api.github.com/users/keithwillcode/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/keithwillcode/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Local dev crazy slow",
                  "body": "$2000 USD Bounty here!\n\nIf you can find a nice solution to our local dev experience, take home a $2000 USD bounty. We have a very strong feeling the culprit is the App Store. The problem is that refactoring it is a challenge since we have 100+ apps, there are tons of circular dependencies and our `main` branch is constantly merged to, making conflicts hard to work around.\n\nTake these PRs as inspiration to ideally not load the entire App Store upon any given page load in our app. If you debug locally and watch what happens in the `.next/server/chunks` folder, you will see it's compiling a ton of code that's not needed.\n\nThese PRs have gotten close but refactoring the test suite to continue passing has been quite a blocker.\n\nhttps://github.com/calcom/cal.com/pull/22450 - EDIT 2025-08-17 This is being brought back to life with the help of @joeauyeung to fix the test suite.\nhttps://github.com/calcom/cal.com/pull/19771\n\nCurrent initial page load times per page even on a beast MacBook Pro can be upwards of 10-12s. **This needs to be cut by at least 80%.**",
                  "html_url": "https://github.com/calcom/cal.com/issues/23104"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#23104",
              "body": "$2000 USD Bounty here!\n\nIf you can find a nice solution to our local dev experience, take home a $2000 USD bounty. We have a very strong feeling the culprit is the App Store. The problem is that refactoring it is a challenge since we have 100+ apps, there are tons of circular dependencies and our `main` branch is constantly merged to, making conflicts hard to work around.\n\nTake these PRs as inspiration to ideally not load the entire App Store upon any given page load in our app. If you debug locally and watch what happens in the `.next/server/chunks` folder, you will see it's compiling a ton of code that's not needed.\n\nThese PRs have gotten close but refactoring the test suite to continue passing has been quite a blocker.\n\nhttps://github.com/calcom/cal.com/pull/22450 - EDIT 2025-08-17 This is being brought back to life with the help of @joeauyeung to fix the test suite.\nhttps://github.com/calcom/cal.com/pull/19771\n\nCurrent initial page load times per page even on a beast MacBook Pro can be upwards of 10-12s. **This needs to be cut by at least 80%.**",
              "url": "https://github.com/calcom/cal.com/issues/23104",
              "tech": [
                "go"
              ],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#9485",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.246Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.246Z",
            "created_at": "2025-11-09T14:41:11.246Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#9485",
              "status": "open",
              "type": "issue",
              "number": 9485,
              "title": "CalDAV integration with Fastmail is generating duplicate, erroneous invitation emails.",
              "source": {
                "data": {
                  "id": "source-cal#9485",
                  "user": {
                    "login": "mtnowl",
                    "id": 11601915,
                    "node_id": "MDQ6VXNlcjExNjAxOTE1",
                    "avatar_url": "https://avatars.githubusercontent.com/u/11601915?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/mtnowl",
                    "html_url": "https://github.com/mtnowl",
                    "followers_url": "https://api.github.com/users/mtnowl/followers",
                    "following_url": "https://api.github.com/users/mtnowl/following{/other_user}",
                    "gists_url": "https://api.github.com/users/mtnowl/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/mtnowl/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/mtnowl/subscriptions",
                    "organizations_url": "https://api.github.com/users/mtnowl/orgs",
                    "repos_url": "https://api.github.com/users/mtnowl/repos",
                    "events_url": "https://api.github.com/users/mtnowl/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/mtnowl/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "CalDAV integration with Fastmail is generating duplicate, erroneous invitation emails.",
                  "body": "### Issue Summary\r\n\r\ncreated from https://github.com/calcom/cal.com/issues/3457#issuecomment-1581465501, but I am also using Fastmail and seeing the same issues. This is making it so I can't use the integration.\r\n\r\nWe also use Fastmail and have been seeing some oddities with the [cal.com](http://cal.com/) integration. Namely, that participants are receiving multiple calendar invites, which on it's own isn't that big of a deal. However, the Fastmail invites are always in UTC, not in the local time of the cal.com invite.\r\n\r\nThis is creating a lot of confusion with participants, they keep missing our meetings, not realizing one invite is UTC and one in local time. I spoke with the Fastmail team and they relayed the following notes about cal's specific integration.\r\n\r\n1. There is a property (SCHEDULE-AGENT=CLIENT) a CalDAV client can add to the event that would prevent CalDAV servers from sending invitations. It appears cal.com is not setting this property even though they are sending an invitation.\r\n2. The invitation being sent by [cal.com](http://cal.com/) is using a different UID. Basically every event has a UID that uniquely identifies the event. Since [cal.com](http://cal.com/) is sending the invitation of the event with a different UID, if you end up adding this event to your calendar [cal.com](http://cal.com/), it will create a duplicate event in your calendar as well. They need to fix it so that all emails associated with a single event use the same original UID.\r\n3. The event added from [cal.com](http://cal.com/) doesn't have a timezone associated with it. Though it mentions the invitees timezone as \"America/Chicago\" in the notes, the event itself doesn't include a timezone and only specifies the time in UTC. Since the event time is specified in UTC, Fastmail's invitation email will also be sent in UTC.\r\n4. There seems to be no way to stop Cal.com from creating the calendar events at all, and thus no real workaround.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Connect to Fastmail using CalDav integration\r\n2. Create a test booking\r\n3. See issues above.\r\n\r\nAny other relevant information. For example, why do you consider this a bug and what did you expect to happen instead?\r\n1. SCHEDULE-AGENT=CLIENT should be set, so Fastmail knows not to send an invitation. **This is the main one.**\r\n2. UID should be unique per event.\r\n3. Event time should have a timezone associated with it.\r\n4. An option could be added to make calendar event creation optional (nice-to-have, not essential).\r\n\r\n### Technical details\r\n\r\nNone other than Fastmail/CalDAV.",
                  "html_url": "https://github.com/calcom/cal.com/issues/9485"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#9485",
              "body": "### Issue Summary\r\n\r\ncreated from https://github.com/calcom/cal.com/issues/3457#issuecomment-1581465501, but I am also using Fastmail and seeing the same issues. This is making it so I can't use the integration.\r\n\r\nWe also use Fastmail and have been seeing some oddities with the [cal.com](http://cal.com/) integration. Namely, that participants are receiving multiple calendar invites, which on it's own isn't that big of a deal. However, the Fastmail invites are always in UTC, not in the local time of the cal.com invite.\r\n\r\nThis is creating a lot of confusion with participants, they keep missing our meetings, not realizing one invite is UTC and one in local time. I spoke with the Fastmail team and they relayed the following notes about cal's specific integration.\r\n\r\n1. There is a property (SCHEDULE-AGENT=CLIENT) a CalDAV client can add to the event that would prevent CalDAV servers from sending invitations. It appears cal.com is not setting this property even though they are sending an invitation.\r\n2. The invitation being sent by [cal.com](http://cal.com/) is using a different UID. Basically every event has a UID that uniquely identifies the event. Since [cal.com](http://cal.com/) is sending the invitation of the event with a different UID, if you end up adding this event to your calendar [cal.com](http://cal.com/), it will create a duplicate event in your calendar as well. They need to fix it so that all emails associated with a single event use the same original UID.\r\n3. The event added from [cal.com](http://cal.com/) doesn't have a timezone associated with it. Though it mentions the invitees timezone as \"America/Chicago\" in the notes, the event itself doesn't include a timezone and only specifies the time in UTC. Since the event time is specified in UTC, Fastmail's invitation email will also be sent in UTC.\r\n4. There seems to be no way to stop Cal.com from creating the calendar events at all, and thus no real workaround.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Connect to Fastmail using CalDav integration\r\n2. Create a test booking\r\n3. See issues above.\r\n\r\nAny other relevant information. For example, why do you consider this a bug and what did you expect to happen instead?\r\n1. SCHEDULE-AGENT=CLIENT should be set, so Fastmail knows not to send an invitation. **This is the main one.**\r\n2. UID should be unique per event.\r\n3. Event time should have a timezone associated with it.\r\n4. An option could be added to make calendar event creation optional (nice-to-have, not essential).\r\n\r\n### Technical details\r\n\r\nNone other than Fastmail/CalDAV.",
              "url": "https://github.com/calcom/cal.com/issues/9485",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#18947",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.343Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.343Z",
            "created_at": "2025-11-09T14:41:11.343Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#18947",
              "status": "open",
              "type": "issue",
              "number": 18947,
              "title": "[CAL-5091] additional settings: \"add team member as optional guest",
              "source": {
                "data": {
                  "id": "source-cal#18947",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-5091] additional settings: \"add team member as optional guest",
                  "body": "![Image](https://github.com/user-attachments/assets/305d292b-a984-44c0-b89f-f8ed979d4c71)\n\n1. team members only (to protect spam) and show `<UpgradeTeamsBadge />`\n2.  don't check the team members calendar for conflict checking, only invite them\n3. if possible mark them as \"optional\":\n\n![Image](https://github.com/user-attachments/assets/7d2a1ec0-6c68-4334-a5ea-a1137279b98f)\n\n<sub>[CAL-5091](https://linear.app/calcom/issue/CAL-5091/additional-settings-add-team-member-as-optional-guest)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/18947"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#18947",
              "body": "![Image](https://github.com/user-attachments/assets/305d292b-a984-44c0-b89f-f8ed979d4c71)\n\n1. team members only (to protect spam) and show `<UpgradeTeamsBadge />`\n2.  don't check the team members calendar for conflict checking, only invite them\n3. if possible mark them as \"optional\":\n\n![Image](https://github.com/user-attachments/assets/7d2a1ec0-6c68-4334-a5ea-a1137279b98f)\n\n<sub>[CAL-5091](https://linear.app/calcom/issue/CAL-5091/additional-settings-add-team-member-as-optional-guest)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/18947",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#18992",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.444Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.444Z",
            "created_at": "2025-11-09T14:41:11.444Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#18992",
              "status": "open",
              "type": "issue",
              "number": 18992,
              "title": "[CAL-5107] add no-show to zapier (its a webhook only right now)",
              "source": {
                "data": {
                  "id": "source-cal#18992",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-5107] add no-show to zapier (its a webhook only right now)",
                  "body": "![Image](https://github.com/user-attachments/assets/d1992193-e964-4382-aff1-ba90d29c0562)\n\n<sub>[CAL-5107](https://linear.app/calcom/issue/CAL-5107/add-no-show-to-zapier-its-a-webhook-only-right-now)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/18992"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#18992",
              "body": "![Image](https://github.com/user-attachments/assets/d1992193-e964-4382-aff1-ba90d29c0562)\n\n<sub>[CAL-5107](https://linear.app/calcom/issue/CAL-5107/add-no-show-to-zapier-its-a-webhook-only-right-now)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/18992",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#18987",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.536Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.536Z",
            "created_at": "2025-11-09T14:41:11.536Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#18987",
              "status": "open",
              "type": "issue",
              "number": 18987,
              "title": "[CAL-5097] add the same \"booking questions\" to routing forms",
              "source": {
                "data": {
                  "id": "source-cal#18987",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-5097] add the same \"booking questions\" to routing forms",
                  "body": "right now \"booking forms\" for event-types have more (and different) inputs that routing forms:\n\n\nevent-types:\n\nhttps://github.com/user-attachments/assets/29c921ea-8677-4238-a198-55be00ce8188\n\nrouting forms:\n\nhttps://github.com/user-attachments/assets/cdaf70e0-7c5b-416b-a984-a57ded4c8525\n\n\nwe should reuse the entire event-type booking question UI inside routing forms.\n\n\nwe should also ask for the identifier **first** and not prefill it. (see event-types)\n\n<sub>[CAL-5097](https://linear.app/calcom/issue/CAL-5097/add-the-same-booking-questions-to-routing-forms)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/18987"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#18987",
              "body": "right now \"booking forms\" for event-types have more (and different) inputs that routing forms:\n\n\nevent-types:\n\nhttps://github.com/user-attachments/assets/29c921ea-8677-4238-a198-55be00ce8188\n\nrouting forms:\n\nhttps://github.com/user-attachments/assets/cdaf70e0-7c5b-416b-a984-a57ded4c8525\n\n\nwe should reuse the entire event-type booking question UI inside routing forms.\n\n\nwe should also ask for the identifier **first** and not prefill it. (see event-types)\n\n<sub>[CAL-5097](https://linear.app/calcom/issue/CAL-5097/add-the-same-booking-questions-to-routing-forms)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/18987",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#16797",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.652Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.652Z",
            "created_at": "2025-11-09T14:41:11.652Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#16797",
              "status": "open",
              "type": "issue",
              "number": 16797,
              "title": "Native Pipedrive integration",
              "source": {
                "data": {
                  "id": "source-cal#16797",
                  "user": {
                    "login": "JakobStadlhuber",
                    "id": 20328243,
                    "node_id": "MDQ6VXNlcjIwMzI4MjQz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/20328243?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/JakobStadlhuber",
                    "html_url": "https://github.com/JakobStadlhuber",
                    "followers_url": "https://api.github.com/users/JakobStadlhuber/followers",
                    "following_url": "https://api.github.com/users/JakobStadlhuber/following{/other_user}",
                    "gists_url": "https://api.github.com/users/JakobStadlhuber/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/JakobStadlhuber/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/JakobStadlhuber/subscriptions",
                    "organizations_url": "https://api.github.com/users/JakobStadlhuber/orgs",
                    "repos_url": "https://api.github.com/users/JakobStadlhuber/repos",
                    "events_url": "https://api.github.com/users/JakobStadlhuber/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/JakobStadlhuber/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Native Pipedrive integration",
                  "body": "### Is your proposal related to a problem?\r\n\r\nYes, the Pipedrive integration in Cal.com is currently not functioning because it relies on Revert.dev, which has been deprecated and no longer maintained since September 2023. When I try to add Pipedrive via the web UI, I receive an error message stating \"pipedrive client id missing.\"\r\n\r\n### Describe the solution you'd like\r\n\r\nI would like the Pipedrive integration to be updated to remove the dependency on Revert.dev. Instead, the integration should directly use Pipedrive's API, implementing OAuth2 authentication. This would involve updating the web UI to allow users to input their Pipedrive Client ID and Client Secret to establish the connection.\r\n\r\n### Describe alternatives you've considered\r\n\r\nI've considered manually integrating Pipedrive using custom API scripts, but this approach is not practical for most users due to its complexity. I also looked into third-party integration tools, but they may not offer the same seamless experience and could incur additional costs.\r\n\r\n### Additional context\r\n\r\nMaintaining a functional Pipedrive integration is crucial for users who rely on it to sync their scheduling with their CRM systems. The deprecation of Revert.dev has disrupted this workflow, impacting productivity. Updating the integration will restore this essential functionality for many users.\r\n\r\n### Requirement/Document\r\n\r\n- [Pipedrive API Authentication Documentation](https://pipedrive.readme.io/docs/core-api-concepts-authentication)\r\n",
                  "html_url": "https://github.com/calcom/cal.com/issues/16797"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#16797",
              "body": "### Is your proposal related to a problem?\r\n\r\nYes, the Pipedrive integration in Cal.com is currently not functioning because it relies on Revert.dev, which has been deprecated and no longer maintained since September 2023. When I try to add Pipedrive via the web UI, I receive an error message stating \"pipedrive client id missing.\"\r\n\r\n### Describe the solution you'd like\r\n\r\nI would like the Pipedrive integration to be updated to remove the dependency on Revert.dev. Instead, the integration should directly use Pipedrive's API, implementing OAuth2 authentication. This would involve updating the web UI to allow users to input their Pipedrive Client ID and Client Secret to establish the connection.\r\n\r\n### Describe alternatives you've considered\r\n\r\nI've considered manually integrating Pipedrive using custom API scripts, but this approach is not practical for most users due to its complexity. I also looked into third-party integration tools, but they may not offer the same seamless experience and could incur additional costs.\r\n\r\n### Additional context\r\n\r\nMaintaining a functional Pipedrive integration is crucial for users who rely on it to sync their scheduling with their CRM systems. The deprecation of Revert.dev has disrupted this workflow, impacting productivity. Updating the integration will restore this essential functionality for many users.\r\n\r\n### Requirement/Document\r\n\r\n- [Pipedrive API Authentication Documentation](https://pipedrive.readme.io/docs/core-api-concepts-authentication)\r\n",
              "url": "https://github.com/calcom/cal.com/issues/16797",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#16651",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.767Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.767Z",
            "created_at": "2025-11-09T14:41:11.767Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#16651",
              "status": "open",
              "type": "issue",
              "number": 16651,
              "title": "[CAL-4716] Missing Notifications for Events",
              "source": {
                "data": {
                  "id": "source-cal#16651",
                  "user": {
                    "login": "tiagovasc",
                    "id": 39215546,
                    "node_id": "MDQ6VXNlcjM5MjE1NTQ2",
                    "avatar_url": "https://avatars.githubusercontent.com/u/39215546?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/tiagovasc",
                    "html_url": "https://github.com/tiagovasc",
                    "followers_url": "https://api.github.com/users/tiagovasc/followers",
                    "following_url": "https://api.github.com/users/tiagovasc/following{/other_user}",
                    "gists_url": "https://api.github.com/users/tiagovasc/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/tiagovasc/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/tiagovasc/subscriptions",
                    "organizations_url": "https://api.github.com/users/tiagovasc/orgs",
                    "repos_url": "https://api.github.com/users/tiagovasc/repos",
                    "events_url": "https://api.github.com/users/tiagovasc/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/tiagovasc/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-4716] Missing Notifications for Events",
                  "body": "I propose integrating a feature in Cal.com that allows users to set default Google Calendar notifications for events booked through the platform. This functionality should enable users to select preferred notification times (e.g., 10 minutes, 30 minutes, 1 hour before the event) that automatically apply to all events created via Cal.com bookings. Currently, the absence of such a feature leads to missed appointments since default notifications set in Google Calendar do not apply to these events. Integrating this feature would enhance user experience by ensuring consistency in how notifications are handled, regardless of how an event is booked.\n\n<sub>[CAL-4716](https://linear.app/calcom/issue/CAL-4716/missing-notifications-for-events)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/16651"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#16651",
              "body": "I propose integrating a feature in Cal.com that allows users to set default Google Calendar notifications for events booked through the platform. This functionality should enable users to select preferred notification times (e.g., 10 minutes, 30 minutes, 1 hour before the event) that automatically apply to all events created via Cal.com bookings. Currently, the absence of such a feature leads to missed appointments since default notifications set in Google Calendar do not apply to these events. Integrating this feature would enhance user experience by ensuring consistency in how notifications are handled, regardless of how an event is booked.\n\n<sub>[CAL-4716](https://linear.app/calcom/issue/CAL-4716/missing-notifications-for-events)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/16651",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#8123",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.867Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.867Z",
            "created_at": "2025-11-09T14:41:11.867Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#8123",
              "status": "open",
              "type": "issue",
              "number": 8123,
              "title": "[CAL-1425] Exchange on Premise 2016",
              "source": {
                "data": {
                  "id": "source-cal#8123",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-1425] Exchange on Premise 2016",
                  "body": " > If I use Exchange 2016 i get the message \"cannot added\". and if i use standard Exchange as Option I get \"unauthorized\". but i checked that the EWS login works fine via URL\r\n\r\n\n\n<sub>[CAL-1425](https://linear.app/calcom/issue/CAL-1425/exchange-on-premise-2016)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/8123"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#8123",
              "body": " > If I use Exchange 2016 i get the message \"cannot added\". and if i use standard Exchange as Option I get \"unauthorized\". but i checked that the EWS login works fine via URL\r\n\r\n\n\n<sub>[CAL-1425](https://linear.app/calcom/issue/CAL-1425/exchange-on-premise-2016)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/8123",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#16378",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:11.986Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:11.986Z",
            "created_at": "2025-11-09T14:41:11.986Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#16378",
              "status": "open",
              "type": "issue",
              "number": 16378,
              "title": "[CAL-4531] Take into account guest's availability when rescheduling",
              "source": {
                "data": {
                  "id": "source-cal#16378",
                  "user": {
                    "login": "pumfleet",
                    "id": 25907159,
                    "node_id": "MDQ6VXNlcjI1OTA3MTU5",
                    "avatar_url": "https://avatars.githubusercontent.com/u/25907159?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/pumfleet",
                    "html_url": "https://github.com/pumfleet",
                    "followers_url": "https://api.github.com/users/pumfleet/followers",
                    "following_url": "https://api.github.com/users/pumfleet/following{/other_user}",
                    "gists_url": "https://api.github.com/users/pumfleet/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/pumfleet/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/pumfleet/subscriptions",
                    "organizations_url": "https://api.github.com/users/pumfleet/orgs",
                    "repos_url": "https://api.github.com/users/pumfleet/repos",
                    "events_url": "https://api.github.com/users/pumfleet/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/pumfleet/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-4531] Take into account guest's availability when rescheduling",
                  "body": "If I (a Cal.com user) book another Cal.com user and they want to reschedule it on their end (instead of requesting me to reschedule), it doesn't take into account my availability and just lets them freely choose a time.\r\n\r\nThis is because the person who booked the meeting may or may not be a Cal.com user, hence we don't bother checking for their availability. But we do have the user's email, so we should look up the user and see if they're a Cal.com user, and then if so, retrieve the available times for them, and only display those when the host tries to reschedule\n\n<sub>[CAL-4531](https://linear.app/calcom/issue/CAL-4531/take-into-account-guests-availability-when-rescheduling)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/16378"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#16378",
              "body": "If I (a Cal.com user) book another Cal.com user and they want to reschedule it on their end (instead of requesting me to reschedule), it doesn't take into account my availability and just lets them freely choose a time.\r\n\r\nThis is because the person who booked the meeting may or may not be a Cal.com user, hence we don't bother checking for their availability. But we do have the user's email, so we should look up the user and see if they're a Cal.com user, and then if so, retrieve the available times for them, and only display those when the host tries to reschedule\n\n<sub>[CAL-4531](https://linear.app/calcom/issue/CAL-4531/take-into-account-guests-availability-when-rescheduling)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/16378",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "cal#13532",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "cal",
              "id": "generated-cal",
              "name": "Cal",
              "description": "",
              "members": [],
              "display_name": "Cal",
              "created_at": "2025-11-09T14:41:12.085Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/cal?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "calcom",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:12.085Z",
            "created_at": "2025-11-09T14:41:12.085Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-cal#13532",
              "status": "open",
              "type": "issue",
              "number": 13532,
              "title": "[CAL-3076] allow emails and invite people to a team event-type directly from \"assignment\" if not in team yet",
              "source": {
                "data": {
                  "id": "source-cal#13532",
                  "user": {
                    "login": "PeerRich",
                    "id": 8019099,
                    "node_id": "MDQ6VXNlcjgwMTkwOTk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8019099?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/PeerRich",
                    "html_url": "https://github.com/PeerRich",
                    "followers_url": "https://api.github.com/users/PeerRich/followers",
                    "following_url": "https://api.github.com/users/PeerRich/following{/other_user}",
                    "gists_url": "https://api.github.com/users/PeerRich/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/PeerRich/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/PeerRich/subscriptions",
                    "organizations_url": "https://api.github.com/users/PeerRich/orgs",
                    "repos_url": "https://api.github.com/users/PeerRich/repos",
                    "events_url": "https://api.github.com/users/PeerRich/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/PeerRich/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[CAL-3076] allow emails and invite people to a team event-type directly from \"assignment\" if not in team yet",
                  "body": "- [ ] allow emails in assignment\n- [ ] invite person if its not in the team yet (as Member)\n- [ ] allow multiple emails comma separated: [user1@example.com](mailto:user1@example.com), [user2@example.com](mailto:user2@example.com)\n\n![CleanShot 2024-02-05 at 10 17 08@2x](https://uploads.linear.app/e86bf957-d82f-465e-b205-135559f4b623/598b18ab-0aed-491d-90f4-cd2afa2f1bbb/24eba054-180f-45cf-b858-4764d896572f?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiL2U4NmJmOTU3LWQ4MmYtNDY1ZS1iMjA1LTEzNTU1OWY0YjYyMy81OThiMThhYi0wYWVkLTQ5MWQtOTBmNC1jZDJhZmEyZjFiYmIvMjRlYmEwNTQtMTgwZi00NWNmLWI4NTgtNDc2NGQ4OTY1NzJmIiwiaWF0IjoxNzA3MTI4MjY3LCJleHAiOjE3MDcyMTQ2Njd9.DQDBQ-o7r2wBUIvnU0ZH2MoSkKZMbzDB9cIHtLUvV4Q)\n\n<sub>From [SyncLinear.com](https://synclinear.com) | [CAL-3076](https://linear.app/calcom/issue/CAL-3076/allow-emails-and-invite-people-to-a-team-event-type-directly-from)</sub>",
                  "html_url": "https://github.com/calcom/cal.com/issues/13532"
                },
                "type": "github"
              },
              "hash": "calcom/cal.com#13532",
              "body": "- [ ] allow emails in assignment\n- [ ] invite person if its not in the team yet (as Member)\n- [ ] allow multiple emails comma separated: [user1@example.com](mailto:user1@example.com), [user2@example.com](mailto:user2@example.com)\n\n![CleanShot 2024-02-05 at 10 17 08@2x](https://uploads.linear.app/e86bf957-d82f-465e-b205-135559f4b623/598b18ab-0aed-491d-90f4-cd2afa2f1bbb/24eba054-180f-45cf-b858-4764d896572f?signature=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJwYXRoIjoiL2U4NmJmOTU3LWQ4MmYtNDY1ZS1iMjA1LTEzNTU1OWY0YjYyMy81OThiMThhYi0wYWVkLTQ5MWQtOTBmNC1jZDJhZmEyZjFiYmIvMjRlYmEwNTQtMTgwZi00NWNmLWI4NTgtNDc2NGQ4OTY1NzJmIiwiaWF0IjoxNzA3MTI4MjY3LCJleHAiOjE3MDcyMTQ2Njd9.DQDBQ-o7r2wBUIvnU0ZH2MoSkKZMbzDB9cIHtLUvV4Q)\n\n<sub>From [SyncLinear.com](https://synclinear.com) | [CAL-3076](https://linear.app/calcom/issue/CAL-3076/allow-emails-and-invite-people-to-a-team-event-type-directly-from)</sub>",
              "url": "https://github.com/calcom/cal.com/issues/13532",
              "tech": [],
              "repo_name": "cal.com",
              "repo_owner": "calcom",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#7113",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.002Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.002Z",
            "created_at": "2025-11-09T14:41:15.002Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#7113",
              "status": "open",
              "type": "issue",
              "number": 7113,
              "title": "[Bug]: Deployments failing without clear cause / error",
              "source": {
                "data": {
                  "id": "source-coollabsio#7113",
                  "user": {
                    "login": "suptejas",
                    "id": 63039748,
                    "node_id": "MDQ6VXNlcjYzMDM5NzQ4",
                    "avatar_url": "https://avatars.githubusercontent.com/u/63039748?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/suptejas",
                    "html_url": "https://github.com/suptejas",
                    "followers_url": "https://api.github.com/users/suptejas/followers",
                    "following_url": "https://api.github.com/users/suptejas/following{/other_user}",
                    "gists_url": "https://api.github.com/users/suptejas/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/suptejas/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/suptejas/subscriptions",
                    "organizations_url": "https://api.github.com/users/suptejas/orgs",
                    "repos_url": "https://api.github.com/users/suptejas/repos",
                    "events_url": "https://api.github.com/users/suptejas/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/suptejas/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Bug]: Deployments failing without clear cause / error",
                  "body": "### Error Message and Logs\n\nOur deployments have been in-deterministically failing with no obvious cause. Here's a few deployment logs on our end that just failed, and would work perfectly with a redeploy.\n\n<details>\n\n```\n2025-Nov-05 10:19:19.363823\nFound a suitable build server (Builder).\n2025-Nov-05 10:19:19.370090\nStarting deployment of dimensionhq/ai:master to Webhooks - 2.\n2025-Nov-05 10:19:19.728111\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 10:19:19.905615\n[CMD]: docker stop --time=30 uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:19.905615\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-05 10:19:19.907955\nError response from daemon: No such container: uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:20.143670\n[CMD]: docker rm -f uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:20.143670\nError response from daemon: No such container: uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:20.365835\n[CMD]: docker run -d --name uw4kosoc84ooc808ww4080ww  --rm -v /root/.docker/config.json:/root/.docker/config.json:ro -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 10:19:20.365835\n0b60882c666a468473fc10c4a2350c55587bb7cd4cb62e200bba037838f453e8\n2025-Nov-05 10:19:22.053986\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'GIT_SSH_COMMAND=\"ssh -o ConnectTimeout=30 -p 22 -o Port=22 -o LogLevel=ERROR -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" git ls-remote [https://x-access-token:<REDACTED>@github.com/dimensionhq/ai.git](https://x-access-token:%3CREDACTED%3E@github.com/dimensionhq/ai.git) refs/heads/master'\n2025-Nov-05 10:19:22.053986\neb20a0771f925bf19364784030b4df74a55441f5\trefs/heads/master\n2025-Nov-05 10:19:22.188928\n----------------------------------------\n2025-Nov-05 10:19:22.194547\nImporting dimensionhq/ai:master (commit sha eb20a0771f925bf19364784030b4df74a55441f5) to /artifacts/uw4kosoc84ooc808ww4080ww.\n2025-Nov-05 10:19:22.451630\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'git clone --depth=1 --recurse-submodules --shallow-submodules -b 'master' '[https://x-access-token:<REDACTED>@github.com/dimensionhq/ai.git'](https://x-access-token:%3CREDACTED%3E@github.com/dimensionhq/ai.git') '/artifacts/uw4kosoc84ooc808ww4080ww' && cd '/artifacts/uw4kosoc84ooc808ww4080ww' && if [ -f .gitmodules ]; then git submodule sync && GIT_SSH_COMMAND=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" git submodule update --init --recursive --depth=1; fi && cd '/artifacts/uw4kosoc84ooc808ww4080ww' && GIT_SSH_COMMAND=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" git lfs pull'\n2025-Nov-05 10:19:22.451630\nCloning into '/artifacts/uw4kosoc84ooc808ww4080ww'...\n2025-Nov-05 10:19:24.006780\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cd /artifacts/uw4kosoc84ooc808ww4080ww && git log -1 eb20a0771f925bf19364784030b4df74a55441f5 --pretty=%B'\n2025-Nov-05 10:19:24.006780\nMerge branch 'master' of https://github.com/dimensionhq/ai\n2025-Nov-05 10:19:24.769603\nImage not found (realmsoftwareinc/webhooks-prod:eb20a0771f925bf19364784030b4df74a55441f5). Building new image.\n2025-Nov-05 10:19:26.046158\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile'\n2025-Nov-05 10:19:26.046158\n# Build dependencies and virtualenv in a dedicated stage\n2025-Nov-05 10:19:26.046158\nFROM python:3.11-slim AS builder\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\nWORKDIR /app\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Install build tooling and uv (no recommends to keep layer slim)\n2025-Nov-05 10:19:26.046158\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n2025-Nov-05 10:19:26.046158\ngcc \\\n2025-Nov-05 10:19:26.046158\ng++ \\\n2025-Nov-05 10:19:26.046158\ncurl \\\n2025-Nov-05 10:19:26.046158\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\nRUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Copy workspace files needed for dependency resolution\n2025-Nov-05 10:19:26.046158\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:26.046158\nCOPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:26.046158\nCOPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Install Python dependencies into .venv\n2025-Nov-05 10:19:26.046158\nRUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Clean up build artefacts to keep the copied layer trim\n2025-Nov-05 10:19:26.046158\nRUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Final runtime image\n2025-Nov-05 10:19:26.046158\nFROM python:3.11-slim\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\nWORKDIR /app\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Install curl (for health check) and Infisical CLI with minimal extras\n2025-Nov-05 10:19:26.046158\nRUN apt-get update && apt-get install -y --no-install-recommends curl \\\n2025-Nov-05 10:19:26.046158\n&& curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash \\\n2025-Nov-05 10:19:26.046158\n&& apt-get install -y --no-install-recommends infisical \\\n2025-Nov-05 10:19:26.046158\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Copy in the prebuilt virtualenv and application code\n2025-Nov-05 10:19:26.046158\nCOPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:26.046158\nCOPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:26.046158\nCOPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:26.046158\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Expose port 8000\n2025-Nov-05 10:19:26.046158\nEXPOSE 8000\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Set environment variables\n2025-Nov-05 10:19:26.046158\nENV PYTHONPATH=/app\n2025-Nov-05 10:19:26.046158\nENV PYTHONUNBUFFERED=1\n2025-Nov-05 10:19:26.046158\nENV PATH=\"/app/.venv/bin:$PATH\"\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Health check\n2025-Nov-05 10:19:26.046158\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n2025-Nov-05 10:19:26.046158\nCMD curl -f http://localhost:8000/health || exit 1\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Run the application with Infisical and Gunicorn\n2025-Nov-05 10:19:26.046158\n# Note: Set INFISICAL_TOKEN and INFISICAL_PROJECT_ID environment variables when running the container\n2025-Nov-05 10:19:26.046158\n# Example: docker run --env INFISICAL_TOKEN=$INFISICAL_TOKEN --env INFISICAL_PROJECT_ID=your-project-id your-image\n2025-Nov-05 10:19:26.046158\nCMD [\"sh\", \"-c\", \"infisical run --projectId $INFISICAL_PROJECT_ID --command 'cd /app/apps/webhooks/webhooks && /app/.venv/bin/gunicorn main:app -c gunicorn.conf.py'\"]\n2025-Nov-05 10:19:26.382688\nCreating build-time .env file in /artifacts (outside Docker context).\n2025-Nov-05 10:19:26.930010\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/build-time.env'\n2025-Nov-05 10:19:26.930010\nSOURCE_COMMIT='eb20a0771f925bf19364784030b4df74a55441f5'\n2025-Nov-05 10:19:26.930010\nCOOLIFY_URL=''\n2025-Nov-05 10:19:26.930010\nCOOLIFY_FQDN=''\n2025-Nov-05 10:19:26.930010\nCOOLIFY_BRANCH='master'\n2025-Nov-05 10:19:26.930010\nCOOLIFY_RESOURCE_UUID='b0808cwg8c4ko4kk84kkcks0'\n2025-Nov-05 10:19:26.930010\nCOOLIFY_CONTAINER_NAME='b0808cwg8c4ko4kk84kkcks0-101917583267'\n2025-Nov-05 10:19:26.930010\nINFISICAL_PROJECT_ID=\"65a15447c63fc2a03460007d\"\n2025-Nov-05 10:19:26.930010\nINFISICAL_TOKEN=\"st.885a8fa6-7c1c-47bb-8bfb-715555b46050.b9107cb09ef5069f89ac2720ab9c424a.1bd120fcdff9cbbab6c6c0e1f7678875\"\n2025-Nov-05 10:19:27.264299\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile'\n2025-Nov-05 10:19:27.264299\n# Build dependencies and virtualenv in a dedicated stage\n2025-Nov-05 10:19:27.264299\nFROM python:3.11-slim AS builder\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\nWORKDIR /app\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Install build tooling and uv (no recommends to keep layer slim)\n2025-Nov-05 10:19:27.264299\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n2025-Nov-05 10:19:27.264299\ngcc \\\n2025-Nov-05 10:19:27.264299\ng++ \\\n2025-Nov-05 10:19:27.264299\ncurl \\\n2025-Nov-05 10:19:27.264299\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\nRUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Copy workspace files needed for dependency resolution\n2025-Nov-05 10:19:27.264299\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.264299\nCOPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.264299\nCOPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Install Python dependencies into .venv\n2025-Nov-05 10:19:27.264299\nRUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Clean up build artefacts to keep the copied layer trim\n2025-Nov-05 10:19:27.264299\nRUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Final runtime image\n2025-Nov-05 10:19:27.264299\nFROM python:3.11-slim\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\nWORKDIR /app\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Install curl (for health check) and Infisical CLI with minimal extras\n2025-Nov-05 10:19:27.264299\nRUN apt-get update && apt-get install -y --no-install-recommends curl \\\n2025-Nov-05 10:19:27.264299\n&& curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash \\\n2025-Nov-05 10:19:27.264299\n&& apt-get install -y --no-install-recommends infisical \\\n2025-Nov-05 10:19:27.264299\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Copy in the prebuilt virtualenv and application code\n2025-Nov-05 10:19:27.264299\nCOPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:27.264299\nCOPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.264299\nCOPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.264299\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Expose port 8000\n2025-Nov-05 10:19:27.264299\nEXPOSE 8000\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Set environment variables\n2025-Nov-05 10:19:27.264299\nENV PYTHONPATH=/app\n2025-Nov-05 10:19:27.264299\nENV PYTHONUNBUFFERED=1\n2025-Nov-05 10:19:27.264299\nENV PATH=\"/app/.venv/bin:$PATH\"\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Health check\n2025-Nov-05 10:19:27.264299\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n2025-Nov-05 10:19:27.264299\nCMD curl -f http://localhost:8000/health || exit 1\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Run the application with Infisical and Gunicorn\n2025-Nov-05 10:19:27.264299\n# Note: Set INFISICAL_TOKEN and INFISICAL_PROJECT_ID environment variables when running the container\n2025-Nov-05 10:19:27.264299\n# Example: docker run --env INFISICAL_TOKEN=$INFISICAL_TOKEN --env INFISICAL_PROJECT_ID=your-project-id your-image\n2025-Nov-05 10:19:27.264299\nCMD [\"sh\", \"-c\", \"infisical run --projectId $INFISICAL_PROJECT_ID --command 'cd /app/apps/webhooks/webhooks && /app/.venv/bin/gunicorn main:app -c gunicorn.conf.py'\"]\n2025-Nov-05 10:19:27.306071\nFinal Dockerfile:\n2025-Nov-05 10:19:27.928385\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile'\n2025-Nov-05 10:19:27.928385\n# Build dependencies and virtualenv in a dedicated stage\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_BUILD_SECRETS_HASH=4387256370ab25c71c6c7f93ce46e2a426fafb4ba429afe60f75c57a2cc75894\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_CONTAINER_NAME=b0808cwg8c4ko4kk84kkcks0-101917583267\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_RESOURCE_UUID=b0808cwg8c4ko4kk84kkcks0\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_BRANCH=master\n2025-Nov-05 10:19:27.928385\nARG SOURCE_COMMIT=eb20a0771f925bf19364784030b4df74a55441f5\n2025-Nov-05 10:19:27.928385\nARG INFISICAL_TOKEN=st.885a8fa6-7c1c-47bb-8bfb-715555b46050.b9107cb09ef5069f89ac2720ab9c424a.1bd120fcdff9cbbab6c6c0e1f7678875\n2025-Nov-05 10:19:27.928385\nARG INFISICAL_PROJECT_ID=65a15447c63fc2a03460007d\n2025-Nov-05 10:19:27.928385\nFROM python:3.11-slim AS builder\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\nWORKDIR /app\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Install build tooling and uv (no recommends to keep layer slim)\n2025-Nov-05 10:19:27.928385\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n2025-Nov-05 10:19:27.928385\ngcc \\\n2025-Nov-05 10:19:27.928385\ng++ \\\n2025-Nov-05 10:19:27.928385\ncurl \\\n2025-Nov-05 10:19:27.928385\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\nRUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Copy workspace files needed for dependency resolution\n2025-Nov-05 10:19:27.928385\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.928385\nCOPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.928385\nCOPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Install Python dependencies into .venv\n2025-Nov-05 10:19:27.928385\nRUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Clean up build artefacts to keep the copied layer trim\n2025-Nov-05 10:19:27.928385\nRUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Final runtime image\n2025-Nov-05 10:19:27.928385\nFROM python:3.11-slim\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\nWORKDIR /app\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Install curl (for health check) and Infisical CLI with minimal extras\n2025-Nov-05 10:19:27.928385\nRUN apt-get update && apt-get install -y --no-install-recommends curl \\\n2025-Nov-05 10:19:27.928385\n&& curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash \\\n2025-Nov-05 10:19:27.928385\n&& apt-get install -y --no-install-recommends infisical \\\n2025-Nov-05 10:19:27.928385\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Copy in the prebuilt virtualenv and application code\n2025-Nov-05 10:19:27.928385\nCOPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:27.928385\nCOPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.928385\nCOPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.928385\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Expose port 8000\n2025-Nov-05 10:19:27.928385\nEXPOSE 8000\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Set environment variables\n2025-Nov-05 10:19:27.928385\nENV PYTHONPATH=/app\n2025-Nov-05 10:19:27.928385\nENV PYTHONUNBUFFERED=1\n2025-Nov-05 10:19:27.928385\nENV PATH=\"/app/.venv/bin:$PATH\"\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Health check\n2025-Nov-05 10:19:27.928385\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n2025-Nov-05 10:19:27.928385\nCMD curl -f http://localhost:8000/health || exit 1\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Run the application with Infisical and Gunicorn\n2025-Nov-05 10:19:27.928385\n# Note: Set INFISICAL_TOKEN and INFISICAL_PROJECT_ID environment variables when running the container\n2025-Nov-05 10:19:27.928385\n# Example: docker run --env INFISICAL_TOKEN=$INFISICAL_TOKEN --env INFISICAL_PROJECT_ID=your-project-id your-image\n2025-Nov-05 10:19:27.928385\nCMD [\"sh\", \"-c\", \"infisical run --projectId $INFISICAL_PROJECT_ID --command 'cd /app/apps/webhooks/webhooks && /app/.venv/bin/gunicorn main:app -c gunicorn.conf.py'\"]\n2025-Nov-05 10:19:27.942703\n----------------------------------------\n2025-Nov-05 10:19:27.949099\nBuilding docker image started.\n2025-Nov-05 10:19:27.957779\nTo check the current progress, click on Show Debug Logs.\n2025-Nov-05 10:19:28.573409\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/build.sh'\n2025-Nov-05 10:19:28.573409\ncd /artifacts/uw4kosoc84ooc808ww4080ww && set -a && source /artifacts/build-time.env && set +a && docker build   --network host -f /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile --build-arg SOURCE_COMMIT --build-arg COOLIFY_URL --build-arg COOLIFY_FQDN --build-arg COOLIFY_BRANCH --build-arg COOLIFY_RESOURCE_UUID --build-arg COOLIFY_CONTAINER_NAME --build-arg INFISICAL_PROJECT_ID --build-arg INFISICAL_TOKEN --build-arg COOLIFY_BUILD_SECRETS_HASH=636980d03d37af4b459b987881db690134f9b90ab8d67d21c1fc8e99e0f83d41 --build-arg 'SOURCE_COMMIT' --build-arg 'COOLIFY_URL' --build-arg 'COOLIFY_FQDN' --build-arg 'COOLIFY_BRANCH' --build-arg 'COOLIFY_RESOURCE_UUID' --build-arg 'COOLIFY_CONTAINER_NAME' --progress plain -t realmsoftwareinc/webhooks-prod:eb20a0771f925bf19364784030b4df74a55441f5 /artifacts/uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:29.156678\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'bash /artifacts/build.sh'\n2025-Nov-05 10:19:29.156678\n#0 building with \"default\" instance using docker driver\n2025-Nov-05 10:19:29.156678\n2025-Nov-05 10:19:29.156678\n#1 [internal] load build definition from Dockerfile\n2025-Nov-05 10:19:29.156678\n#1 transferring dockerfile: 2.52kB done\n2025-Nov-05 10:19:29.156678\n#1 DONE 0.0s\n2025-Nov-05 10:19:29.156678\n2025-Nov-05 10:19:29.156678\n#2 [auth] library/python:pull token for registry-1.docker.io\n2025-Nov-05 10:19:29.156678\n#2 DONE 0.0s\n2025-Nov-05 10:19:29.156678\n2025-Nov-05 10:19:29.156678\n#3 [internal] load metadata for docker.io/library/python:3.11-slim\n2025-Nov-05 10:19:29.156678\n#3 DONE 0.1s\n2025-Nov-05 10:19:29.262918\n#4 [builder 1/9] FROM docker.io/library/python:3.11-slim@sha256:fa9b525a0be0c5ae5e6f2209f4be6fdc5a15a36fed0222144d98ac0d08f876d4\n2025-Nov-05 10:19:29.262918\n#4 DONE 0.0s\n2025-Nov-05 10:19:29.262918\n2025-Nov-05 10:19:29.262918\n#5 [internal] load .dockerignore\n2025-Nov-05 10:19:29.262918\n#5 transferring context: 2B done\n2025-Nov-05 10:19:29.262918\n#5 DONE 0.0s\n2025-Nov-05 10:19:29.262918\n2025-Nov-05 10:19:29.262918\n#6 [internal] load build context\n2025-Nov-05 10:19:29.262918\n#6 transferring context: 1.44MB 0.1s done\n2025-Nov-05 10:19:29.510044\n#6 DONE 0.1s\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#7 [builder 6/9] COPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:29.510044\n#7 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#8 [builder 2/9] WORKDIR /app\n2025-Nov-05 10:19:29.510044\n#8 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#9 [builder 4/9] RUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:29.510044\n#9 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#10 [builder 3/9] RUN apt-get update && apt-get install -y --no-install-recommends   gcc   g++   curl   && rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:29.510044\n#10 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#11 [builder 5/9] COPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:29.510044\n#11 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#12 [builder 6/9] COPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:29.510044\n#12 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#13 [builder 7/9] COPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:29.510044\n#13 DONE 0.1s\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#14 [builder 8/9] RUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:29.594969\n#14 0.238 Using CPython 3.11.14 interpreter at: /usr/local/bin/python3\n2025-Nov-05 10:19:29.594969\n#14 0.238 Creating virtual environment at: .venv\n2025-Nov-05 10:19:29.749236\n#14 0.305    Building webhooks @ file:///app/apps/webhooks\n2025-Nov-05 10:19:29.749236\n#14 0.309    Building shared @ file:///app/packages/shared\n2025-Nov-05 10:19:29.749236\n#14 0.392 Downloading aiohttp (1.7MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.392 Downloading psycopg-binary (4.2MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.393 Downloading pynacl (1.3MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.394 Downloading sqlalchemy (3.2MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.396 Downloading asyncpg (3.0MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.397 Downloading pydantic-core (2.0MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.398 Downloading cryptography (4.3MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.399 Downloading virtualenv (5.7MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading ruff (12.8MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading uvloop (3.8MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading pygments (1.2MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading grpcio (6.2MiB)\n2025-Nov-05 10:19:30.555268\n#14 1.199    Building nats-py==2.11.0\n2025-Nov-05 10:19:31.339158\n#14 1.983  Downloading pydantic-core\n2025-Nov-05 10:19:31.491550\n#14 2.034    Building tonyg-rfc3339==0.1\n2025-Nov-05 10:19:31.491550\n#14 2.133    Building nkeys==0.2.1\n2025-Nov-05 10:19:31.598344\n#14 2.147  Downloading pynacl\n2025-Nov-05 10:19:31.602095\n#14 2.241  Downloading aiohttp\n2025-Nov-05 10:19:31.763871\n#14 2.407  Downloading asyncpg\n2025-Nov-05 10:19:31.999697\n#14 2.643  Downloading psycopg-binary\n2025-Nov-05 10:19:32.105619\n#14 2.749  Downloading virtualenv\n2025-Nov-05 10:19:32.252014\n#14 2.755  Downloading ruff\n2025-Nov-05 10:19:32.252014\n#14 2.818  Downloading uvloop\n2025-Nov-05 10:19:32.252014\n#14 2.890  Downloading grpcio\n2025-Nov-05 10:19:32.489971\n#14 2.929  Downloading sqlalchemy\n2025-Nov-05 10:19:32.489971\n#14 2.943  Downloading cryptography\n2025-Nov-05 10:19:32.489971\n#14 2.983  Downloading pygments\n2025-Nov-05 10:19:35.169371\n#14 5.813       Built nkeys==0.2.1\n2025-Nov-05 10:19:35.410078\n#14 6.052       Built nats-py==2.11.0\n2025-Nov-05 10:19:35.576134\n#14 6.084       Built shared @ file:///app/packages/shared\n2025-Nov-05 10:19:35.576134\n#14 6.218       Built webhooks @ file:///app/apps/webhooks\n2025-Nov-05 10:19:35.576134\n#14 6.218       Built tonyg-rfc3339==0.1\n2025-Nov-05 10:19:35.729658\n#14 6.221 Prepared 115 packages in 5.94s\n2025-Nov-05 10:19:35.793532\n#14 6.435 Installed 115 packages in 212ms\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiofiles==25.1.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiogoogle==5.17.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiohappyeyeballs==2.6.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiohttp==3.13.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiosignal==1.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + annotated-types==0.7.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + anyio==4.11.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + apscheduler==3.11.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + async-timeout==5.0.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + asyncpg==0.30.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + attrs==25.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cachetools==6.2.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + certifi==2025.10.5\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cffi==2.0.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cfgv==3.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + charset-normalizer==3.4.4\n2025-Nov-05 10:19:35.793532\n#14 6.435  + click==8.3.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cryptography==46.0.2\n2025-Nov-05 10:19:35.793532\n#14 6.435  + distlib==0.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + distro==1.9.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + dnspython==2.8.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + email-validator==2.3.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + fastapi==0.119.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + fastapi-cli==0.0.13\n2025-Nov-05 10:19:35.793532\n#14 6.435  + fastapi-cloud-cli==0.3.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + filelock==3.20.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + frozenlist==1.8.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-api-core==2.26.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-auth==2.41.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-cloud-kms==3.6.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-cloud-pubsub==2.31.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + googleapis-common-protos==1.70.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + greenlet\n2025-Nov-05 10:19:35.950300\n==3.2.4\n2025-Nov-05 10:19:35.950300\n#14 6.435  + grpc-google-iam-v1==0.14.2\n2025-Nov-05 10:19:35.950300\n#14 6.435  + grpcio==1.75.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + grpcio-status==1.75.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + gunicorn==23.0.0\n2025-Nov-05 10:19:35.950300\n#14 6.435  + h11==0.16.0\n2025-Nov-05 10:19:35.950300\n#14 6.435  + httpcore==1.0.9\n2025-Nov-05 10:19:35.950300\n#14 6.435  + httptools==0.7.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + httpx==0.28.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + identify==2.6.15\n2025-Nov-05 10:19:35.950300\n#14 6.435  + idna==3.11\n2025-Nov-05 10:19:35.950300\n#14 6.439  + importlib-metadata==8.7.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + iniconfig==2.1.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + jinja2==3.1.6\n2025-Nov-05 10:19:35.950300\n#14 6.439  + jiter==0.11.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + markdown-it-py==4.0.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + markupsafe==3.0.3\n2025-Nov-05 10:19:35.950300\n#14 6.439  + mdurl==0.1.2\n2025-Nov-05 10:19:35.950300\n#14 6.440  + multidict==6.7.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + nats-py==2.11.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + nkeys==0.2.1\n2025-Nov-05 10:19:35.950300\n#14 6.440  + nodeenv==1.9.1\n2025-Nov-05 10:19:35.950300\n#14 6.440  + openai==1.109.1\n2025-Nov-05 10:19:35.950300\n#14 6.440  + opentelemetry-api==1.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + opentelemetry-sdk==1.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + opentelemetry-semantic-conventions==0.58b0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + packaging==25.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + platformdirs==4.5.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + pluggy==1.6.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + pre-commit==4.3.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + propcache==0.4.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + proto-plus==1.26.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + protobuf==6.32.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + psycopg==3.2.10\n2025-Nov-05 10:19:35.950300\n#14 6.441  + psycopg-binary==3.2.10\n2025-Nov-05 10:19:35.950300\n#14 6.441  + psycopg-pool==3.2.6\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pyasn1==0.6.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pyasn1-modules==0.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pybase64==1.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pycparser==2.23\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pydantic==2.12.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pydantic-core==2.41.3\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pydantic-settings==2.11.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pygments==2.19.2\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pynacl==1.6.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pytest==8.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.442  + python-dateutil==2.9.0.post0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + python-dotenv==1.1.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + python-multipart==0.0.20\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pyyaml==6.0.3\n2025-Nov-05 10:19:35.950300\n#14 6.442  + redis==6.4.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + requests==2.32.5\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rich==14.2.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rich-toolkit==0.15.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rignore==0.7.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rsa==4.9.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + ruff==0.14.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + sentry-sdk==2.41.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + shared==0.1.0 (from file:///app/packages/shared)\n2025-Nov-05 10:19:35.950300\n#14 6.443  + shellingham==1.5.4\n2025-Nov-05 10:19:35.950300\n#14 6.443  + six==1.17.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + slack-sdk==3.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + sniffio==1.3.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + sqlalchemy==2.0.44\n2025-Nov-05 10:19:35.950300\n#14 6.443  + starlette==0.48.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + structlog==25.4.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + tonyg-rfc3339==0.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + tqdm==4.67.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + turbopuffer==1.3.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + typer==0.19.2\n2025-Nov-05 10:19:35.950300\n#14 6.443  + typing-extensions==4.15.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + typing-inspection==0.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.443  + tzlocal==5.3.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + urllib3==2.5.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + uvicorn==0.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + uvloop==0.21.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + virtualenv==20.35.3\n2025-Nov-05 10:19:35.950300\n#14 6.443  + watchfiles==1.1.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + webhooks==0.1.0 (from file:///app/apps/webhooks)\n2025-Nov-05 10:19:35.950300\n#14 6.443  + websockets==15.0.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + yarl==1.22.0\n2025-Nov-05 10:19:35.950300\n#14 6.444  + zep-cloud==3.5.0\n2025-Nov-05 10:19:35.950300\n#14 6.444  + zipp==3.23.0\n2025-Nov-05 10:19:37.586385\n#14 DONE 8.2s\n2025-Nov-05 10:19:37.746014\n#15 [builder 9/9] RUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:44.770014\n#15 DONE 7.2s\n2025-Nov-05 10:19:50.259887\n#16 [stage-1 3/7] RUN apt-get update && apt-get install -y --no-install-recommends curl   && curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash   && apt-get install -y --no-install-recommends infisical   && rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:50.412590\n#16 CACHED\n2025-Nov-05 10:19:50.412590\n2025-Nov-05 10:19:50.412590\n#17 [stage-1 4/7] COPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:54.030020\n#17 DONE 3.8s\n2025-Nov-05 10:19:54.139159\n#18 [stage-1 5/7] COPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:54.139159\n#18 DONE 0.1s\n2025-Nov-05 10:19:54.250116\n#19 [stage-1 6/7] COPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:54.250116\n#19 DONE 0.1s\n2025-Nov-05 10:19:54.405723\n#20 [stage-1 7/7] COPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:54.455673\n#20 DONE 0.2s\n2025-Nov-05 10:19:54.645685\n#21 exporting to image\n2025-Nov-05 10:19:54.645685\n#21 exporting layers\n2025-Nov-05 10:19:56.973033\n#21 exporting layers 2.5s done\n2025-Nov-05 10:19:57.087921\n#21 writing image sha256:d284e7f248539ffb86adae9e166d03cd29425ac74c5bbe5691a0aa3219dfe2eb done\n2025-Nov-05 10:19:57.087921\n#21 naming to docker.io/realmsoftwareinc/webhooks-prod:eb20a0771f925bf19364784030b4df74a55441f5 done\n2025-Nov-05 10:19:57.087921\n#21 DONE 2.5s\n2025-Nov-05 10:19:57.140786\nBuilding docker image completed.\n2025-Nov-05 10:19:57.156075\nCreating .env file with runtime variables for build phase.\n2025-Nov-05 10:19:57.691747\nOops something is not okay, are you okay? \n2025-Nov-05 10:19:57.701848\nDeployment failed. Removing the new version of your application.\n2025-Nov-05 10:19:57.952779\nGracefully shutting down build container: uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.199636\n[CMD]: docker stop --time=30 uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.199636\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-05 10:19:58.603460\nuw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.857150\n[CMD]: docker rm -f uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.857150\nError response from daemon: No such container: uw4kosoc84ooc808ww4080ww\n```\n\nOr, another build that failed for no apparent reason, this time on the builder:\n\n```\n2025-Nov-04 21:11:16.151285\nFound a suitable build server (Builder).\n2025-Nov-04 21:11:16.162132\nStarting deployment of dimensionhq/ai:master to Webhooks - 2.\n2025-Nov-04 21:11:16.500066\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-04 21:11:16.703351\n[CMD]: docker stop --time=30 b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:16.703351\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-04 21:11:16.705523\nError response from daemon: No such container: b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:16.890495\n[CMD]: docker rm -f b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:16.890495\nError response from daemon: No such container: b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:17.130715\n[CMD]: docker run -d --name b0o844sgkwkow48oc4gc8cgk  --rm -v /root/.docker/config.json:/root/.docker/config.json:ro -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-04 21:11:17.130715\n702832ce4fbd3a6abf2976103a23fec9c9d5642de8d7fbbe920ed7645cc74ea9\n2025-Nov-04 21:11:18.542726\nOops something is not okay, are you okay? \n2025-Nov-04 21:11:18.550276\nDeployment failed. Removing the new version of your application.\n2025-Nov-04 21:11:18.736879\nGracefully shutting down build container: b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:18.923578\n[CMD]: docker stop --time=30 b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:18.923578\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-04 21:11:19.088910\nb0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:19.297149\n[CMD]: docker rm -f b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:19.297149\nError response from daemon: No such container: b0o844sgkwkow48oc4gc8cgk\n```\n\nFinal example:\n```\n2025-Nov-05 02:32:59.297151\nFound a suitable build server (Builder).\n2025-Nov-05 02:32:59.306247\nStarting deployment of dimensionhq/gmail-mcp-server:master to MCP - 2.\n2025-Nov-05 02:32:59.721620\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 02:33:02.138241\n----------------------------------------\n2025-Nov-05 02:33:02.147524\nImporting dimensionhq/gmail-mcp-server:master (commit sha 0062e28d4ef88173db8c0a9e33eca23b8b8c75d6) to /artifacts/i8k04s4scgkcok0wwwswoscs.\n2025-Nov-05 02:33:04.576587\nImage not found (realmsoftwareinc/gmail-mcp-server-production:0062e28d4ef88173db8c0a9e33eca23b8b8c75d6). Building new image.\n2025-Nov-05 02:33:07.614573\n----------------------------------------\n2025-Nov-05 02:33:07.624301\nBuilding docker image started.\n2025-Nov-05 02:33:07.633300\nTo check the current progress, click on Show Debug Logs.\n2025-Nov-05 02:33:51.372999\nBuilding docker image completed.\n2025-Nov-05 02:33:52.638262\n----------------------------------------\n2025-Nov-05 02:33:52.649146\nPushing image to docker registry (realmsoftwareinc/gmail-mcp-server-production:0062e28d4ef88173db8c0a9e33eca23b8b8c75d6).\n2025-Nov-05 02:34:14.362435\nTagging and pushing image with latest tag.\n2025-Nov-05 02:34:17.144804\nFound a suitable build server (Builder).\n2025-Nov-05 02:34:17.155960\nStarting deployment of dimensionhq/gmail-mcp-server:master to MCP - 2.\n2025-Nov-05 02:34:17.526919\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 02:34:18.291348\n----------------------------------------\n2025-Nov-05 02:34:18.300366\nRolling update started.\n2025-Nov-05 02:34:34.775583\n----------------------------------------\n2025-Nov-05 02:34:34.792699\nRolling update started.\n2025-Nov-05 02:34:38.307388\nNew container started.\n2025-Nov-05 02:34:38.318536\nNew container started.\n2025-Nov-05 02:34:38.328167\nCustom healthcheck found in Dockerfile.\n2025-Nov-05 02:34:38.337170\nWaiting for healthcheck to pass on the new container.\n2025-Nov-05 02:34:38.346321\nWaiting for the start period (5 seconds) before starting healthcheck.\n2025-Nov-05 02:34:43.892990\nAttempt 1 of 3 | Healthcheck status: \"healthy\"\n2025-Nov-05 02:34:43.910443\nHealthcheck logs:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n2025-Nov-05 02:34:43.910443\nDload  Upload   Total   Spent    Left  Speed\n2025-Nov-05 02:34:43.910443\n0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100    64  100    64    0     0   4002      0 --:--:-- --:--:-- --:--:--  4266\n2025-Nov-05 02:34:43.910443\n{\"status\":\"ok\",\"transport\":\"streamable-http\",\"mode\":\"stateless\"} | Return code: 0\n2025-Nov-05 02:34:43.929915\nNew container is healthy.\n2025-Nov-05 02:34:43.945834\nRemoving old containers.\n2025-Nov-05 02:34:46.188552\nRolling update completed.\n2025-Nov-05 02:34:46.260320\nGracefully shutting down build container: i8k04s4scgkcok0wwwswoscs\n2025-Nov-05 02:35:14.238820\nOops something is not okay, are you okay? \n2025-Nov-05 02:35:14.250226\nError: No such object: k00cg8gccos8s888cgo4w804-023024867976\n2025-Nov-05 02:35:14.261788\nDeployment failed. Removing the new version of your application.\n2025-Nov-05 02:35:14.553878\nGracefully shutting down build container: i8k04s4scgkcok0wwwswoscs\n```\n\n</details>\n\n### Steps to Reproduce\n\nI'm not certain how to reproduce this issue, but I would appreciate guidance on how to look deeper into the logs or inside Coolify to see what actually went wrong.\n\nWe setup a Builder instance with 6 vCPU & 16 GB of ram, with a maximum of 2 concurrent deployments. This issue occurs across several different resources and several different Docker images.\n\n### Example Repository URL\n\n_No response_\n\n### Coolify Version\n\nv4.0.0-beta.441\n\n### Are you using Coolify Cloud?\n\nNo (self-hosted)\n\n### Operating System and Version (self-hosted)\n\nUbuntu 24.04\n\n### Additional Information\n\n_No response_",
                  "html_url": "https://github.com/coollabsio/coolify/issues/7113"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#7113",
              "body": "### Error Message and Logs\n\nOur deployments have been in-deterministically failing with no obvious cause. Here's a few deployment logs on our end that just failed, and would work perfectly with a redeploy.\n\n<details>\n\n```\n2025-Nov-05 10:19:19.363823\nFound a suitable build server (Builder).\n2025-Nov-05 10:19:19.370090\nStarting deployment of dimensionhq/ai:master to Webhooks - 2.\n2025-Nov-05 10:19:19.728111\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 10:19:19.905615\n[CMD]: docker stop --time=30 uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:19.905615\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-05 10:19:19.907955\nError response from daemon: No such container: uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:20.143670\n[CMD]: docker rm -f uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:20.143670\nError response from daemon: No such container: uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:20.365835\n[CMD]: docker run -d --name uw4kosoc84ooc808ww4080ww  --rm -v /root/.docker/config.json:/root/.docker/config.json:ro -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 10:19:20.365835\n0b60882c666a468473fc10c4a2350c55587bb7cd4cb62e200bba037838f453e8\n2025-Nov-05 10:19:22.053986\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'GIT_SSH_COMMAND=\"ssh -o ConnectTimeout=30 -p 22 -o Port=22 -o LogLevel=ERROR -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" git ls-remote [https://x-access-token:<REDACTED>@github.com/dimensionhq/ai.git](https://x-access-token:%3CREDACTED%3E@github.com/dimensionhq/ai.git) refs/heads/master'\n2025-Nov-05 10:19:22.053986\neb20a0771f925bf19364784030b4df74a55441f5\trefs/heads/master\n2025-Nov-05 10:19:22.188928\n----------------------------------------\n2025-Nov-05 10:19:22.194547\nImporting dimensionhq/ai:master (commit sha eb20a0771f925bf19364784030b4df74a55441f5) to /artifacts/uw4kosoc84ooc808ww4080ww.\n2025-Nov-05 10:19:22.451630\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'git clone --depth=1 --recurse-submodules --shallow-submodules -b 'master' '[https://x-access-token:<REDACTED>@github.com/dimensionhq/ai.git'](https://x-access-token:%3CREDACTED%3E@github.com/dimensionhq/ai.git') '/artifacts/uw4kosoc84ooc808ww4080ww' && cd '/artifacts/uw4kosoc84ooc808ww4080ww' && if [ -f .gitmodules ]; then git submodule sync && GIT_SSH_COMMAND=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" git submodule update --init --recursive --depth=1; fi && cd '/artifacts/uw4kosoc84ooc808ww4080ww' && GIT_SSH_COMMAND=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\" git lfs pull'\n2025-Nov-05 10:19:22.451630\nCloning into '/artifacts/uw4kosoc84ooc808ww4080ww'...\n2025-Nov-05 10:19:24.006780\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cd /artifacts/uw4kosoc84ooc808ww4080ww && git log -1 eb20a0771f925bf19364784030b4df74a55441f5 --pretty=%B'\n2025-Nov-05 10:19:24.006780\nMerge branch 'master' of https://github.com/dimensionhq/ai\n2025-Nov-05 10:19:24.769603\nImage not found (realmsoftwareinc/webhooks-prod:eb20a0771f925bf19364784030b4df74a55441f5). Building new image.\n2025-Nov-05 10:19:26.046158\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile'\n2025-Nov-05 10:19:26.046158\n# Build dependencies and virtualenv in a dedicated stage\n2025-Nov-05 10:19:26.046158\nFROM python:3.11-slim AS builder\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\nWORKDIR /app\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Install build tooling and uv (no recommends to keep layer slim)\n2025-Nov-05 10:19:26.046158\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n2025-Nov-05 10:19:26.046158\ngcc \\\n2025-Nov-05 10:19:26.046158\ng++ \\\n2025-Nov-05 10:19:26.046158\ncurl \\\n2025-Nov-05 10:19:26.046158\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\nRUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Copy workspace files needed for dependency resolution\n2025-Nov-05 10:19:26.046158\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:26.046158\nCOPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:26.046158\nCOPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Install Python dependencies into .venv\n2025-Nov-05 10:19:26.046158\nRUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Clean up build artefacts to keep the copied layer trim\n2025-Nov-05 10:19:26.046158\nRUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Final runtime image\n2025-Nov-05 10:19:26.046158\nFROM python:3.11-slim\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\nWORKDIR /app\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Install curl (for health check) and Infisical CLI with minimal extras\n2025-Nov-05 10:19:26.046158\nRUN apt-get update && apt-get install -y --no-install-recommends curl \\\n2025-Nov-05 10:19:26.046158\n&& curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash \\\n2025-Nov-05 10:19:26.046158\n&& apt-get install -y --no-install-recommends infisical \\\n2025-Nov-05 10:19:26.046158\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Copy in the prebuilt virtualenv and application code\n2025-Nov-05 10:19:26.046158\nCOPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:26.046158\nCOPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:26.046158\nCOPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:26.046158\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Expose port 8000\n2025-Nov-05 10:19:26.046158\nEXPOSE 8000\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Set environment variables\n2025-Nov-05 10:19:26.046158\nENV PYTHONPATH=/app\n2025-Nov-05 10:19:26.046158\nENV PYTHONUNBUFFERED=1\n2025-Nov-05 10:19:26.046158\nENV PATH=\"/app/.venv/bin:$PATH\"\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Health check\n2025-Nov-05 10:19:26.046158\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n2025-Nov-05 10:19:26.046158\nCMD curl -f http://localhost:8000/health || exit 1\n2025-Nov-05 10:19:26.046158\n2025-Nov-05 10:19:26.046158\n# Run the application with Infisical and Gunicorn\n2025-Nov-05 10:19:26.046158\n# Note: Set INFISICAL_TOKEN and INFISICAL_PROJECT_ID environment variables when running the container\n2025-Nov-05 10:19:26.046158\n# Example: docker run --env INFISICAL_TOKEN=$INFISICAL_TOKEN --env INFISICAL_PROJECT_ID=your-project-id your-image\n2025-Nov-05 10:19:26.046158\nCMD [\"sh\", \"-c\", \"infisical run --projectId $INFISICAL_PROJECT_ID --command 'cd /app/apps/webhooks/webhooks && /app/.venv/bin/gunicorn main:app -c gunicorn.conf.py'\"]\n2025-Nov-05 10:19:26.382688\nCreating build-time .env file in /artifacts (outside Docker context).\n2025-Nov-05 10:19:26.930010\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/build-time.env'\n2025-Nov-05 10:19:26.930010\nSOURCE_COMMIT='eb20a0771f925bf19364784030b4df74a55441f5'\n2025-Nov-05 10:19:26.930010\nCOOLIFY_URL=''\n2025-Nov-05 10:19:26.930010\nCOOLIFY_FQDN=''\n2025-Nov-05 10:19:26.930010\nCOOLIFY_BRANCH='master'\n2025-Nov-05 10:19:26.930010\nCOOLIFY_RESOURCE_UUID='b0808cwg8c4ko4kk84kkcks0'\n2025-Nov-05 10:19:26.930010\nCOOLIFY_CONTAINER_NAME='b0808cwg8c4ko4kk84kkcks0-101917583267'\n2025-Nov-05 10:19:26.930010\nINFISICAL_PROJECT_ID=\"65a15447c63fc2a03460007d\"\n2025-Nov-05 10:19:26.930010\nINFISICAL_TOKEN=\"st.885a8fa6-7c1c-47bb-8bfb-715555b46050.b9107cb09ef5069f89ac2720ab9c424a.1bd120fcdff9cbbab6c6c0e1f7678875\"\n2025-Nov-05 10:19:27.264299\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile'\n2025-Nov-05 10:19:27.264299\n# Build dependencies and virtualenv in a dedicated stage\n2025-Nov-05 10:19:27.264299\nFROM python:3.11-slim AS builder\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\nWORKDIR /app\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Install build tooling and uv (no recommends to keep layer slim)\n2025-Nov-05 10:19:27.264299\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n2025-Nov-05 10:19:27.264299\ngcc \\\n2025-Nov-05 10:19:27.264299\ng++ \\\n2025-Nov-05 10:19:27.264299\ncurl \\\n2025-Nov-05 10:19:27.264299\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\nRUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Copy workspace files needed for dependency resolution\n2025-Nov-05 10:19:27.264299\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.264299\nCOPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.264299\nCOPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Install Python dependencies into .venv\n2025-Nov-05 10:19:27.264299\nRUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Clean up build artefacts to keep the copied layer trim\n2025-Nov-05 10:19:27.264299\nRUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Final runtime image\n2025-Nov-05 10:19:27.264299\nFROM python:3.11-slim\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\nWORKDIR /app\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Install curl (for health check) and Infisical CLI with minimal extras\n2025-Nov-05 10:19:27.264299\nRUN apt-get update && apt-get install -y --no-install-recommends curl \\\n2025-Nov-05 10:19:27.264299\n&& curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash \\\n2025-Nov-05 10:19:27.264299\n&& apt-get install -y --no-install-recommends infisical \\\n2025-Nov-05 10:19:27.264299\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Copy in the prebuilt virtualenv and application code\n2025-Nov-05 10:19:27.264299\nCOPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:27.264299\nCOPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.264299\nCOPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.264299\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Expose port 8000\n2025-Nov-05 10:19:27.264299\nEXPOSE 8000\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Set environment variables\n2025-Nov-05 10:19:27.264299\nENV PYTHONPATH=/app\n2025-Nov-05 10:19:27.264299\nENV PYTHONUNBUFFERED=1\n2025-Nov-05 10:19:27.264299\nENV PATH=\"/app/.venv/bin:$PATH\"\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Health check\n2025-Nov-05 10:19:27.264299\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n2025-Nov-05 10:19:27.264299\nCMD curl -f http://localhost:8000/health || exit 1\n2025-Nov-05 10:19:27.264299\n2025-Nov-05 10:19:27.264299\n# Run the application with Infisical and Gunicorn\n2025-Nov-05 10:19:27.264299\n# Note: Set INFISICAL_TOKEN and INFISICAL_PROJECT_ID environment variables when running the container\n2025-Nov-05 10:19:27.264299\n# Example: docker run --env INFISICAL_TOKEN=$INFISICAL_TOKEN --env INFISICAL_PROJECT_ID=your-project-id your-image\n2025-Nov-05 10:19:27.264299\nCMD [\"sh\", \"-c\", \"infisical run --projectId $INFISICAL_PROJECT_ID --command 'cd /app/apps/webhooks/webhooks && /app/.venv/bin/gunicorn main:app -c gunicorn.conf.py'\"]\n2025-Nov-05 10:19:27.306071\nFinal Dockerfile:\n2025-Nov-05 10:19:27.928385\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile'\n2025-Nov-05 10:19:27.928385\n# Build dependencies and virtualenv in a dedicated stage\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_BUILD_SECRETS_HASH=4387256370ab25c71c6c7f93ce46e2a426fafb4ba429afe60f75c57a2cc75894\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_CONTAINER_NAME=b0808cwg8c4ko4kk84kkcks0-101917583267\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_RESOURCE_UUID=b0808cwg8c4ko4kk84kkcks0\n2025-Nov-05 10:19:27.928385\nARG COOLIFY_BRANCH=master\n2025-Nov-05 10:19:27.928385\nARG SOURCE_COMMIT=eb20a0771f925bf19364784030b4df74a55441f5\n2025-Nov-05 10:19:27.928385\nARG INFISICAL_TOKEN=st.885a8fa6-7c1c-47bb-8bfb-715555b46050.b9107cb09ef5069f89ac2720ab9c424a.1bd120fcdff9cbbab6c6c0e1f7678875\n2025-Nov-05 10:19:27.928385\nARG INFISICAL_PROJECT_ID=65a15447c63fc2a03460007d\n2025-Nov-05 10:19:27.928385\nFROM python:3.11-slim AS builder\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\nWORKDIR /app\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Install build tooling and uv (no recommends to keep layer slim)\n2025-Nov-05 10:19:27.928385\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n2025-Nov-05 10:19:27.928385\ngcc \\\n2025-Nov-05 10:19:27.928385\ng++ \\\n2025-Nov-05 10:19:27.928385\ncurl \\\n2025-Nov-05 10:19:27.928385\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\nRUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Copy workspace files needed for dependency resolution\n2025-Nov-05 10:19:27.928385\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.928385\nCOPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.928385\nCOPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Install Python dependencies into .venv\n2025-Nov-05 10:19:27.928385\nRUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Clean up build artefacts to keep the copied layer trim\n2025-Nov-05 10:19:27.928385\nRUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Final runtime image\n2025-Nov-05 10:19:27.928385\nFROM python:3.11-slim\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\nWORKDIR /app\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Install curl (for health check) and Infisical CLI with minimal extras\n2025-Nov-05 10:19:27.928385\nRUN apt-get update && apt-get install -y --no-install-recommends curl \\\n2025-Nov-05 10:19:27.928385\n&& curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash \\\n2025-Nov-05 10:19:27.928385\n&& apt-get install -y --no-install-recommends infisical \\\n2025-Nov-05 10:19:27.928385\n&& rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Copy in the prebuilt virtualenv and application code\n2025-Nov-05 10:19:27.928385\nCOPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:27.928385\nCOPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:27.928385\nCOPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:27.928385\nCOPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Expose port 8000\n2025-Nov-05 10:19:27.928385\nEXPOSE 8000\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Set environment variables\n2025-Nov-05 10:19:27.928385\nENV PYTHONPATH=/app\n2025-Nov-05 10:19:27.928385\nENV PYTHONUNBUFFERED=1\n2025-Nov-05 10:19:27.928385\nENV PATH=\"/app/.venv/bin:$PATH\"\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Health check\n2025-Nov-05 10:19:27.928385\nHEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \\\n2025-Nov-05 10:19:27.928385\nCMD curl -f http://localhost:8000/health || exit 1\n2025-Nov-05 10:19:27.928385\n2025-Nov-05 10:19:27.928385\n# Run the application with Infisical and Gunicorn\n2025-Nov-05 10:19:27.928385\n# Note: Set INFISICAL_TOKEN and INFISICAL_PROJECT_ID environment variables when running the container\n2025-Nov-05 10:19:27.928385\n# Example: docker run --env INFISICAL_TOKEN=$INFISICAL_TOKEN --env INFISICAL_PROJECT_ID=your-project-id your-image\n2025-Nov-05 10:19:27.928385\nCMD [\"sh\", \"-c\", \"infisical run --projectId $INFISICAL_PROJECT_ID --command 'cd /app/apps/webhooks/webhooks && /app/.venv/bin/gunicorn main:app -c gunicorn.conf.py'\"]\n2025-Nov-05 10:19:27.942703\n----------------------------------------\n2025-Nov-05 10:19:27.949099\nBuilding docker image started.\n2025-Nov-05 10:19:27.957779\nTo check the current progress, click on Show Debug Logs.\n2025-Nov-05 10:19:28.573409\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'cat /artifacts/build.sh'\n2025-Nov-05 10:19:28.573409\ncd /artifacts/uw4kosoc84ooc808ww4080ww && set -a && source /artifacts/build-time.env && set +a && docker build   --network host -f /artifacts/uw4kosoc84ooc808ww4080ww/apps/webhooks/Dockerfile --build-arg SOURCE_COMMIT --build-arg COOLIFY_URL --build-arg COOLIFY_FQDN --build-arg COOLIFY_BRANCH --build-arg COOLIFY_RESOURCE_UUID --build-arg COOLIFY_CONTAINER_NAME --build-arg INFISICAL_PROJECT_ID --build-arg INFISICAL_TOKEN --build-arg COOLIFY_BUILD_SECRETS_HASH=636980d03d37af4b459b987881db690134f9b90ab8d67d21c1fc8e99e0f83d41 --build-arg 'SOURCE_COMMIT' --build-arg 'COOLIFY_URL' --build-arg 'COOLIFY_FQDN' --build-arg 'COOLIFY_BRANCH' --build-arg 'COOLIFY_RESOURCE_UUID' --build-arg 'COOLIFY_CONTAINER_NAME' --progress plain -t realmsoftwareinc/webhooks-prod:eb20a0771f925bf19364784030b4df74a55441f5 /artifacts/uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:29.156678\n[CMD]: docker exec uw4kosoc84ooc808ww4080ww bash -c 'bash /artifacts/build.sh'\n2025-Nov-05 10:19:29.156678\n#0 building with \"default\" instance using docker driver\n2025-Nov-05 10:19:29.156678\n2025-Nov-05 10:19:29.156678\n#1 [internal] load build definition from Dockerfile\n2025-Nov-05 10:19:29.156678\n#1 transferring dockerfile: 2.52kB done\n2025-Nov-05 10:19:29.156678\n#1 DONE 0.0s\n2025-Nov-05 10:19:29.156678\n2025-Nov-05 10:19:29.156678\n#2 [auth] library/python:pull token for registry-1.docker.io\n2025-Nov-05 10:19:29.156678\n#2 DONE 0.0s\n2025-Nov-05 10:19:29.156678\n2025-Nov-05 10:19:29.156678\n#3 [internal] load metadata for docker.io/library/python:3.11-slim\n2025-Nov-05 10:19:29.156678\n#3 DONE 0.1s\n2025-Nov-05 10:19:29.262918\n#4 [builder 1/9] FROM docker.io/library/python:3.11-slim@sha256:fa9b525a0be0c5ae5e6f2209f4be6fdc5a15a36fed0222144d98ac0d08f876d4\n2025-Nov-05 10:19:29.262918\n#4 DONE 0.0s\n2025-Nov-05 10:19:29.262918\n2025-Nov-05 10:19:29.262918\n#5 [internal] load .dockerignore\n2025-Nov-05 10:19:29.262918\n#5 transferring context: 2B done\n2025-Nov-05 10:19:29.262918\n#5 DONE 0.0s\n2025-Nov-05 10:19:29.262918\n2025-Nov-05 10:19:29.262918\n#6 [internal] load build context\n2025-Nov-05 10:19:29.262918\n#6 transferring context: 1.44MB 0.1s done\n2025-Nov-05 10:19:29.510044\n#6 DONE 0.1s\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#7 [builder 6/9] COPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:29.510044\n#7 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#8 [builder 2/9] WORKDIR /app\n2025-Nov-05 10:19:29.510044\n#8 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#9 [builder 4/9] RUN pip install --no-cache-dir uv\n2025-Nov-05 10:19:29.510044\n#9 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#10 [builder 3/9] RUN apt-get update && apt-get install -y --no-install-recommends   gcc   g++   curl   && rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:29.510044\n#10 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#11 [builder 5/9] COPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:29.510044\n#11 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#12 [builder 6/9] COPY packages/shared /app/packages/shared\n2025-Nov-05 10:19:29.510044\n#12 CACHED\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#13 [builder 7/9] COPY apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:29.510044\n#13 DONE 0.1s\n2025-Nov-05 10:19:29.510044\n2025-Nov-05 10:19:29.510044\n#14 [builder 8/9] RUN uv sync --frozen --package webhooks\n2025-Nov-05 10:19:29.594969\n#14 0.238 Using CPython 3.11.14 interpreter at: /usr/local/bin/python3\n2025-Nov-05 10:19:29.594969\n#14 0.238 Creating virtual environment at: .venv\n2025-Nov-05 10:19:29.749236\n#14 0.305    Building webhooks @ file:///app/apps/webhooks\n2025-Nov-05 10:19:29.749236\n#14 0.309    Building shared @ file:///app/packages/shared\n2025-Nov-05 10:19:29.749236\n#14 0.392 Downloading aiohttp (1.7MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.392 Downloading psycopg-binary (4.2MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.393 Downloading pynacl (1.3MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.394 Downloading sqlalchemy (3.2MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.396 Downloading asyncpg (3.0MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.397 Downloading pydantic-core (2.0MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.398 Downloading cryptography (4.3MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.399 Downloading virtualenv (5.7MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading ruff (12.8MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading uvloop (3.8MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading pygments (1.2MiB)\n2025-Nov-05 10:19:29.911749\n#14 0.400 Downloading grpcio (6.2MiB)\n2025-Nov-05 10:19:30.555268\n#14 1.199    Building nats-py==2.11.0\n2025-Nov-05 10:19:31.339158\n#14 1.983  Downloading pydantic-core\n2025-Nov-05 10:19:31.491550\n#14 2.034    Building tonyg-rfc3339==0.1\n2025-Nov-05 10:19:31.491550\n#14 2.133    Building nkeys==0.2.1\n2025-Nov-05 10:19:31.598344\n#14 2.147  Downloading pynacl\n2025-Nov-05 10:19:31.602095\n#14 2.241  Downloading aiohttp\n2025-Nov-05 10:19:31.763871\n#14 2.407  Downloading asyncpg\n2025-Nov-05 10:19:31.999697\n#14 2.643  Downloading psycopg-binary\n2025-Nov-05 10:19:32.105619\n#14 2.749  Downloading virtualenv\n2025-Nov-05 10:19:32.252014\n#14 2.755  Downloading ruff\n2025-Nov-05 10:19:32.252014\n#14 2.818  Downloading uvloop\n2025-Nov-05 10:19:32.252014\n#14 2.890  Downloading grpcio\n2025-Nov-05 10:19:32.489971\n#14 2.929  Downloading sqlalchemy\n2025-Nov-05 10:19:32.489971\n#14 2.943  Downloading cryptography\n2025-Nov-05 10:19:32.489971\n#14 2.983  Downloading pygments\n2025-Nov-05 10:19:35.169371\n#14 5.813       Built nkeys==0.2.1\n2025-Nov-05 10:19:35.410078\n#14 6.052       Built nats-py==2.11.0\n2025-Nov-05 10:19:35.576134\n#14 6.084       Built shared @ file:///app/packages/shared\n2025-Nov-05 10:19:35.576134\n#14 6.218       Built webhooks @ file:///app/apps/webhooks\n2025-Nov-05 10:19:35.576134\n#14 6.218       Built tonyg-rfc3339==0.1\n2025-Nov-05 10:19:35.729658\n#14 6.221 Prepared 115 packages in 5.94s\n2025-Nov-05 10:19:35.793532\n#14 6.435 Installed 115 packages in 212ms\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiofiles==25.1.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiogoogle==5.17.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiohappyeyeballs==2.6.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiohttp==3.13.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + aiosignal==1.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + annotated-types==0.7.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + anyio==4.11.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + apscheduler==3.11.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + async-timeout==5.0.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + asyncpg==0.30.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + attrs==25.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cachetools==6.2.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + certifi==2025.10.5\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cffi==2.0.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cfgv==3.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + charset-normalizer==3.4.4\n2025-Nov-05 10:19:35.793532\n#14 6.435  + click==8.3.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + cryptography==46.0.2\n2025-Nov-05 10:19:35.793532\n#14 6.435  + distlib==0.4.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + distro==1.9.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + dnspython==2.8.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + email-validator==2.3.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + fastapi==0.119.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + fastapi-cli==0.0.13\n2025-Nov-05 10:19:35.793532\n#14 6.435  + fastapi-cloud-cli==0.3.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + filelock==3.20.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + frozenlist==1.8.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-api-core==2.26.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-auth==2.41.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-cloud-kms==3.6.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + google-cloud-pubsub==2.31.1\n2025-Nov-05 10:19:35.793532\n#14 6.435  + googleapis-common-protos==1.70.0\n2025-Nov-05 10:19:35.793532\n#14 6.435  + greenlet\n2025-Nov-05 10:19:35.950300\n==3.2.4\n2025-Nov-05 10:19:35.950300\n#14 6.435  + grpc-google-iam-v1==0.14.2\n2025-Nov-05 10:19:35.950300\n#14 6.435  + grpcio==1.75.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + grpcio-status==1.75.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + gunicorn==23.0.0\n2025-Nov-05 10:19:35.950300\n#14 6.435  + h11==0.16.0\n2025-Nov-05 10:19:35.950300\n#14 6.435  + httpcore==1.0.9\n2025-Nov-05 10:19:35.950300\n#14 6.435  + httptools==0.7.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + httpx==0.28.1\n2025-Nov-05 10:19:35.950300\n#14 6.435  + identify==2.6.15\n2025-Nov-05 10:19:35.950300\n#14 6.435  + idna==3.11\n2025-Nov-05 10:19:35.950300\n#14 6.439  + importlib-metadata==8.7.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + iniconfig==2.1.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + jinja2==3.1.6\n2025-Nov-05 10:19:35.950300\n#14 6.439  + jiter==0.11.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + markdown-it-py==4.0.0\n2025-Nov-05 10:19:35.950300\n#14 6.439  + markupsafe==3.0.3\n2025-Nov-05 10:19:35.950300\n#14 6.439  + mdurl==0.1.2\n2025-Nov-05 10:19:35.950300\n#14 6.440  + multidict==6.7.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + nats-py==2.11.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + nkeys==0.2.1\n2025-Nov-05 10:19:35.950300\n#14 6.440  + nodeenv==1.9.1\n2025-Nov-05 10:19:35.950300\n#14 6.440  + openai==1.109.1\n2025-Nov-05 10:19:35.950300\n#14 6.440  + opentelemetry-api==1.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + opentelemetry-sdk==1.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + opentelemetry-semantic-conventions==0.58b0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + packaging==25.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + platformdirs==4.5.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + pluggy==1.6.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + pre-commit==4.3.0\n2025-Nov-05 10:19:35.950300\n#14 6.440  + propcache==0.4.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + proto-plus==1.26.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + protobuf==6.32.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + psycopg==3.2.10\n2025-Nov-05 10:19:35.950300\n#14 6.441  + psycopg-binary==3.2.10\n2025-Nov-05 10:19:35.950300\n#14 6.441  + psycopg-pool==3.2.6\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pyasn1==0.6.1\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pyasn1-modules==0.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pybase64==1.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.441  + pycparser==2.23\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pydantic==2.12.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pydantic-core==2.41.3\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pydantic-settings==2.11.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pygments==2.19.2\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pynacl==1.6.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pytest==8.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.442  + python-dateutil==2.9.0.post0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + python-dotenv==1.1.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + python-multipart==0.0.20\n2025-Nov-05 10:19:35.950300\n#14 6.442  + pyyaml==6.0.3\n2025-Nov-05 10:19:35.950300\n#14 6.442  + redis==6.4.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + requests==2.32.5\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rich==14.2.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rich-toolkit==0.15.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rignore==0.7.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + rsa==4.9.1\n2025-Nov-05 10:19:35.950300\n#14 6.442  + ruff==0.14.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + sentry-sdk==2.41.0\n2025-Nov-05 10:19:35.950300\n#14 6.442  + shared==0.1.0 (from file:///app/packages/shared)\n2025-Nov-05 10:19:35.950300\n#14 6.443  + shellingham==1.5.4\n2025-Nov-05 10:19:35.950300\n#14 6.443  + six==1.17.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + slack-sdk==3.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + sniffio==1.3.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + sqlalchemy==2.0.44\n2025-Nov-05 10:19:35.950300\n#14 6.443  + starlette==0.48.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + structlog==25.4.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + tonyg-rfc3339==0.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + tqdm==4.67.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + turbopuffer==1.3.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + typer==0.19.2\n2025-Nov-05 10:19:35.950300\n#14 6.443  + typing-extensions==4.15.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + typing-inspection==0.4.2\n2025-Nov-05 10:19:35.950300\n#14 6.443  + tzlocal==5.3.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + urllib3==2.5.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + uvicorn==0.37.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + uvloop==0.21.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + virtualenv==20.35.3\n2025-Nov-05 10:19:35.950300\n#14 6.443  + watchfiles==1.1.0\n2025-Nov-05 10:19:35.950300\n#14 6.443  + webhooks==0.1.0 (from file:///app/apps/webhooks)\n2025-Nov-05 10:19:35.950300\n#14 6.443  + websockets==15.0.1\n2025-Nov-05 10:19:35.950300\n#14 6.443  + yarl==1.22.0\n2025-Nov-05 10:19:35.950300\n#14 6.444  + zep-cloud==3.5.0\n2025-Nov-05 10:19:35.950300\n#14 6.444  + zipp==3.23.0\n2025-Nov-05 10:19:37.586385\n#14 DONE 8.2s\n2025-Nov-05 10:19:37.746014\n#15 [builder 9/9] RUN rm -rf /root/.cache /root/.uv-cache\n2025-Nov-05 10:19:44.770014\n#15 DONE 7.2s\n2025-Nov-05 10:19:50.259887\n#16 [stage-1 3/7] RUN apt-get update && apt-get install -y --no-install-recommends curl   && curl -1sLf 'https://artifacts-cli.infisical.com/setup.deb.sh' | bash   && apt-get install -y --no-install-recommends infisical   && rm -rf /var/lib/apt/lists/*\n2025-Nov-05 10:19:50.412590\n#16 CACHED\n2025-Nov-05 10:19:50.412590\n2025-Nov-05 10:19:50.412590\n#17 [stage-1 4/7] COPY --from=builder /app/.venv /app/.venv\n2025-Nov-05 10:19:54.030020\n#17 DONE 3.8s\n2025-Nov-05 10:19:54.139159\n#18 [stage-1 5/7] COPY --from=builder /app/packages/shared /app/packages/shared\n2025-Nov-05 10:19:54.139159\n#18 DONE 0.1s\n2025-Nov-05 10:19:54.250116\n#19 [stage-1 6/7] COPY --from=builder /app/apps/webhooks /app/apps/webhooks\n2025-Nov-05 10:19:54.250116\n#19 DONE 0.1s\n2025-Nov-05 10:19:54.405723\n#20 [stage-1 7/7] COPY pyproject.toml uv.lock ./\n2025-Nov-05 10:19:54.455673\n#20 DONE 0.2s\n2025-Nov-05 10:19:54.645685\n#21 exporting to image\n2025-Nov-05 10:19:54.645685\n#21 exporting layers\n2025-Nov-05 10:19:56.973033\n#21 exporting layers 2.5s done\n2025-Nov-05 10:19:57.087921\n#21 writing image sha256:d284e7f248539ffb86adae9e166d03cd29425ac74c5bbe5691a0aa3219dfe2eb done\n2025-Nov-05 10:19:57.087921\n#21 naming to docker.io/realmsoftwareinc/webhooks-prod:eb20a0771f925bf19364784030b4df74a55441f5 done\n2025-Nov-05 10:19:57.087921\n#21 DONE 2.5s\n2025-Nov-05 10:19:57.140786\nBuilding docker image completed.\n2025-Nov-05 10:19:57.156075\nCreating .env file with runtime variables for build phase.\n2025-Nov-05 10:19:57.691747\nOops something is not okay, are you okay? \n2025-Nov-05 10:19:57.701848\nDeployment failed. Removing the new version of your application.\n2025-Nov-05 10:19:57.952779\nGracefully shutting down build container: uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.199636\n[CMD]: docker stop --time=30 uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.199636\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-05 10:19:58.603460\nuw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.857150\n[CMD]: docker rm -f uw4kosoc84ooc808ww4080ww\n2025-Nov-05 10:19:58.857150\nError response from daemon: No such container: uw4kosoc84ooc808ww4080ww\n```\n\nOr, another build that failed for no apparent reason, this time on the builder:\n\n```\n2025-Nov-04 21:11:16.151285\nFound a suitable build server (Builder).\n2025-Nov-04 21:11:16.162132\nStarting deployment of dimensionhq/ai:master to Webhooks - 2.\n2025-Nov-04 21:11:16.500066\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-04 21:11:16.703351\n[CMD]: docker stop --time=30 b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:16.703351\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-04 21:11:16.705523\nError response from daemon: No such container: b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:16.890495\n[CMD]: docker rm -f b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:16.890495\nError response from daemon: No such container: b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:17.130715\n[CMD]: docker run -d --name b0o844sgkwkow48oc4gc8cgk  --rm -v /root/.docker/config.json:/root/.docker/config.json:ro -v /var/run/docker.sock:/var/run/docker.sock ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-04 21:11:17.130715\n702832ce4fbd3a6abf2976103a23fec9c9d5642de8d7fbbe920ed7645cc74ea9\n2025-Nov-04 21:11:18.542726\nOops something is not okay, are you okay? \n2025-Nov-04 21:11:18.550276\nDeployment failed. Removing the new version of your application.\n2025-Nov-04 21:11:18.736879\nGracefully shutting down build container: b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:18.923578\n[CMD]: docker stop --time=30 b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:18.923578\nFlag --time has been deprecated, use --timeout instead\n2025-Nov-04 21:11:19.088910\nb0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:19.297149\n[CMD]: docker rm -f b0o844sgkwkow48oc4gc8cgk\n2025-Nov-04 21:11:19.297149\nError response from daemon: No such container: b0o844sgkwkow48oc4gc8cgk\n```\n\nFinal example:\n```\n2025-Nov-05 02:32:59.297151\nFound a suitable build server (Builder).\n2025-Nov-05 02:32:59.306247\nStarting deployment of dimensionhq/gmail-mcp-server:master to MCP - 2.\n2025-Nov-05 02:32:59.721620\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 02:33:02.138241\n----------------------------------------\n2025-Nov-05 02:33:02.147524\nImporting dimensionhq/gmail-mcp-server:master (commit sha 0062e28d4ef88173db8c0a9e33eca23b8b8c75d6) to /artifacts/i8k04s4scgkcok0wwwswoscs.\n2025-Nov-05 02:33:04.576587\nImage not found (realmsoftwareinc/gmail-mcp-server-production:0062e28d4ef88173db8c0a9e33eca23b8b8c75d6). Building new image.\n2025-Nov-05 02:33:07.614573\n----------------------------------------\n2025-Nov-05 02:33:07.624301\nBuilding docker image started.\n2025-Nov-05 02:33:07.633300\nTo check the current progress, click on Show Debug Logs.\n2025-Nov-05 02:33:51.372999\nBuilding docker image completed.\n2025-Nov-05 02:33:52.638262\n----------------------------------------\n2025-Nov-05 02:33:52.649146\nPushing image to docker registry (realmsoftwareinc/gmail-mcp-server-production:0062e28d4ef88173db8c0a9e33eca23b8b8c75d6).\n2025-Nov-05 02:34:14.362435\nTagging and pushing image with latest tag.\n2025-Nov-05 02:34:17.144804\nFound a suitable build server (Builder).\n2025-Nov-05 02:34:17.155960\nStarting deployment of dimensionhq/gmail-mcp-server:master to MCP - 2.\n2025-Nov-05 02:34:17.526919\nPreparing container with helper image: ghcr.io/coollabsio/coolify-helper:1.0.11\n2025-Nov-05 02:34:18.291348\n----------------------------------------\n2025-Nov-05 02:34:18.300366\nRolling update started.\n2025-Nov-05 02:34:34.775583\n----------------------------------------\n2025-Nov-05 02:34:34.792699\nRolling update started.\n2025-Nov-05 02:34:38.307388\nNew container started.\n2025-Nov-05 02:34:38.318536\nNew container started.\n2025-Nov-05 02:34:38.328167\nCustom healthcheck found in Dockerfile.\n2025-Nov-05 02:34:38.337170\nWaiting for healthcheck to pass on the new container.\n2025-Nov-05 02:34:38.346321\nWaiting for the start period (5 seconds) before starting healthcheck.\n2025-Nov-05 02:34:43.892990\nAttempt 1 of 3 | Healthcheck status: \"healthy\"\n2025-Nov-05 02:34:43.910443\nHealthcheck logs:   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n2025-Nov-05 02:34:43.910443\nDload  Upload   Total   Spent    Left  Speed\n2025-Nov-05 02:34:43.910443\n0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100    64  100    64    0     0   4002      0 --:--:-- --:--:-- --:--:--  4266\n2025-Nov-05 02:34:43.910443\n{\"status\":\"ok\",\"transport\":\"streamable-http\",\"mode\":\"stateless\"} | Return code: 0\n2025-Nov-05 02:34:43.929915\nNew container is healthy.\n2025-Nov-05 02:34:43.945834\nRemoving old containers.\n2025-Nov-05 02:34:46.188552\nRolling update completed.\n2025-Nov-05 02:34:46.260320\nGracefully shutting down build container: i8k04s4scgkcok0wwwswoscs\n2025-Nov-05 02:35:14.238820\nOops something is not okay, are you okay? \n2025-Nov-05 02:35:14.250226\nError: No such object: k00cg8gccos8s888cgo4w804-023024867976\n2025-Nov-05 02:35:14.261788\nDeployment failed. Removing the new version of your application.\n2025-Nov-05 02:35:14.553878\nGracefully shutting down build container: i8k04s4scgkcok0wwwswoscs\n```\n\n</details>\n\n### Steps to Reproduce\n\nI'm not certain how to reproduce this issue, but I would appreciate guidance on how to look deeper into the logs or inside Coolify to see what actually went wrong.\n\nWe setup a Builder instance with 6 vCPU & 16 GB of ram, with a maximum of 2 concurrent deployments. This issue occurs across several different resources and several different Docker images.\n\n### Example Repository URL\n\n_No response_\n\n### Coolify Version\n\nv4.0.0-beta.441\n\n### Are you using Coolify Cloud?\n\nNo (self-hosted)\n\n### Operating System and Version (self-hosted)\n\nUbuntu 24.04\n\n### Additional Information\n\n_No response_",
              "url": "https://github.com/coollabsio/coolify/issues/7113",
              "tech": [
                "go"
              ],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#7110",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.099Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.099Z",
            "created_at": "2025-11-09T14:41:15.099Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#7110",
              "status": "open",
              "type": "issue",
              "number": 7110,
              "title": "[Enhancement]: Update Clickhouse template",
              "source": {
                "data": {
                  "id": "source-coollabsio#7110",
                  "user": {
                    "login": "ronenteva",
                    "id": 2454954,
                    "node_id": "MDQ6VXNlcjI0NTQ5NTQ=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/2454954?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ronenteva",
                    "html_url": "https://github.com/ronenteva",
                    "followers_url": "https://api.github.com/users/ronenteva/followers",
                    "following_url": "https://api.github.com/users/ronenteva/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ronenteva/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ronenteva/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ronenteva/subscriptions",
                    "organizations_url": "https://api.github.com/users/ronenteva/orgs",
                    "repos_url": "https://api.github.com/users/ronenteva/repos",
                    "events_url": "https://api.github.com/users/ronenteva/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ronenteva/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Enhancement]: Update Clickhouse template",
                  "body": "### Request Type\n\nImprovement\n\n### Description\n\nCurrently the Clickhouse template is using `bitnamilegacy/clickhouse` which is not updated anymore.\nI believe the correct image to be used is `clickhouse:lts`.\n\nSimply changing the image doesn't work, probably just different environment variables.\nThere should be a migration path so data won't be lost (written explanation is fine).\n",
                  "html_url": "https://github.com/coollabsio/coolify/issues/7110"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#7110",
              "body": "### Request Type\n\nImprovement\n\n### Description\n\nCurrently the Clickhouse template is using `bitnamilegacy/clickhouse` which is not updated anymore.\nI believe the correct image to be used is `clickhouse:lts`.\n\nSimply changing the image doesn't work, probably just different environment variables.\nThere should be a migration path so data won't be lost (written explanation is fine).\n",
              "url": "https://github.com/coollabsio/coolify/issues/7110",
              "tech": [
                "go"
              ],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#6894",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.189Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.189Z",
            "created_at": "2025-11-09T14:41:15.189Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#6894",
              "status": "open",
              "type": "issue",
              "number": 6894,
              "title": "[Enhancement]: Project-specific members",
              "source": {
                "data": {
                  "id": "source-coollabsio#6894",
                  "user": {
                    "login": "zachlatta",
                    "id": 992248,
                    "node_id": "MDQ6VXNlcjk5MjI0OA==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/992248?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/zachlatta",
                    "html_url": "https://github.com/zachlatta",
                    "followers_url": "https://api.github.com/users/zachlatta/followers",
                    "following_url": "https://api.github.com/users/zachlatta/following{/other_user}",
                    "gists_url": "https://api.github.com/users/zachlatta/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/zachlatta/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/zachlatta/subscriptions",
                    "organizations_url": "https://api.github.com/users/zachlatta/orgs",
                    "repos_url": "https://api.github.com/users/zachlatta/repos",
                    "events_url": "https://api.github.com/users/zachlatta/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/zachlatta/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Enhancement]: Project-specific members",
                  "body": "### Request Type\n\nNew Feature\n\n### Description\n\n$1,000 USD bounty.\n\nAdd support for project-specific team members.\n\nProblem: I have team members I want to add to a project on Coolify, but I dont want to give them access to the full account and to all projects on the server. Instead, I want to invite them to a specific project on Coolify and for them to only have access to that project.\n\nAdditionally, I want to optionally allow the project-specific member to deploy their own apps on the servers on the account. This will just create a new project in the Team with them as a project-specific member.\n\nI should be able to manage all project-specific members from the main team page in addition to on each projects page.\n\nAny APIs that exist for managing team members should also work for project-specific members.\n\nThe purpose of this bounty is to create a secure way to give people limited access to projects on Coolify. They must not be able to break out of their permissions. For example, if I can SSH into a server as a project member using one of the keys - that would allow me to break out of my permissions.\n\nAcceptance criteria: Once your PR is merged and deployed on Coolify Cloud, I will test the features and award the bounty. Thanks!\n\n[Hack Club](https://hackclub.com) previously awarded bounties to implement database SSL and APIs to manage backups. Were a nonprofit for teenage coders!",
                  "html_url": "https://github.com/coollabsio/coolify/issues/6894"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#6894",
              "body": "### Request Type\n\nNew Feature\n\n### Description\n\n$1,000 USD bounty.\n\nAdd support for project-specific team members.\n\nProblem: I have team members I want to add to a project on Coolify, but I dont want to give them access to the full account and to all projects on the server. Instead, I want to invite them to a specific project on Coolify and for them to only have access to that project.\n\nAdditionally, I want to optionally allow the project-specific member to deploy their own apps on the servers on the account. This will just create a new project in the Team with them as a project-specific member.\n\nI should be able to manage all project-specific members from the main team page in addition to on each projects page.\n\nAny APIs that exist for managing team members should also work for project-specific members.\n\nThe purpose of this bounty is to create a secure way to give people limited access to projects on Coolify. They must not be able to break out of their permissions. For example, if I can SSH into a server as a project member using one of the keys - that would allow me to break out of my permissions.\n\nAcceptance criteria: Once your PR is merged and deployed on Coolify Cloud, I will test the features and award the bounty. Thanks!\n\n[Hack Club](https://hackclub.com) previously awarded bounties to implement database SSL and APIs to manage backups. Were a nonprofit for teenage coders!",
              "url": "https://github.com/coollabsio/coolify/issues/6894",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#4523",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.285Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.285Z",
            "created_at": "2025-11-09T14:41:15.285Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#4523",
              "status": "open",
              "type": "issue",
              "number": 4523,
              "title": "[Bug]: Arch Linux is not handled in InstallDocker.php",
              "source": {
                "data": {
                  "id": "source-coollabsio#4523",
                  "user": {
                    "login": "NeoxyBox",
                    "id": 69315360,
                    "node_id": "MDQ6VXNlcjY5MzE1MzYw",
                    "avatar_url": "https://avatars.githubusercontent.com/u/69315360?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/NeoxyBox",
                    "html_url": "https://github.com/NeoxyBox",
                    "followers_url": "https://api.github.com/users/NeoxyBox/followers",
                    "following_url": "https://api.github.com/users/NeoxyBox/following{/other_user}",
                    "gists_url": "https://api.github.com/users/NeoxyBox/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/NeoxyBox/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/NeoxyBox/subscriptions",
                    "organizations_url": "https://api.github.com/users/NeoxyBox/orgs",
                    "repos_url": "https://api.github.com/users/NeoxyBox/repos",
                    "events_url": "https://api.github.com/users/NeoxyBox/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/NeoxyBox/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Bug]: Arch Linux is not handled in InstallDocker.php",
                  "body": "### Error Message and Logs\n\nArch Linux is not handled in the `app/Actions/Server/InstallDocker.php` file, which makes it impossible to install an Arch Linux remote server.\n\nError:\n\n500\nWait, this is not cool...\nThere has been an error, we are working on it.\nError: Unsupported OS\n\n### Steps to Reproduce\n\n1. Create an Arch Linux server and install Coolify on it\n2. Try adding an Arch Linux server as a remote server\n\n### Example Repository URL\n\nhttps://github.com/coollabsio/coolify/blob/8d779c88ff4f4709ac0353713533a65fe7d0b86c/app/Actions/Server/InstallDocker.php#L76\n\n### Coolify Version\n\nv4.0.0-beta.376\n\n### Are you using Coolify Cloud?\n\nNo (self-hosted)\n\n### Operating System and Version (self-hosted)\n\nArch Linux\n\n### Additional Information\n\n_No response_",
                  "html_url": "https://github.com/coollabsio/coolify/issues/4523"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#4523",
              "body": "### Error Message and Logs\n\nArch Linux is not handled in the `app/Actions/Server/InstallDocker.php` file, which makes it impossible to install an Arch Linux remote server.\n\nError:\n\n500\nWait, this is not cool...\nThere has been an error, we are working on it.\nError: Unsupported OS\n\n### Steps to Reproduce\n\n1. Create an Arch Linux server and install Coolify on it\n2. Try adding an Arch Linux server as a remote server\n\n### Example Repository URL\n\nhttps://github.com/coollabsio/coolify/blob/8d779c88ff4f4709ac0353713533a65fe7d0b86c/app/Actions/Server/InstallDocker.php#L76\n\n### Coolify Version\n\nv4.0.0-beta.376\n\n### Are you using Coolify Cloud?\n\nNo (self-hosted)\n\n### Operating System and Version (self-hosted)\n\nArch Linux\n\n### Additional Information\n\n_No response_",
              "url": "https://github.com/coollabsio/coolify/issues/4523",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#6567",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.378Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.378Z",
            "created_at": "2025-11-09T14:41:15.378Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#6567",
              "status": "open",
              "type": "issue",
              "number": 6567,
              "title": "[Enhancement]: Add Soju IRC bouncer",
              "source": {
                "data": {
                  "id": "source-coollabsio#6567",
                  "user": {
                    "login": "XEJK",
                    "id": 4692876,
                    "node_id": "MDQ6VXNlcjQ2OTI4NzY=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/4692876?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/XEJK",
                    "html_url": "https://github.com/XEJK",
                    "followers_url": "https://api.github.com/users/XEJK/followers",
                    "following_url": "https://api.github.com/users/XEJK/following{/other_user}",
                    "gists_url": "https://api.github.com/users/XEJK/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/XEJK/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/XEJK/subscriptions",
                    "organizations_url": "https://api.github.com/users/XEJK/orgs",
                    "repos_url": "https://api.github.com/users/XEJK/repos",
                    "events_url": "https://api.github.com/users/XEJK/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/XEJK/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Enhancement]: Add Soju IRC bouncer",
                  "body": "### Request Type\n\nNew Service\n\n### Description\n\nIt would be nice to add the Soju IRC bouncer to the list of services.\nThe documentation is here: \n1. https://codeberg.org/emersion/soju/src/branch/master/doc/getting-started.md\n2. https://codeberg.org/emersion/soju-containers\n3. https://codeberg.org/emersion/soju/src/branch/master/doc/file-upload.md\n\nIt's based on a docker compose seems to need some extra thought since user creation is done through the command line \n\n\nWhen the template is merged in Coolify and I can spin-up a working instance of soju \nincluding file upload support, secure connections, and full support for the protocols supported by soju wjhich can be tested using an IRCv3 compatible client such as goguma or halloy will the attached bounty be seen as completed.\n\n",
                  "html_url": "https://github.com/coollabsio/coolify/issues/6567"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#6567",
              "body": "### Request Type\n\nNew Service\n\n### Description\n\nIt would be nice to add the Soju IRC bouncer to the list of services.\nThe documentation is here: \n1. https://codeberg.org/emersion/soju/src/branch/master/doc/getting-started.md\n2. https://codeberg.org/emersion/soju-containers\n3. https://codeberg.org/emersion/soju/src/branch/master/doc/file-upload.md\n\nIt's based on a docker compose seems to need some extra thought since user creation is done through the command line \n\n\nWhen the template is merged in Coolify and I can spin-up a working instance of soju \nincluding file upload support, secure connections, and full support for the protocols supported by soju wjhich can be tested using an IRCv3 compatible client such as goguma or halloy will the attached bounty be seen as completed.\n\n",
              "url": "https://github.com/coollabsio/coolify/issues/6567",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#6566",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.498Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.498Z",
            "created_at": "2025-11-09T14:41:15.498Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#6566",
              "status": "open",
              "type": "issue",
              "number": 6566,
              "title": "[Bug]: Scheduled tasks fails without logs",
              "source": {
                "data": {
                  "id": "source-coollabsio#6566",
                  "user": {
                    "login": "vlourme",
                    "id": 58728578,
                    "node_id": "MDQ6VXNlcjU4NzI4NTc4",
                    "avatar_url": "https://avatars.githubusercontent.com/u/58728578?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/vlourme",
                    "html_url": "https://github.com/vlourme",
                    "followers_url": "https://api.github.com/users/vlourme/followers",
                    "following_url": "https://api.github.com/users/vlourme/following{/other_user}",
                    "gists_url": "https://api.github.com/users/vlourme/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/vlourme/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/vlourme/subscriptions",
                    "organizations_url": "https://api.github.com/users/vlourme/orgs",
                    "repos_url": "https://api.github.com/users/vlourme/repos",
                    "events_url": "https://api.github.com/users/vlourme/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/vlourme/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Bug]: Scheduled tasks fails without logs",
                  "body": "### Error Message and Logs\n\nHello,\n\nI have a worker container that execute multiple tasks based on cron, around 20 cron with different schedules, hourly or daily mostly.\n\nI have a lot of cron that fails without any logs, without any reason  and, weirdly they work when I launch them manually.\n\nExpending logs just shows: \"Waiting for task output...\"\n\n<img width=\"1371\" height=\"703\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d086e872-4717-4cab-8f33-5398a4675280\" />\n\n### Steps to Reproduce\n\nI don't know\n\n\n\n### Example Repository URL\n\n_No response_\n\n### Coolify Version\n\nv4.0.0-beta.426\n\n### Are you using Coolify Cloud?\n\nNo (self-hosted)\n\n### Operating System and Version (self-hosted)\n\nDebian 13 (trixie)\n\n### Additional Information\n\n_No response_",
                  "html_url": "https://github.com/coollabsio/coolify/issues/6566"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#6566",
              "body": "### Error Message and Logs\n\nHello,\n\nI have a worker container that execute multiple tasks based on cron, around 20 cron with different schedules, hourly or daily mostly.\n\nI have a lot of cron that fails without any logs, without any reason  and, weirdly they work when I launch them manually.\n\nExpending logs just shows: \"Waiting for task output...\"\n\n<img width=\"1371\" height=\"703\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/d086e872-4717-4cab-8f33-5398a4675280\" />\n\n### Steps to Reproduce\n\nI don't know\n\n\n\n### Example Repository URL\n\n_No response_\n\n### Coolify Version\n\nv4.0.0-beta.426\n\n### Are you using Coolify Cloud?\n\nNo (self-hosted)\n\n### Operating System and Version (self-hosted)\n\nDebian 13 (trixie)\n\n### Additional Information\n\n_No response_",
              "url": "https://github.com/coollabsio/coolify/issues/6566",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#6519",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.627Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.627Z",
            "created_at": "2025-11-09T14:41:15.627Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#6519",
              "status": "open",
              "type": "issue",
              "number": 6519,
              "title": "[Enhancement]: Filebrowser for Containerlevel",
              "source": {
                "data": {
                  "id": "source-coollabsio#6519",
                  "user": {
                    "login": "swissbyte",
                    "id": 33572050,
                    "node_id": "MDQ6VXNlcjMzNTcyMDUw",
                    "avatar_url": "https://avatars.githubusercontent.com/u/33572050?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/swissbyte",
                    "html_url": "https://github.com/swissbyte",
                    "followers_url": "https://api.github.com/users/swissbyte/followers",
                    "following_url": "https://api.github.com/users/swissbyte/following{/other_user}",
                    "gists_url": "https://api.github.com/users/swissbyte/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/swissbyte/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/swissbyte/subscriptions",
                    "organizations_url": "https://api.github.com/users/swissbyte/orgs",
                    "repos_url": "https://api.github.com/users/swissbyte/repos",
                    "events_url": "https://api.github.com/users/swissbyte/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/swissbyte/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Enhancement]: Filebrowser for Containerlevel",
                  "body": "### Request Type\n\nNew Feature\n\n### Description\n\nI would like to get the ability to have a filebrowser on containerlevel. The idea is, that, for example, one can download or upload config file or images or any other type of files within the scope of the container. This includes mounts and also the overlay fs. Just a filebrowser for everything that you can see inside the container itself. \n\nPrio 1: mapped folders\nPrio 2: everything \"inside\" the container. \n\nIt should support: \n\n- Browsing like windows explorer or nautilus under linux\n- Upload and download single files\n- Upload and download folders\n- Create folders\n- Delete folders / files\n- Check Filesize\n- Check permissions (optional: set them)\n",
                  "html_url": "https://github.com/coollabsio/coolify/issues/6519"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#6519",
              "body": "### Request Type\n\nNew Feature\n\n### Description\n\nI would like to get the ability to have a filebrowser on containerlevel. The idea is, that, for example, one can download or upload config file or images or any other type of files within the scope of the container. This includes mounts and also the overlay fs. Just a filebrowser for everything that you can see inside the container itself. \n\nPrio 1: mapped folders\nPrio 2: everything \"inside\" the container. \n\nIt should support: \n\n- Browsing like windows explorer or nautilus under linux\n- Upload and download single files\n- Upload and download folders\n- Create folders\n- Delete folders / files\n- Check Filesize\n- Check permissions (optional: set them)\n",
              "url": "https://github.com/coollabsio/coolify/issues/6519",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#4117",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.733Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.733Z",
            "created_at": "2025-11-09T14:41:15.733Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#4117",
              "status": "open",
              "type": "issue",
              "number": 4117,
              "title": "[Enhancement]: Improve navigation from breadcrumb",
              "source": {
                "data": {
                  "id": "source-coollabsio#4117",
                  "user": {
                    "login": "apperside",
                    "id": 5955338,
                    "node_id": "MDQ6VXNlcjU5NTUzMzg=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/5955338?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/apperside",
                    "html_url": "https://github.com/apperside",
                    "followers_url": "https://api.github.com/users/apperside/followers",
                    "following_url": "https://api.github.com/users/apperside/following{/other_user}",
                    "gists_url": "https://api.github.com/users/apperside/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/apperside/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/apperside/subscriptions",
                    "organizations_url": "https://api.github.com/users/apperside/orgs",
                    "repos_url": "https://api.github.com/users/apperside/repos",
                    "events_url": "https://api.github.com/users/apperside/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/apperside/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Enhancement]: Improve navigation from breadcrumb",
                  "body": "### Request Type\n\nNew Feature\n\n### Description\n\nIt would be a big UX improvement if it would be possible to navigate resources and sections of resources directly from the breadcrumbs.\nBelow you will find a very explicative screenshot\n\n![Image](https://github.com/user-attachments/assets/2566cb0b-6be6-40dc-8448-b640b2d6ebe5)\n",
                  "html_url": "https://github.com/coollabsio/coolify/issues/4117"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#4117",
              "body": "### Request Type\n\nNew Feature\n\n### Description\n\nIt would be a big UX improvement if it would be possible to navigate resources and sections of resources directly from the breadcrumbs.\nBelow you will find a very explicative screenshot\n\n![Image](https://github.com/user-attachments/assets/2566cb0b-6be6-40dc-8448-b640b2d6ebe5)\n",
              "url": "https://github.com/coollabsio/coolify/issues/4117",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#2495",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.833Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.833Z",
            "created_at": "2025-11-09T14:41:15.833Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#2495",
              "status": "open",
              "type": "issue",
              "number": 2495,
              "title": "[Improvement]: Container Info and Network tab (for each container and service)",
              "source": {
                "data": {
                  "id": "source-coollabsio#2495",
                  "user": {
                    "login": "peaklabs-dev",
                    "id": 122374094,
                    "node_id": "U_kgDOB0tHzg",
                    "avatar_url": "https://avatars.githubusercontent.com/u/122374094?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/peaklabs-dev",
                    "html_url": "https://github.com/peaklabs-dev",
                    "followers_url": "https://api.github.com/users/peaklabs-dev/followers",
                    "following_url": "https://api.github.com/users/peaklabs-dev/following{/other_user}",
                    "gists_url": "https://api.github.com/users/peaklabs-dev/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/peaklabs-dev/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/peaklabs-dev/subscriptions",
                    "organizations_url": "https://api.github.com/users/peaklabs-dev/orgs",
                    "repos_url": "https://api.github.com/users/peaklabs-dev/repos",
                    "events_url": "https://api.github.com/users/peaklabs-dev/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/peaklabs-dev/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Improvement]: Container Info and Network tab (for each container and service)",
                  "body": "### Description\n\nI had a deep look into the coolify containers and found some things I believe could be improved (hope I can help like this as I am at the moment not ready to start code contributing)\n\n### Minimal Reproduction (if possible, example repository)\n\nNetwork page on the individual Container/ services:\n- [ ] Let us select network via dropdown and also see which network the container is in\n- [ ] Make sure we can connect containers to networks from the UI after they are created, especially for databases this would be great as they do not have a predefined network checkbox. The ability to have a UI where we can select networks would also eliminate the connect to predefined network checkbox, as if we select a network when creating or on the Network tab, it will automatically connect to that.\n- [ ] When you select a network when creating a resource, it is automatically connected to the resource without you having to check a checkbox.\n- [ ] Let us remove networks with a click of a button\n- [ ] let us add multiple networks to the container 3-4...\n- [ ] Also enabled Ipv6 on docker networks (all of them should have Ipv4 and Ipv6): https://github.com/coollabsio/coolify/discussions/4048\n\n\nGeneral info tab for the current container:\n- [ ] IPv4 and IPv6 IP addresses displayed for the container with a copy button\n- [ ] Mac Adresse is displayed here in the network tab for the container\n- [ ] let us see the container ID \n- [ ] Let us see the container name -> the name with the uuid\n- [ ] Let us set the container name for databases manually if we want\n- [ ] container Image hash \n- [ ] when was the container create\n- [ ] when was it startet -> especially for services\n- [ ] when was the image last pulled\n- [ ] when was the container updated -> depended on this: https://github.com/coollabsio/coolify/issues/2500\n",
                  "html_url": "https://github.com/coollabsio/coolify/issues/2495"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#2495",
              "body": "### Description\n\nI had a deep look into the coolify containers and found some things I believe could be improved (hope I can help like this as I am at the moment not ready to start code contributing)\n\n### Minimal Reproduction (if possible, example repository)\n\nNetwork page on the individual Container/ services:\n- [ ] Let us select network via dropdown and also see which network the container is in\n- [ ] Make sure we can connect containers to networks from the UI after they are created, especially for databases this would be great as they do not have a predefined network checkbox. The ability to have a UI where we can select networks would also eliminate the connect to predefined network checkbox, as if we select a network when creating or on the Network tab, it will automatically connect to that.\n- [ ] When you select a network when creating a resource, it is automatically connected to the resource without you having to check a checkbox.\n- [ ] Let us remove networks with a click of a button\n- [ ] let us add multiple networks to the container 3-4...\n- [ ] Also enabled Ipv6 on docker networks (all of them should have Ipv4 and Ipv6): https://github.com/coollabsio/coolify/discussions/4048\n\n\nGeneral info tab for the current container:\n- [ ] IPv4 and IPv6 IP addresses displayed for the container with a copy button\n- [ ] Mac Adresse is displayed here in the network tab for the container\n- [ ] let us see the container ID \n- [ ] Let us see the container name -> the name with the uuid\n- [ ] Let us set the container name for databases manually if we want\n- [ ] container Image hash \n- [ ] when was the container create\n- [ ] when was it startet -> especially for services\n- [ ] when was the image last pulled\n- [ ] when was the container updated -> depended on this: https://github.com/coollabsio/coolify/issues/2500\n",
              "url": "https://github.com/coollabsio/coolify/issues/2495",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "coollabsio#4458",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "coollabsio",
              "id": "generated-coollabsio",
              "name": "Coollabsio",
              "description": "",
              "members": [],
              "display_name": "Coollabsio",
              "created_at": "2025-11-09T14:41:15.934Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/coollabsio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "coollabsio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-11-09T14:41:15.934Z",
            "created_at": "2025-11-09T14:41:15.934Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-coollabsio#4458",
              "status": "open",
              "type": "issue",
              "number": 4458,
              "title": "[Enhancement]: AppFlowy Serice",
              "source": {
                "data": {
                  "id": "source-coollabsio#4458",
                  "user": {
                    "login": "imaron85",
                    "id": 45713863,
                    "node_id": "MDQ6VXNlcjQ1NzEzODYz",
                    "avatar_url": "https://avatars.githubusercontent.com/u/45713863?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/imaron85",
                    "html_url": "https://github.com/imaron85",
                    "followers_url": "https://api.github.com/users/imaron85/followers",
                    "following_url": "https://api.github.com/users/imaron85/following{/other_user}",
                    "gists_url": "https://api.github.com/users/imaron85/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/imaron85/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/imaron85/subscriptions",
                    "organizations_url": "https://api.github.com/users/imaron85/orgs",
                    "repos_url": "https://api.github.com/users/imaron85/repos",
                    "events_url": "https://api.github.com/users/imaron85/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/imaron85/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "[Enhancement]: AppFlowy Serice",
                  "body": "### Request Type\n\nNew Service\n\n### Description\n\nI would like a Coolify Service for easily deploying AppFlowy.\nI've tried myself, but can't figure out recreating the nginx config in coolify.\nIt might not even be neccessary, all I want is be able to easily deploy AppFlowy for our small team.",
                  "html_url": "https://github.com/coollabsio/coolify/issues/4458"
                },
                "type": "github"
              },
              "hash": "coollabsio/coolify#4458",
              "body": "### Request Type\n\nNew Service\n\n### Description\n\nI would like a Coolify Service for easily deploying AppFlowy.\nI've tried myself, but can't figure out recreating the nginx config in coolify.\nIt might not even be neccessary, all I want is be able to easily deploy AppFlowy for our small team.",
              "url": "https://github.com/coollabsio/coolify/issues/4458",
              "tech": [],
              "repo_name": "coolify",
              "repo_owner": "coollabsio",
              "forge": "github"
            },
            "timeouts_disabled": false
          }
        ],
        "next_cursor": null
      }
    }
  }
}