{
  "result": {
    "data": {
      "json": {
        "items": [
          {
            "id": "outerbase#59",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "outerbase",
              "id": "generated-outerbase",
              "name": "Outerbase",
              "description": "",
              "members": [],
              "display_name": "Outerbase",
              "created_at": "2025-12-18T12:31:09.483Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/outerbase?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "outerbase",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:09.483Z",
            "created_at": "2025-12-18T12:31:09.483Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-outerbase#59",
              "status": "open",
              "type": "issue",
              "number": 59,
              "title": "Database dumps do not work on large databases",
              "source": {
                "data": {
                  "id": "source-outerbase#59",
                  "user": {
                    "login": "Brayden",
                    "id": 1066085,
                    "node_id": "MDQ6VXNlcjEwNjYwODU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1066085?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Brayden",
                    "html_url": "https://github.com/Brayden",
                    "followers_url": "https://api.github.com/users/Brayden/followers",
                    "following_url": "https://api.github.com/users/Brayden/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Brayden/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Brayden/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Brayden/subscriptions",
                    "organizations_url": "https://api.github.com/users/Brayden/orgs",
                    "repos_url": "https://api.github.com/users/Brayden/repos",
                    "events_url": "https://api.github.com/users/Brayden/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Brayden/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Database dumps do not work on large databases",
                  "body": "**Describe the bug**\nIf you try to use any of the database dump endpoints such as SQL, CSV or JSON the data is loaded into memory and then created as a dump file. To support any size database we should investigate enhancements to allow any sized database to be exported. Currently the size limitations are 1GB for Durable Objects with 10GB in the future. Operate under the assumption that we might be attempting to dump a 10GB database into a `.sql` file.\n\nAnother consideration to make is because Durable Objects execute synchronous operations we may need to allow for \"breathing intervals\". An example might be we allow our export operation to run for 5 seconds, and take 5 seconds off if other requests are in a queue, then it can pick up again. The goal here would be to prevent locking the database for long periods of time.\n\nBut then poses the questions: \n1. How do we continue operations that need more than 30 seconds to work?\n2. Where is the data stored as it's being created? (R2, S3, something else)?\n3. How do we deliver that dump information to the user after its completed?\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n1. Hit the `/export/dump` endpoint on a large database\n2. Will eventually fail when the 30 second request response time window closes\n\nRun the following command in Terminal (replace the URL with yours) and if your operation exceeds 30 seconds you should see a failed network response instead of a dump file.\n```\ncurl --location 'https://starbasedb.YOUR-ID-HERE.workers.dev/export/dump' \\\n--header 'Authorization: Bearer ABC123' \\\n--output database_dump.sql\n```\n\nIf you can't create a large enough test database feel free to add code in to `sleep` for 29 seconds before proceeding with the `/export/dump` functional code and should also see the failure.\n\n**Expected behavior**\nAs a user I would expect any and all of the specified data to be dumped out without an error and without partial results. Where it ends up for the user to access if the operation takes more than 30 seconds is up for discussion. Ideally if shorter than 30 seconds it could be returned as our cURL above works today (downloads the file from the response of the origin request), but perhaps after the timeout it continues on uploads it to a destination source to access afterwards?\n\n**Proposed Solution:**\n1. For backups require an R2 binding\n2. Have a `.sql` file that gets created in R2 with the filename like `dump_20240101-170000.sql` where it represents `2024-01-01 17:00:00`\n3. Create the file and continuously append new chunks to it until reaching the end\n4. May need to utilize a DO alarm to continue the work after X time if a timeout occurs & mark where it currently is in the process in internal memory so it can pick up and continue.\n5. Provide a callback URL when the operation is finally completed so users can create custom logic to notify them (e.g. Email, Slack, etc)",
                  "html_url": "https://github.com/outerbase/starbasedb/issues/59"
                },
                "type": "github"
              },
              "hash": "outerbase/starbasedb#59",
              "body": "**Describe the bug**\nIf you try to use any of the database dump endpoints such as SQL, CSV or JSON the data is loaded into memory and then created as a dump file. To support any size database we should investigate enhancements to allow any sized database to be exported. Currently the size limitations are 1GB for Durable Objects with 10GB in the future. Operate under the assumption that we might be attempting to dump a 10GB database into a `.sql` file.\n\nAnother consideration to make is because Durable Objects execute synchronous operations we may need to allow for \"breathing intervals\". An example might be we allow our export operation to run for 5 seconds, and take 5 seconds off if other requests are in a queue, then it can pick up again. The goal here would be to prevent locking the database for long periods of time.\n\nBut then poses the questions: \n1. How do we continue operations that need more than 30 seconds to work?\n2. Where is the data stored as it's being created? (R2, S3, something else)?\n3. How do we deliver that dump information to the user after its completed?\n\n**To Reproduce**\nSteps to reproduce the behavior:\n\n1. Hit the `/export/dump` endpoint on a large database\n2. Will eventually fail when the 30 second request response time window closes\n\nRun the following command in Terminal (replace the URL with yours) and if your operation exceeds 30 seconds you should see a failed network response instead of a dump file.\n```\ncurl --location 'https://starbasedb.YOUR-ID-HERE.workers.dev/export/dump' \\\n--header 'Authorization: Bearer ABC123' \\\n--output database_dump.sql\n```\n\nIf you can't create a large enough test database feel free to add code in to `sleep` for 29 seconds before proceeding with the `/export/dump` functional code and should also see the failure.\n\n**Expected behavior**\nAs a user I would expect any and all of the specified data to be dumped out without an error and without partial results. Where it ends up for the user to access if the operation takes more than 30 seconds is up for discussion. Ideally if shorter than 30 seconds it could be returned as our cURL above works today (downloads the file from the response of the origin request), but perhaps after the timeout it continues on uploads it to a destination source to access afterwards?\n\n**Proposed Solution:**\n1. For backups require an R2 binding\n2. Have a `.sql` file that gets created in R2 with the filename like `dump_20240101-170000.sql` where it represents `2024-01-01 17:00:00`\n3. Create the file and continuously append new chunks to it until reaching the end\n4. May need to utilize a DO alarm to continue the work after X time if a timeout occurs & mark where it currently is in the process in internal memory so it can pick up and continue.\n5. Provide a callback URL when the operation is finally completed so users can create custom logic to notify them (e.g. Email, Slack, etc)",
              "url": "https://github.com/outerbase/starbasedb/issues/59",
              "tech": [
                "go"
              ],
              "repo_name": "starbasedb",
              "repo_owner": "outerbase",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "outerbase#72",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "outerbase",
              "id": "generated-outerbase",
              "name": "Outerbase",
              "description": "",
              "members": [],
              "display_name": "Outerbase",
              "created_at": "2025-12-18T12:31:09.712Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/outerbase?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "outerbase",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:09.712Z",
            "created_at": "2025-12-18T12:31:09.712Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-outerbase#72",
              "status": "open",
              "type": "issue",
              "number": 72,
              "title": "Replicate data from external source to internal source with a Plugin",
              "source": {
                "data": {
                  "id": "source-outerbase#72",
                  "user": {
                    "login": "Brayden",
                    "id": 1066085,
                    "node_id": "MDQ6VXNlcjEwNjYwODU=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1066085?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Brayden",
                    "html_url": "https://github.com/Brayden",
                    "followers_url": "https://api.github.com/users/Brayden/followers",
                    "following_url": "https://api.github.com/users/Brayden/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Brayden/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Brayden/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Brayden/subscriptions",
                    "organizations_url": "https://api.github.com/users/Brayden/orgs",
                    "repos_url": "https://api.github.com/users/Brayden/repos",
                    "events_url": "https://api.github.com/users/Brayden/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Brayden/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Replicate data from external source to internal source with a Plugin",
                  "body": "**Is your feature request related to a problem? Please describe.**\nStarbaseDB instances support by default an internal database (SQLite offered by the Durable Object) as well as an optional external data source. External data sources can be powered in one of two ways, both by providing values in the `wrangler.toml` file of the project.\n\n- Outerbase API Key\n- Connection details of the database\n\n<img width=\"481\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/485d4b88-a7f8-432d-9f29-d3239a6e6577\" />\n\n**Describe the solution you'd like**\nWhat would be beneficial for some use cases is the ability to bring in an external data source (e.g. a Postgres on Supabase) and have a pull mechanism where data can be brought into the internal DO SQLite so that the instance serves as a close-to-edge replica that can be queried alternatively to querying the Supabase Postgres instance.\n\n**Describe alternatives you've considered**\n- Considering the pull vs push mechanism. A pull mechanism seems to be a better global solution where a push mechanism would be required to live elsewhere on a per provider basis.\n\n**Additional context**\n- Might be beneficial for users to be able to define in the plugin what intervals data should be pulled at\n- Might be beneficial to allow users to define which tables should have data pulled into it (perhaps not all tables need replicated)\n- Likely need a way to know for each table what the last queried items were so you can do append-only type polling for new data. Does a user need to define a column to base this on (e.g. `id` or `created_at` columns perhaps)?\n",
                  "html_url": "https://github.com/outerbase/starbasedb/issues/72"
                },
                "type": "github"
              },
              "hash": "outerbase/starbasedb#72",
              "body": "**Is your feature request related to a problem? Please describe.**\nStarbaseDB instances support by default an internal database (SQLite offered by the Durable Object) as well as an optional external data source. External data sources can be powered in one of two ways, both by providing values in the `wrangler.toml` file of the project.\n\n- Outerbase API Key\n- Connection details of the database\n\n<img width=\"481\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/485d4b88-a7f8-432d-9f29-d3239a6e6577\" />\n\n**Describe the solution you'd like**\nWhat would be beneficial for some use cases is the ability to bring in an external data source (e.g. a Postgres on Supabase) and have a pull mechanism where data can be brought into the internal DO SQLite so that the instance serves as a close-to-edge replica that can be queried alternatively to querying the Supabase Postgres instance.\n\n**Describe alternatives you've considered**\n- Considering the pull vs push mechanism. A pull mechanism seems to be a better global solution where a push mechanism would be required to live elsewhere on a per provider basis.\n\n**Additional context**\n- Might be beneficial for users to be able to define in the plugin what intervals data should be pulled at\n- Might be beneficial to allow users to define which tables should have data pulled into it (perhaps not all tables need replicated)\n- Likely need a way to know for each table what the last queried items were so you can do append-only type polling for new data. Does a user need to define a column to base this on (e.g. `id` or `created_at` columns perhaps)?\n",
              "url": "https://github.com/outerbase/starbasedb/issues/72",
              "tech": [],
              "repo_name": "starbasedb",
              "repo_owner": "outerbase",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "permitio#716",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "permitio",
              "id": "generated-permitio",
              "name": "Permitio",
              "description": "",
              "members": [],
              "display_name": "Permitio",
              "created_at": "2025-12-18T12:31:09.486Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/permitio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "permitio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:09.486Z",
            "created_at": "2025-12-18T12:31:09.486Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-permitio#716",
              "status": "open",
              "type": "issue",
              "number": 716,
              "title": "Error resolving broadcast hostname being swallowed",
              "source": {
                "data": {
                  "id": "source-permitio#716",
                  "user": {
                    "login": "keyz182",
                    "id": 693408,
                    "node_id": "MDQ6VXNlcjY5MzQwOA==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/693408?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/keyz182",
                    "html_url": "https://github.com/keyz182",
                    "followers_url": "https://api.github.com/users/keyz182/followers",
                    "following_url": "https://api.github.com/users/keyz182/following{/other_user}",
                    "gists_url": "https://api.github.com/users/keyz182/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/keyz182/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/keyz182/subscriptions",
                    "organizations_url": "https://api.github.com/users/keyz182/orgs",
                    "repos_url": "https://api.github.com/users/keyz182/repos",
                    "events_url": "https://api.github.com/users/keyz182/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/keyz182/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Error resolving broadcast hostname being swallowed",
                  "body": "**Describe the bug**\r\n\r\nWe had a deployment of opal-server that we'd typoed the broadcast URI. We were seeing websockets disconnect errors in the clients, but no errors server side, so no clues it was the broadcast URI. \r\n\r\nAfter turning debug logging on for the server, I spotted the following output:\r\n\r\n```\r\n2024-12-04T15:27:01.018085+0000| fastapi_websocket_rpc.websocket_rpc_e...| INFO  | Client connected\r\n2024-12-04T15:27:01.018500+0000| websockets.legacy.server                | INFO  | connection open\r\n2024-12-04T15:27:01.022749+0000| fastapi_websocket_rpc.rpc_channel       |DEBUG  | Handling RPC request - {'request': RpcRequest(method='_ping_', arguments={}, call_id='5d8d421e3dc94751a035b202644c8e8a'), 'channel': '72c2a547ffc64741bd095079bc778d7d'}\r\n2024-12-04T15:27:01.023420+0000| fastapi_websocket_pubsub.pub_sub_server |DEBUG  | task is done: {<Task finished name='Task-3053' coro=<EventBroadcaster.__read_notifications__() done, defined at /usr/local/lib/python3.10/site-packages/fastapi_websocket_pubsub/event_broadcaster.py:245> exception=gaierror(-2, 'Name or service not known')>}\r\n2024-12-04T15:27:01.023565+0000| fastapi_websocket_pubsub.event_broadc...| INFO  | Cancelling broadcast listen task\r\n2024-12-04T15:27:01.023677+0000| fastapi_websocket_pubsub.event_broadc...|DEBUG  | Unsubscribing from ALL TOPICS\r\n2024-12-04T15:27:01.023790+0000| fastapi_websocket_pubsub.event_notifier |DEBUG  | Removing Subscription of topic='__EventNotifier_ALL_TOPICS__' for subscriber=cc43be7c3ebb41e1b4869ace10d213db\r\n2024-12-04T15:27:01.023948+0000| fastapi_websocket_rpc.websocket_rpc_e...| INFO  | Client connection failed - 42723 :: 72c2a547ffc64741bd095079bc778d7d\r\n2024-12-04T15:27:01.024165+0000| fastapi_websocket_pubsub.pub_sub_server |DEBUG  | Leaving endpoint's main loop\r\n2024-12-04T15:27:01.026785+0000| websockets.legacy.server                | INFO  | connection closed\r\n```\r\n\r\nBased on the 4th line, I went to look at `fastapi_websocket_pubsub/event_broadcaster.py:245` and saw it was referencing the broadcast URI, at which point I double checked ours and saw the typo. \r\n\r\nWhat I believe to be a bug is that the error is being swallowed, and should probably be elevated to an `ERROR` level log for visibility.\r\n\r\n**To Reproduce**\r\nSet the broadcast URI to an invalid value - in our case, a postgres URI with a hostname that didn't resolve.\r\n\r\nLogs: [se616-opal-opal-server-545c454db-hx2nc.log](https://github.com/user-attachments/files/18010731/se616-opal-opal-server-545c454db-hx2nc.log)\r\n\r\n**Expected behavior**\r\nA clear error level log to indicate that the broadcast URI could not be resolved\r\n\r\n**OPAL version**\r\n - Version: Client - 0.7.15, Server - 0.7.8\r\n",
                  "html_url": "https://github.com/permitio/opal/issues/716"
                },
                "type": "github"
              },
              "hash": "permitio/opal#716",
              "body": "**Describe the bug**\r\n\r\nWe had a deployment of opal-server that we'd typoed the broadcast URI. We were seeing websockets disconnect errors in the clients, but no errors server side, so no clues it was the broadcast URI. \r\n\r\nAfter turning debug logging on for the server, I spotted the following output:\r\n\r\n```\r\n2024-12-04T15:27:01.018085+0000| fastapi_websocket_rpc.websocket_rpc_e...| INFO  | Client connected\r\n2024-12-04T15:27:01.018500+0000| websockets.legacy.server                | INFO  | connection open\r\n2024-12-04T15:27:01.022749+0000| fastapi_websocket_rpc.rpc_channel       |DEBUG  | Handling RPC request - {'request': RpcRequest(method='_ping_', arguments={}, call_id='5d8d421e3dc94751a035b202644c8e8a'), 'channel': '72c2a547ffc64741bd095079bc778d7d'}\r\n2024-12-04T15:27:01.023420+0000| fastapi_websocket_pubsub.pub_sub_server |DEBUG  | task is done: {<Task finished name='Task-3053' coro=<EventBroadcaster.__read_notifications__() done, defined at /usr/local/lib/python3.10/site-packages/fastapi_websocket_pubsub/event_broadcaster.py:245> exception=gaierror(-2, 'Name or service not known')>}\r\n2024-12-04T15:27:01.023565+0000| fastapi_websocket_pubsub.event_broadc...| INFO  | Cancelling broadcast listen task\r\n2024-12-04T15:27:01.023677+0000| fastapi_websocket_pubsub.event_broadc...|DEBUG  | Unsubscribing from ALL TOPICS\r\n2024-12-04T15:27:01.023790+0000| fastapi_websocket_pubsub.event_notifier |DEBUG  | Removing Subscription of topic='__EventNotifier_ALL_TOPICS__' for subscriber=cc43be7c3ebb41e1b4869ace10d213db\r\n2024-12-04T15:27:01.023948+0000| fastapi_websocket_rpc.websocket_rpc_e...| INFO  | Client connection failed - 42723 :: 72c2a547ffc64741bd095079bc778d7d\r\n2024-12-04T15:27:01.024165+0000| fastapi_websocket_pubsub.pub_sub_server |DEBUG  | Leaving endpoint's main loop\r\n2024-12-04T15:27:01.026785+0000| websockets.legacy.server                | INFO  | connection closed\r\n```\r\n\r\nBased on the 4th line, I went to look at `fastapi_websocket_pubsub/event_broadcaster.py:245` and saw it was referencing the broadcast URI, at which point I double checked ours and saw the typo. \r\n\r\nWhat I believe to be a bug is that the error is being swallowed, and should probably be elevated to an `ERROR` level log for visibility.\r\n\r\n**To Reproduce**\r\nSet the broadcast URI to an invalid value - in our case, a postgres URI with a hostname that didn't resolve.\r\n\r\nLogs: [se616-opal-opal-server-545c454db-hx2nc.log](https://github.com/user-attachments/files/18010731/se616-opal-opal-server-545c454db-hx2nc.log)\r\n\r\n**Expected behavior**\r\nA clear error level log to indicate that the broadcast URI could not be resolved\r\n\r\n**OPAL version**\r\n - Version: Client - 0.7.15, Server - 0.7.8\r\n",
              "url": "https://github.com/permitio/opal/issues/716",
              "tech": [
                "go"
              ],
              "repo_name": "opal",
              "repo_owner": "permitio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "permitio#677",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "permitio",
              "id": "generated-permitio",
              "name": "Permitio",
              "description": "",
              "members": [],
              "display_name": "Permitio",
              "created_at": "2025-12-18T12:31:09.815Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/permitio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "permitio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:09.815Z",
            "created_at": "2025-12-18T12:31:09.815Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-permitio#677",
              "status": "open",
              "type": "issue",
              "number": 677,
              "title": "Create E2E tests framework using PyTest",
              "source": {
                "data": {
                  "id": "source-permitio#677",
                  "user": {
                    "login": "danyi1212",
                    "id": 12188774,
                    "node_id": "MDQ6VXNlcjEyMTg4Nzc0",
                    "avatar_url": "https://avatars.githubusercontent.com/u/12188774?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/danyi1212",
                    "html_url": "https://github.com/danyi1212",
                    "followers_url": "https://api.github.com/users/danyi1212/followers",
                    "following_url": "https://api.github.com/users/danyi1212/following{/other_user}",
                    "gists_url": "https://api.github.com/users/danyi1212/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/danyi1212/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/danyi1212/subscriptions",
                    "organizations_url": "https://api.github.com/users/danyi1212/orgs",
                    "repos_url": "https://api.github.com/users/danyi1212/repos",
                    "events_url": "https://api.github.com/users/danyi1212/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/danyi1212/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Create E2E tests framework using PyTest",
                  "body": "We want to create a new baseline framework for writing E2E tests for OPAL Client and Server using PyTest\r\n\r\nThe test framework should do the following tasks on its baseline/initial run:\r\n* Run an OPAL Server and Client inside Docker\r\n* Initially test for health check responsivity\r\n* Check logs for errors and critical alerts\r\n* Check the client and server are connected using the [Statistics API](https://opal-v2.permit.io/redoc#tag/Server-Statistics/operation/get_statistics_statistics_get)\r\n\r\nThe acceptance criteria for this issue is the ability to run a single test command that will be based on the framework specified above and run a very basic assertion test on OPAL",
                  "html_url": "https://github.com/permitio/opal/issues/677"
                },
                "type": "github"
              },
              "hash": "permitio/opal#677",
              "body": "We want to create a new baseline framework for writing E2E tests for OPAL Client and Server using PyTest\r\n\r\nThe test framework should do the following tasks on its baseline/initial run:\r\n* Run an OPAL Server and Client inside Docker\r\n* Initially test for health check responsivity\r\n* Check logs for errors and critical alerts\r\n* Check the client and server are connected using the [Statistics API](https://opal-v2.permit.io/redoc#tag/Server-Statistics/operation/get_statistics_statistics_get)\r\n\r\nThe acceptance criteria for this issue is the ability to run a single test command that will be based on the framework specified above and run a very basic assertion test on OPAL",
              "url": "https://github.com/permitio/opal/issues/677",
              "tech": [],
              "repo_name": "opal",
              "repo_owner": "permitio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "permitio#634",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "permitio",
              "id": "generated-permitio",
              "name": "Permitio",
              "description": "",
              "members": [],
              "display_name": "Permitio",
              "created_at": "2025-12-18T12:31:10.030Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/permitio?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "permitio",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:10.030Z",
            "created_at": "2025-12-18T12:31:10.030Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-permitio#634",
              "status": "open",
              "type": "issue",
              "number": 634,
              "title": "OPAL Server doesn't clean up symbolic links when github is down",
              "source": {
                "data": {
                  "id": "source-permitio#634",
                  "user": {
                    "login": "kreyyser",
                    "id": 8156669,
                    "node_id": "MDQ6VXNlcjgxNTY2Njk=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/8156669?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/kreyyser",
                    "html_url": "https://github.com/kreyyser",
                    "followers_url": "https://api.github.com/users/kreyyser/followers",
                    "following_url": "https://api.github.com/users/kreyyser/following{/other_user}",
                    "gists_url": "https://api.github.com/users/kreyyser/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/kreyyser/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/kreyyser/subscriptions",
                    "organizations_url": "https://api.github.com/users/kreyyser/orgs",
                    "repos_url": "https://api.github.com/users/kreyyser/repos",
                    "events_url": "https://api.github.com/users/kreyyser/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/kreyyser/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "OPAL Server doesn't clean up symbolic links when github is down",
                  "body": "When opal server has github policies setup and github is down for some time opal server seems like spawn zombie processes but apparently looks like it is just a list of symbolic links that are not cleaned up.\r\n\r\n**To Reproduce**\r\nrun OPAL with github policies source as a container\r\nmake somehow github return 500\r\nlist processes in opal server container\r\n\r\n**Expected behavior**\r\nNo zombie processes or broken links proc directory\r\n\r\n**Screenshots**\r\n<img width=\"1329\" alt=\"opal-server-proc\" src=\"https://github.com/user-attachments/assets/9f4fa8a5-9867-45d2-a21d-2a7c133ac9c1\">\r\n\r\n**OPAL version**\r\n - Version: 0.7.6\r\n",
                  "html_url": "https://github.com/permitio/opal/issues/634"
                },
                "type": "github"
              },
              "hash": "permitio/opal#634",
              "body": "When opal server has github policies setup and github is down for some time opal server seems like spawn zombie processes but apparently looks like it is just a list of symbolic links that are not cleaned up.\r\n\r\n**To Reproduce**\r\nrun OPAL with github policies source as a container\r\nmake somehow github return 500\r\nlist processes in opal server container\r\n\r\n**Expected behavior**\r\nNo zombie processes or broken links proc directory\r\n\r\n**Screenshots**\r\n<img width=\"1329\" alt=\"opal-server-proc\" src=\"https://github.com/user-attachments/assets/9f4fa8a5-9867-45d2-a21d-2a7c133ac9c1\">\r\n\r\n**OPAL version**\r\n - Version: 0.7.6\r\n",
              "url": "https://github.com/permitio/opal/issues/634",
              "tech": [],
              "repo_name": "opal",
              "repo_owner": "permitio",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "highlight#8635",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "highlight",
              "id": "generated-highlight",
              "name": "Highlight",
              "description": "",
              "members": [],
              "display_name": "Highlight",
              "created_at": "2025-12-18T12:31:09.393Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/highlight?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "highlight",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:09.393Z",
            "created_at": "2025-12-18T12:31:09.393Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [
              "go"
            ],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-highlight#8635",
              "status": "open",
              "type": "issue",
              "number": 8635,
              "title": "Update workspace and project settings to not use antd components",
              "source": {
                "data": {
                  "id": "source-highlight#8635",
                  "user": {
                    "login": "ccschmitz",
                    "id": 308182,
                    "node_id": "MDQ6VXNlcjMwODE4Mg==",
                    "avatar_url": "https://avatars.githubusercontent.com/u/308182?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/ccschmitz",
                    "html_url": "https://github.com/ccschmitz",
                    "followers_url": "https://api.github.com/users/ccschmitz/followers",
                    "following_url": "https://api.github.com/users/ccschmitz/following{/other_user}",
                    "gists_url": "https://api.github.com/users/ccschmitz/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/ccschmitz/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/ccschmitz/subscriptions",
                    "organizations_url": "https://api.github.com/users/ccschmitz/orgs",
                    "repos_url": "https://api.github.com/users/ccschmitz/repos",
                    "events_url": "https://api.github.com/users/ccschmitz/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/ccschmitz/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Update workspace and project settings to not use antd components",
                  "body": "Whoever implements should feel free to break this up into multiple tickets, perhaps one per page.",
                  "html_url": "https://github.com/highlight/highlight/issues/8635"
                },
                "type": "github"
              },
              "hash": "highlight/highlight#8635",
              "body": "Whoever implements should feel free to break this up into multiple tickets, perhaps one per page.",
              "url": "https://github.com/highlight/highlight/issues/8635",
              "tech": [
                "go"
              ],
              "repo_name": "highlight",
              "repo_owner": "highlight",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "highlight#8614",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "highlight",
              "id": "generated-highlight",
              "name": "Highlight",
              "description": "",
              "members": [],
              "display_name": "Highlight",
              "created_at": "2025-12-18T12:31:09.629Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/highlight?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "highlight",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:09.630Z",
            "created_at": "2025-12-18T12:31:09.630Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-highlight#8614",
              "status": "open",
              "type": "issue",
              "number": 8614,
              "title": "update design of integrations page",
              "source": {
                "data": {
                  "id": "source-highlight#8614",
                  "user": {
                    "login": "Vadman97",
                    "id": 1351531,
                    "node_id": "MDQ6VXNlcjEzNTE1MzE=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1351531?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Vadman97",
                    "html_url": "https://github.com/Vadman97",
                    "followers_url": "https://api.github.com/users/Vadman97/followers",
                    "following_url": "https://api.github.com/users/Vadman97/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Vadman97/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Vadman97/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Vadman97/subscriptions",
                    "organizations_url": "https://api.github.com/users/Vadman97/orgs",
                    "repos_url": "https://api.github.com/users/Vadman97/repos",
                    "events_url": "https://api.github.com/users/Vadman97/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Vadman97/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "update design of integrations page",
                  "body": "Call us +1 (650) 420-2207  https://github.com/highlight/highlight/issues/8635 https://github.com/highlight/highlight/pull/9716 https://github.com/highlight/highlight/issues/8614 https://github.com/hig",
                  "html_url": "https://github.com/highlight/highlight/issues/8614"
                },
                "type": "github"
              },
              "hash": "highlight/highlight#8614",
              "body": "Call us +1 (650) 420-2207  https://github.com/highlight/highlight/issues/8635 https://github.com/highlight/highlight/pull/9716 https://github.com/highlight/highlight/issues/8614 https://github.com/hig",
              "url": "https://github.com/highlight/highlight/issues/8614",
              "tech": [],
              "repo_name": "highlight",
              "repo_owner": "highlight",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "highlight#8032",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "highlight",
              "id": "generated-highlight",
              "name": "Highlight",
              "description": "",
              "members": [],
              "display_name": "Highlight",
              "created_at": "2025-12-18T12:31:09.853Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/highlight?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "highlight",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:09.853Z",
            "created_at": "2025-12-18T12:31:09.853Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-highlight#8032",
              "status": "open",
              "type": "issue",
              "number": 8032,
              "title": "document sveltekit backend instrumentation",
              "source": {
                "data": {
                  "id": "source-highlight#8032",
                  "user": {
                    "login": "Vadman97",
                    "id": 1351531,
                    "node_id": "MDQ6VXNlcjEzNTE1MzE=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/1351531?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Vadman97",
                    "html_url": "https://github.com/Vadman97",
                    "followers_url": "https://api.github.com/users/Vadman97/followers",
                    "following_url": "https://api.github.com/users/Vadman97/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Vadman97/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Vadman97/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Vadman97/subscriptions",
                    "organizations_url": "https://api.github.com/users/Vadman97/orgs",
                    "repos_url": "https://api.github.com/users/Vadman97/repos",
                    "events_url": "https://api.github.com/users/Vadman97/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Vadman97/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "document sveltekit backend instrumentation",
                  "body": "see linear thread linked for starting point\n\n[https://discord.com/channels/1026884757667188757/1217184768001839225/1218189516041621524](https://discord.com/channels/1026884757667188757/1217184768001839225/1218189516041621524)",
                  "html_url": "https://github.com/highlight/highlight/issues/8032"
                },
                "type": "github"
              },
              "hash": "highlight/highlight#8032",
              "body": "see linear thread linked for starting point\n\n[https://discord.com/channels/1026884757667188757/1217184768001839225/1218189516041621524](https://discord.com/channels/1026884757667188757/1217184768001839225/1218189516041621524)",
              "url": "https://github.com/highlight/highlight/issues/8032",
              "tech": [],
              "repo_name": "highlight",
              "repo_owner": "highlight",
              "forge": "github"
            },
            "timeouts_disabled": false
          },
          {
            "id": "highlight#6775",
            "status": "open",
            "type": "standard",
            "kind": "dev",
            "org": {
              "handle": "highlight",
              "id": "generated-highlight",
              "name": "Highlight",
              "description": "",
              "members": [],
              "display_name": "Highlight",
              "created_at": "2025-12-18T12:31:10.105Z",
              "website_url": "",
              "avatar_url": "https://avatars.githubusercontent.com/u/highlight?v=4",
              "discord_url": "",
              "slack_url": "",
              "stargazers_count": 0,
              "twitter_url": "",
              "youtube_url": "",
              "tech": [],
              "github_handle": "highlight",
              "accepts_sponsorships": false,
              "days_until_timeout": null,
              "enabled_expert_recs": false,
              "enabled_private_bounties": false
            },
            "updated_at": "2025-12-18T12:31:10.105Z",
            "created_at": "2025-12-18T12:31:10.105Z",
            "visibility": "public",
            "autopay_disabled": false,
            "tech": [],
            "bids": [],
            "is_external": false,
            "manual_assignments": false,
            "point_reward": null,
            "reward": {
              "currency": "USD",
              "amount": 10000
            },
            "reward_formatted": "$100",
            "reward_tiers": [],
            "reward_type": "cash",
            "task": {
              "id": "task-highlight#6775",
              "status": "open",
              "type": "issue",
              "number": 6775,
              "title": "Performance of canvas snapshotting on Safari is poor",
              "source": {
                "data": {
                  "id": "source-highlight#6775",
                  "user": {
                    "login": "Pinpickle",
                    "id": 3238878,
                    "node_id": "MDQ6VXNlcjMyMzg4Nzg=",
                    "avatar_url": "https://avatars.githubusercontent.com/u/3238878?v=4",
                    "gravatar_id": "",
                    "url": "https://api.github.com/users/Pinpickle",
                    "html_url": "https://github.com/Pinpickle",
                    "followers_url": "https://api.github.com/users/Pinpickle/followers",
                    "following_url": "https://api.github.com/users/Pinpickle/following{/other_user}",
                    "gists_url": "https://api.github.com/users/Pinpickle/gists{/gist_id}",
                    "starred_url": "https://api.github.com/users/Pinpickle/starred{/owner}{/repo}",
                    "subscriptions_url": "https://api.github.com/users/Pinpickle/subscriptions",
                    "organizations_url": "https://api.github.com/users/Pinpickle/orgs",
                    "repos_url": "https://api.github.com/users/Pinpickle/repos",
                    "events_url": "https://api.github.com/users/Pinpickle/events{/privacy}",
                    "received_events_url": "https://api.github.com/users/Pinpickle/received_events",
                    "type": "User",
                    "user_view_type": "public",
                    "site_admin": false
                  },
                  "title": "Performance of canvas snapshotting on Safari is poor",
                  "body": "**Describe the bug**\r\n\r\nSnapshotting a canvas element in Safari 16.x takes a long time (>20ms). This leads to a frame drop every time the canvas is snapshotted.\r\n\r\n**To Reproduce**\r\n\r\nGo to the Codsandbox full-screen here https://jqgtld.csb.app/ \r\n\r\nLook at the \"Snapshot\" stat in the top right corner (or in the console). On my computer it is ~20ms on Safari 16.5. On Chrome, it is between 0 and 0.3ms.\r\n\r\nThe window size is 1057 x 734 (retina display, so double the dimensions for the canvas)\r\n\r\nHere's the codesandbox with the editor: https://codesandbox.io/s/dazzling-roentgen-jqgtld?file=/src/App.js\r\n\r\n**Expected behavior**\r\n\r\nOverhead for snapshotting to be near 0\r\n\r\n**Screenshots**\r\n\r\n## Safari\r\n\r\n![CleanShot 2023-10-02 at 15 23 03 png](https://github.com/highlight/highlight/assets/3238878/1bc6d193-ea1f-4fda-ae03-7ea9c9cf9143)\r\n\r\n## Chrome\r\n\r\n![CleanShot 2023-10-02 at 15 23 55 png](https://github.com/highlight/highlight/assets/3238878/7a3767f7-94a0-467b-9c23-bac305507d66)\r\n\r\n## iOS\r\n\r\n![CleanShot 2023-10-02 at 15 27 57 png](https://github.com/highlight/highlight/assets/3238878/664fc6c4-27b8-43ce-be71-b35b7553716f)\r\n\r\n\r\n**Additional context**\r\n\r\nEnvironment:\r\n\r\n```\r\nSystem:\r\n    OS: macOS 13.4\r\n    CPU: (8) arm64 Apple M1\r\n    Memory: 51.78 MB / 8.00 GB\r\nBrowsers:\r\n    Chrome: 117.0.5938.92\r\n    Safari: 16.5\r\n```\r\n\r\nProvided this also affects Safari 17 (I don't know if it does), this affects _every_ Safari-based browser (including everything on iOS). This performance drop means we had to disable canvas recording for Safari on our app which really limits its usefulness.\r\n\r\nThe larger the canvas, the longer the snapshot time, from what I can tell. Changing `canvasMaxSnapshotDimension` does not appear to make a difference. Safari doesn't even respect the options parameter for `createBitmapImage`: https://developer.mozilla.org/en-US/docs/Web/API/createImageBitmap\r\n\r\nI'm reasonably confident this comes from the call to `createBitmapImage` here:\r\n\r\nhttps://github.com/highlight/highlight/blob/ed2ea183d0d11781736e2001b01e24e55c33e8cb/frontend/src/__generated/rr/rr.js#L4266-L4269\r\n\r\nIt's supposed to be asynchronous but it looks like it is blocking in Safari. I wonder if there are other ways to get the image data out of a canvas? I understand there are efforts to use WebRTC for canvas recording? I imagine this could help.\r\n",
                  "html_url": "https://github.com/highlight/highlight/issues/6775"
                },
                "type": "github"
              },
              "hash": "highlight/highlight#6775",
              "body": "**Describe the bug**\r\n\r\nSnapshotting a canvas element in Safari 16.x takes a long time (>20ms). This leads to a frame drop every time the canvas is snapshotted.\r\n\r\n**To Reproduce**\r\n\r\nGo to the Codsandbox full-screen here https://jqgtld.csb.app/ \r\n\r\nLook at the \"Snapshot\" stat in the top right corner (or in the console). On my computer it is ~20ms on Safari 16.5. On Chrome, it is between 0 and 0.3ms.\r\n\r\nThe window size is 1057 x 734 (retina display, so double the dimensions for the canvas)\r\n\r\nHere's the codesandbox with the editor: https://codesandbox.io/s/dazzling-roentgen-jqgtld?file=/src/App.js\r\n\r\n**Expected behavior**\r\n\r\nOverhead for snapshotting to be near 0\r\n\r\n**Screenshots**\r\n\r\n## Safari\r\n\r\n![CleanShot 2023-10-02 at 15 23 03 png](https://github.com/highlight/highlight/assets/3238878/1bc6d193-ea1f-4fda-ae03-7ea9c9cf9143)\r\n\r\n## Chrome\r\n\r\n![CleanShot 2023-10-02 at 15 23 55 png](https://github.com/highlight/highlight/assets/3238878/7a3767f7-94a0-467b-9c23-bac305507d66)\r\n\r\n## iOS\r\n\r\n![CleanShot 2023-10-02 at 15 27 57 png](https://github.com/highlight/highlight/assets/3238878/664fc6c4-27b8-43ce-be71-b35b7553716f)\r\n\r\n\r\n**Additional context**\r\n\r\nEnvironment:\r\n\r\n```\r\nSystem:\r\n    OS: macOS 13.4\r\n    CPU: (8) arm64 Apple M1\r\n    Memory: 51.78 MB / 8.00 GB\r\nBrowsers:\r\n    Chrome: 117.0.5938.92\r\n    Safari: 16.5\r\n```\r\n\r\nProvided this also affects Safari 17 (I don't know if it does), this affects _every_ Safari-based browser (including everything on iOS). This performance drop means we had to disable canvas recording for Safari on our app which really limits its usefulness.\r\n\r\nThe larger the canvas, the longer the snapshot time, from what I can tell. Changing `canvasMaxSnapshotDimension` does not appear to make a difference. Safari doesn't even respect the options parameter for `createBitmapImage`: https://developer.mozilla.org/en-US/docs/Web/API/createImageBitmap\r\n\r\nI'm reasonably confident this comes from the call to `createBitmapImage` here:\r\n\r\nhttps://github.com/highlight/highlight/blob/ed2ea183d0d11781736e2001b01e24e55c33e8cb/frontend/src/__generated/rr/rr.js#L4266-L4269\r\n\r\nIt's supposed to be asynchronous but it looks like it is blocking in Safari. I wonder if there are other ways to get the image data out of a canvas? I understand there are efforts to use WebRTC for canvas recording? I imagine this could help.\r\n",
              "url": "https://github.com/highlight/highlight/issues/6775",
              "tech": [],
              "repo_name": "highlight",
              "repo_owner": "highlight",
              "forge": "github"
            },
            "timeouts_disabled": false
          }
        ],
        "next_cursor": null
      }
    }
  }
}