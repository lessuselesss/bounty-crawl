name: ðŸ•·ï¸ Intelligent Bounty Scraper

on:
  schedule:
    # Run every 15 minutes with change detection
    - cron: '*/15 * * * *'

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      force_commit:
        description: 'Force commit even if no changes detected'
        required: false
        default: false
        type: boolean
      full_scan:
        description: 'Perform full scan (ignore cache)'
        required: false
        default: false
        type: boolean

  # Run when organizations list is updated
  push:
    paths:
      - 'data/organizations.json'
      - 'config/scraper-config.json'
    branches:
      - main

  # Run on pull request for testing
  pull_request:
    paths:
      - 'src/**'
      - '.github/workflows/full-scrape.yml'
      - '.github/workflows/targeted-scrape.yml'
      - 'deno.json'
      - 'flake.nix'

env:
  # Nix configuration
  NIX_CONFIG: "experimental-features = nix-command flakes"

  # SOPS configuration
  SOPS_AGE_KEY: ${{ secrets.SOPS_AGE_KEY }}

  # GitHub token for API access and committing
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

jobs:
  scrape:
    name: Full Bounty Scrape
    runs-on: ubuntu-latest

    permissions:
      contents: write
      actions: read

    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4
        with:
          # Fetch full history for proper change detection
          fetch-depth: 0
          # Use the GITHUB_TOKEN for checkout to allow pushing
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: ðŸ¦• Install latest Deno
        run: |
          curl -fsSL https://deno.land/install.sh | sh
          echo "$HOME/.deno/bin" >> $GITHUB_PATH

      - name: ðŸŽ­ Install Playwright browsers
        run: |
          # Install Chromium browser for Playwright
          $HOME/.deno/bin/deno run --allow-all --no-lock npm:playwright@1.48.2/playwright install chromium --with-deps

      - name: ðŸ”‘ Setup SOPS
        continue-on-error: true
        run: |
          # Install SOPS and age
          curl -LO https://github.com/mozilla/sops/releases/download/v3.7.3/sops-v3.7.3.linux
          sudo mv sops-v3.7.3.linux /usr/local/bin/sops
          sudo chmod +x /usr/local/bin/sops

          curl -LO https://github.com/FiloSottile/age/releases/download/v1.1.1/age-v1.1.1-linux-amd64.tar.gz
          tar -xzf age-v1.1.1-linux-amd64.tar.gz
          sudo mv age/age* /usr/local/bin/

          # Create age key file for SOPS
          echo "$SOPS_AGE_KEY" > .age-key
          chmod 600 .age-key
          export SOPS_AGE_KEY_FILE="$PWD/.age-key"
          echo "SOPS_AGE_KEY_FILE=$PWD/.age-key" >> "$GITHUB_ENV"

          # Test SOPS configuration
          sops --version

          # Decrypt secrets if they exist
          if [ -f "secrets/github-token.yaml" ]; then
            echo "ðŸ”“ Decrypting secrets..."
            if sops -d secrets/github-token.yaml > /tmp/secrets.json 2>/tmp/sops_error.log; then
              # Check if the output is valid JSON
              if jq empty /tmp/secrets.json 2>/dev/null; then
                echo "âœ… Successfully decrypted secrets"
                GITHUB_API_TOKEN=$(jq -r '.github_token // empty' /tmp/secrets.json 2>/dev/null || echo "")
                ALGORA_SESSION=$(jq -r '.algora_session // empty' /tmp/secrets.json 2>/dev/null || echo "")

                # Add to GitHub environment
                echo "GITHUB_API_TOKEN=$GITHUB_API_TOKEN" >> "$GITHUB_ENV"
                echo "ALGORA_SESSION=$ALGORA_SESSION" >> "$GITHUB_ENV"
              else
                echo "âš ï¸  SOPS output is not valid JSON:"
                cat /tmp/secrets.json
                echo "â„¹ï¸  Using repository secrets instead"
              fi
              # Clean up
              rm -f /tmp/secrets.json
            else
              echo "âŒ SOPS decryption failed:"
              cat /tmp/sops_error.log
              echo "â„¹ï¸  Using repository secrets instead"
            fi
            rm -f /tmp/sops_error.log
          else
            echo "â„¹ï¸  No encrypted secrets found, using repository secrets"
          fi

      - name: ðŸ“¦ Cache Deno dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/deno
            .deno
            .cache
          key: deno-${{ hashFiles('deno.lock') }}
          restore-keys: |
            deno-

      - name: ðŸ” Detect changes
        id: detect
        run: |
          echo "ðŸ” Detecting changed organizations..."

          # Ensure data directory exists
          mkdir -p data

          # Check if it's Sunday (full scrape day) or manual full_scan requested
          IS_SUNDAY=$(date -u +%u)
          if [ "$IS_SUNDAY" = "7" ] || [ "${{ github.event.inputs.full_scan }}" = "true" ]; then
            echo "ðŸ“… Sunday or full_scan requested - forcing full scrape"
            deno run --allow-all --no-lock scripts/detect-changes.ts --force-all
            echo "mode=full" >> "$GITHUB_OUTPUT"
          else
            # Run change detection
            deno run --allow-all --no-lock scripts/detect-changes.ts
            echo "mode=targeted" >> "$GITHUB_OUTPUT"
          fi

          # Read changed orgs from the file (more reliable than parsing stdout)
          if [ -f "data/changed-orgs.txt" ]; then
            CHANGED_ORGS=$(cat data/changed-orgs.txt)
          else
            CHANGED_ORGS=""
          fi

          echo "changed_orgs=$CHANGED_ORGS" >> "$GITHUB_OUTPUT"
          echo "ðŸ“Š Changed organizations: $CHANGED_ORGS"

          # Count number of changed orgs
          if [ -z "$CHANGED_ORGS" ]; then
            CHANGE_COUNT=0
          else
            CHANGE_COUNT=$(echo "$CHANGED_ORGS" | tr ',' '\n' | wc -l)
          fi
          echo "change_count=$CHANGE_COUNT" >> "$GITHUB_OUTPUT"
          echo "ðŸ“ˆ Number of changed orgs: $CHANGE_COUNT"

      - name: ðŸ” Run targeted scraper
        id: scrape
        if: steps.detect.outputs.change_count > 0
        run: |
          # Configure git for automated commits
          git config --local user.email "scraper@algora-bounty-scraper.com"
          git config --local user.name "Algora Bounty Scraper"
          git config --local commit.gpgsign false

          # Ensure data directory exists
          mkdir -p data/archive logs

          CHANGED_ORGS="${{ steps.detect.outputs.changed_orgs }}"
          SCRAPE_MODE="${{ steps.detect.outputs.mode }}"

          # Set production scraper arguments
          ARGS=""

          # For PR events, disable auto-commit and encryption
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            ARGS="$ARGS --no-commit --no-encrypt"
          fi

          # Add targeted organizations if not full scrape
          if [ "$SCRAPE_MODE" = "targeted" ] && [ -n "$CHANGED_ORGS" ]; then
            ARGS="$ARGS --orgs $CHANGED_ORGS"
            echo "ðŸŽ¯ Running TARGETED scrape for: $CHANGED_ORGS"
          else
            echo "ðŸŒ Running FULL scrape (all 91 organizations)"
          fi

          echo "ðŸš€ Running production scraper with args: ${ARGS:-none}"

          # Run the production scraper with better error handling
          if ! deno run --allow-all --no-lock scripts/production-scraper.ts $ARGS; then
            echo "âŒ Scraper failed, checking logs..."
            if [ -d logs ]; then
              echo "Available log files:"
              ls -la logs/
              echo "Latest log content:"
              find logs -name "*.log" -type f -exec tail -50 {} \;
            fi
            exit 1
          fi

          # Check if the scraper created new commits
          # The production scraper commits changes automatically, so we check for unpushed commits
          if git log origin/main..HEAD --oneline | grep -q .; then
            echo "changes_detected=true" >> "$GITHUB_OUTPUT"
            echo "âœ… Changes detected and committed"

            # Push the commits to origin
            echo "ðŸ“¤ Pushing changes to origin..."
            git push origin main

            # Get summary of changes for the output
            CHANGES_SUMMARY=$(git log origin/main..HEAD --oneline | head -1)
            echo "changes_summary=$CHANGES_SUMMARY" >> "$GITHUB_OUTPUT"
          else
            echo "changes_detected=false" >> "$GITHUB_OUTPUT"
            echo "â„¹ï¸  No changes to commit or push"
          fi

      - name: ðŸ“Š Generate summary report
        if: success()
        run: |
          SCRAPE_MODE="${{ steps.detect.outputs.mode }}"
          CHANGE_COUNT="${{ steps.detect.outputs.change_count }}"
          CHANGED_ORGS="${{ steps.detect.outputs.changed_orgs }}"

          if [ "$SCRAPE_MODE" = "full" ]; then
            echo "## ðŸŒ Full Bounty Scrape (Weekly)" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "*Comprehensive scrape of all 91+ organizations (Sunday safety net)*" >> $GITHUB_STEP_SUMMARY
          elif [ "$CHANGE_COUNT" -gt "0" ]; then
            echo "## ðŸŽ¯ Intelligent Targeted Scrape" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "*Only scraped $CHANGE_COUNT organizations that changed since last run*" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Changed organizations:** \`$CHANGED_ORGS\`" >> $GITHUB_STEP_SUMMARY
          else
            echo "## âœ… No Changes Detected" >> "$GITHUB_STEP_SUMMARY"
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "*All organizations unchanged - no scraping needed*" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f "data/bounty-index.json" ]; then
            TOTAL_ORGS=$(jq -r '.total_organizations' data/bounty-index.json)
            TOTAL_BOUNTIES=$(jq -r '.total_bounties' data/bounty-index.json)
            TOTAL_VALUE=$(jq -r '.total_value_usd' data/bounty-index.json)
            GENERATED_AT=$(jq -r '.generated_at' data/bounty-index.json)
            SUCCESS_RATE=$(jq -r '.metadata.success_rate * 100 | round' data/bounty-index.json)

            echo "### ðŸ“ˆ Statistics" >> $GITHUB_STEP_SUMMARY
            echo "- **Organizations**: $TOTAL_ORGS" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Bounties**: $TOTAL_BOUNTIES" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Value**: \$$(echo $TOTAL_VALUE | numfmt --format=%.0f --grouping)" >> $GITHUB_STEP_SUMMARY
            echo "- **Success Rate**: $SUCCESS_RATE%" >> $GITHUB_STEP_SUMMARY
            echo "- **Generated**: $GENERATED_AT" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Top organizations by bounty count
            echo "### ðŸ† Top Organizations" >> $GITHUB_STEP_SUMMARY
            jq -r '.organizations | to_entries | sort_by(.value.bounty_count) | reverse | limit(5;.[]) | "- **\(.key)**: \(.value.bounty_count) bounties (\(.value.total_value_usd | floor) USD)"' data/bounty-index.json >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY

            # Recent changes if any
            if [ "${{ steps.scrape.outputs.changes_detected }}" = "true" ]; then
              echo "### ðŸ”„ Changes Detected" >> $GITHUB_STEP_SUMMARY
              echo "- Modified files: ${{ steps.scrape.outputs.changes_summary }}" >> $GITHUB_STEP_SUMMARY
            else
              echo "### âœ… No Changes" >> $GITHUB_STEP_SUMMARY
              echo "All bounties up to date!" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âŒ No bounty index generated" >> $GITHUB_STEP_SUMMARY
          fi

      - name: ðŸ“¤ Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraping-artifacts-${{ github.run_id }}
          path: |
            data/bounty-index.json
            data/algora-api-response.json
            data/archive/
            logs/
          retention-days: 7

      - name: ðŸ”” Notify on failure
        if: failure()
        run: |
          echo "## âŒ Scraping Failed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The scraping workflow encountered an error. Please check the logs for details." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID**: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY

  # Cleanup old workflow runs to save storage
  cleanup:
    name: Cleanup Old Runs
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'

    steps:
      - name: ðŸ§¹ Delete old workflow runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 7
          keep_minimum_runs: 5